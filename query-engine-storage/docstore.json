{"docstore/metadata": {"d914f83b-13c7-4aa6-9d99-6c6966cefb76": {"doc_hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990"}, "ace96383-91a8-4ce4-9991-d1f8c9dfca7b": {"doc_hash": "b2e8bd6089e40da549103bc5954b4d73306e65baa998ccd4547258c9a9c92672", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "bbb16dfa-4df8-45e8-8575-c923b0a7a1e6": {"doc_hash": "ea1ebbdb40e526913dc7d06d63246410f4d71359c5905039265451246304b815", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "c431c262-3bd5-4f52-b353-41fbe544193f": {"doc_hash": "1e79b006f00dbd1ff47c8b847cbb0de8a0d5b3ecc6f07742ed84f26fe921ce9d", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "e60d4c04-26e1-4862-bd16-e862f116c1c8": {"doc_hash": "17796c36ee01649bd7b43f01cc956a9c0f090559aadbce9a659f8b8cc2caea1a", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "f3164624-9d62-483b-adfa-22f953d5d72d": {"doc_hash": "228e77e75e5524c64d0ea50740f6168b18e844e3d04dbaf4961ba31f351f24bf", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "6fd714dc-8a91-46f3-bfed-e2e99b2231ad": {"doc_hash": "1fd509256a99277d570c096e63a85b690dd17ce52e8f458e3b171bde500d69d1", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "381b91fd-d618-42b1-a8b0-d7fc717cbe1a": {"doc_hash": "ebc382906565871e1171706ba4dbcad53059445c60f9e998773eadb524bab538", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "f02540d1-dd5b-4373-9e43-75d2b653262f": {"doc_hash": "0c69a4c77e59d7a57f5455ddc8b151fe5c04bf54c5b24ad121b74a17ae355fa6", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "faa7df87-7b97-48e9-9f1e-9851891fbb60": {"doc_hash": "d5a0e6fabe1ff6f67e90c5cc7484a7f779ceb155c9df19c0eea5430edca6497d", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "03d906b2-0f58-4471-aa57-512d430b1a8d": {"doc_hash": "7b976f9c224c426dacaf3086bbc8c926a019895989d51642227829b47301e287", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "fa4cef61-df1f-482f-91cd-afb36144c385": {"doc_hash": "bcc4601b7e735581c0784e3a1d82554a0388b51ec390f441a5ad1f76c46d238f", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "fc885578-181b-4524-87c2-a6101aad791f": {"doc_hash": "4fe28bd06a7bd003bf0ef1e31520ffdfb9414568d99fe1c1e37be0058b85ff94", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "d0421a7e-b2dd-48a9-922f-1a320b86dc68": {"doc_hash": "37c9eefa03e6fbc882a330552584d08eb26798a20551d10ef9bae97ba9484264", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "e97a38de-0c61-4e11-a134-cf6a3494c356": {"doc_hash": "eee7c1c7005b61fce56a2a951ba13d3d568c19cb4c0eff505fe8c4d8ca2f82ae", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "5344fe2a-1777-4a59-8545-83d8a02b0d01": {"doc_hash": "9e14a16c7a516addfae74dbebd0b7e12932d2ed8ca1d15fd8fd40eb2e6467f78", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "61c4479f-e0a9-4972-9b5a-45dbbc87098f": {"doc_hash": "1970ce65a5f7faf68ed576cf2a2ea95a4d50a09c5e24e01325f5f378809b056c", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "2a9b83a3-dd88-4503-8a0a-ab3c9c4e35d1": {"doc_hash": "9bfce298c6fd87e0e4aeb79697f081a0c7bd683108a67ae6881c362d6f338516", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "bcdc4f94-f41b-4a25-9118-cff07121a7fd": {"doc_hash": "f12b4ec3bc5c2ca569b03cdd1ab4cbbee58b668306ca9cedaa355042070dfc60", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "da655a22-2362-4fcb-b871-1be34671bb22": {"doc_hash": "365f36f6586d6597edb769edf8a21ec42a2204959b8445bb56cf05f1d1b789ce", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "a198df72-c7a9-4eeb-94ae-44010993ee07": {"doc_hash": "7fa8ce16c6ba0cc199e87fca6b565dd10ff692fc360c9b9c72a89163f32f194c", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "3685dcf2-1b40-4173-b0d4-0b4ab0600d16": {"doc_hash": "56786d6e57ddc877f10ce91a96bb4875fa69fe2999eef57d4894efec12e7adcd", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "bb6281ce-264e-4d10-903d-945017a9bc91": {"doc_hash": "8cd754d58d654441ece7ef3cdb8fe35aaf6a61d0cd530a049c070c36cecb34f7", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "74c803eb-152f-477b-82ea-a36750e40a11": {"doc_hash": "0103970e0226d5a91e102b705b4a6add0a83f71d269a69e1b4ee8212aecbafc5", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "2de4ea91-6cbd-45cf-841b-73895c2b025c": {"doc_hash": "ebbf3dcdacb3fd446323ff597ad082f2ba48b1951bdcbdb305ab461050c50524", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "3267fb5c-e0ca-451e-be15-e12501b36b41": {"doc_hash": "396d077c9788bbc95c20948b46aed257e0bf473024159c610290969ba390f04a", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "25bf7ade-a83b-408c-9b6a-9219de556a66": {"doc_hash": "8df69b0cbccf212ed9c99eba3f5b028a1b7192ed5bc125f2c9c3dbeec7f548e8", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "ea27b600-b6aa-4287-b010-f870bea23948": {"doc_hash": "938b01f79ed7044a9cada66718ce2a6a2b1bacf09926c4e245ec94db5628331a", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "48041027-0da9-4fef-9cfb-9abad4b9fa31": {"doc_hash": "626ad5a044d87e4ddf3d0d946f0cd7f367cf3278accf5498383a7f2c10c0bc40", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "a7a9fe00-0761-420c-aa86-6f09e532e997": {"doc_hash": "ea7c5ccc331fa7ab6013017454427c97c3f6e90777625e45768fa633bf0c3f03", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "f136077d-cfa8-4e46-9428-70f8a35a3cd9": {"doc_hash": "26ae0ae4a2f611cf18fdd39f23592d0b5558e5ac705d280cc8c9159b2d10333c", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "c9dbb20c-6eec-4fe9-a790-4f71ffb6f519": {"doc_hash": "89ebaa0c52963b66e9fca42e33ed7969310e053e7b47c70d98b8e4369d4fcca7", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "a6d19960-2a44-4207-b169-cfb9751e2e36": {"doc_hash": "1c0145e99b2778ed9251d97c6984b58e26a93e65cbe8677859abafcaed8811ad", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "b9d5ed90-13d6-4d84-bf61-245566012bec": {"doc_hash": "69ddf75e79580613dc37b16c75ad20a37dae5b04cd448d2363757b6713ce0f68", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "fbcb1b65-21f3-4a94-b7cb-0ffd09e7a4b2": {"doc_hash": "8bb20204fc28a952ad6e71bd73bab371050d82f1edb558554400222295c32855", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "2966a825-d7a4-44e9-adca-dd45b773d7b5": {"doc_hash": "3c6fd699db9f6300ac872373c1b8c4f3bd87b5665c66768a92eb7b5fb21e721b", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "aedff500-b446-415a-bd6d-0e47d0c8ede5": {"doc_hash": "ab2469181eaf671724fc0b31b49d62bb2644258527ef9d0d3736812d4498d565", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "49c78894-9ad2-43d4-aaaa-801a8f0ea6b4": {"doc_hash": "b08afea2d95b8b74ae4e5562c2ef062b00899e1cd31d177e84e4a135f09660f3", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "d06066b2-9f34-4b6d-8b6a-bb5da37bdc09": {"doc_hash": "5330769509ed708ac351c7801ac0fdce7943937b000478ae888d6dfd59d93650", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "9ed8b590-d78d-4563-8eaf-c0b9943087a2": {"doc_hash": "af44eb30b0ee9f4fa8d5b029659aea248538f82598cce5d273ec72ea43b2fb49", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "5eea0768-1899-498f-975b-3539cf6918b6": {"doc_hash": "5a0499c33bbaf70a12dec7bcbc07b0242ff49ffac54d3998abc85e851ab392a6", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "7addba30-957c-4b4c-b0dc-45b354c3e054": {"doc_hash": "23b8319e6c32c83a0881a01471e2c98528597965426bd8cff1ae24fc2c94e085", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "18d24414-c932-44fa-a638-367219367557": {"doc_hash": "0a88c86035c9d888c7f6ee2fd7a354b5eaa825b42f3d50a698809f228f78ef68", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "60a99625-ae64-4a40-993a-3ea42f7f301a": {"doc_hash": "b39495cc23c7e4e7c5e18648097b50c3e12a58310a517abdc0bce86105a70e71", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "3572b7c9-76ec-4f9c-b38c-7f273ffaa74d": {"doc_hash": "336b9c40a2039d3d96fe7e6e673fd50c01eaf9f4caf4c5cb95d97484cfadfc5a", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "68b97399-8de0-49ed-b969-77332f492ad7": {"doc_hash": "bf4c7ca2fb41d578e9575c68f91cc97717899e00117c983aac8974afa9477282", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "5ed7643e-deec-4591-a6fe-87c4570b93b0": {"doc_hash": "873e4d2d8db15bf854d38837d92ead5a69ea881486365d805ae69539b3f6b378", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "231c5d59-5343-4c3d-8f55-4ee499745ddc": {"doc_hash": "eb8c5c6b28138a57d6e7ddfcfb6914d3648c1bf77e943faa790069f654630d66", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "ef26e5f2-a389-4894-a859-71c908852cef": {"doc_hash": "b6c82ecc05e137415f1684c839e4264e399a1c57d6731d69c67cfbf8bf9ac087", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "62acc989-c403-467c-965f-2b12e4713814": {"doc_hash": "842d1f73c6a32fda76b8797777e570f732c1dbe3f574dcfd1f872316897d97bd", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "6a83038e-cbba-4fb3-b8b8-b6b689f6a3ed": {"doc_hash": "928a25b93bcd8a23474837147406690fad6857f1a633364f9dba44917bb9970d", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "0439b1ad-5724-4381-afe3-964117079cdc": {"doc_hash": "4d0aee714ebc289db3322b9ad00171f9b249dacb09baf09bdf4056af9cbe508c", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "0807d0df-90d4-4507-9156-329dab5637ab": {"doc_hash": "899c2e3b044632d61626c707aff4afe782c956570ce7999b6c184007aaab3f30", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "ded70955-9f7c-49f5-acdd-49cfde908eba": {"doc_hash": "836f020c0d2e9f7b569ff7d6189f289770ef09f67b275f5ab3f3b3f2b029a2a8", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "ad2a9ea5-0f2f-441f-8592-1c9e0226767b": {"doc_hash": "a03708e2e6e97a91aa26b970ce97069e6dd09e4c34776913aa7bc3c8179ffe38", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "e4185784-9e89-4d35-9edc-24c10fe9ca06": {"doc_hash": "1218e0ea79f87701d3944b2345cba1e4a0a2627dcb6cb6f7e02fcdade58c9e3b", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "6f449a64-d70d-4317-b295-9717ad8b2642": {"doc_hash": "d9331df89f91831bdfdf00a1a3ef83025f2f5537afc186cdaee41a4f51b34cc2", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "69b0019f-26cf-4f6b-8aa2-222ce52928e5": {"doc_hash": "974ad73bfa566345ab0c470a1fb2a8e491dc38cd752e7bbd08f1f1200e75ac07", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "b2d0893b-25ed-4ae7-933b-1971a1b409c6": {"doc_hash": "0ea70fc30c628e9884c58ca3ee15762c2cd1ff967ce713c6f7aa19eb9196745f", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "b82c1f65-168c-4812-8c00-dbcd4a41f388": {"doc_hash": "7acfc8e66dd87e8a3b1c91877d11d60036b0182161fb390634d4b4e08d007873", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "4c24cadc-7500-433d-ad59-c229e4bfda4c": {"doc_hash": "0f73f4d67cf582e9aa66b14b3f616cfe129c994c5f8af651cc43ff78f85b268e", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "ec065687-46bd-4cb7-94e5-8f5f89fcdce6": {"doc_hash": "51bcf1bdf32eef68324a2faf6567ae157e37a61dfabd6a080c2e3e70f3503423", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "3d141dfb-9965-464a-b56b-801c6659061d": {"doc_hash": "e1388872fe6ed2e3ae8fe381496895514b410cf3b12d08bd3d33b3eb5acb0029", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "aa3a684f-0ec7-4281-9a3b-534b9cdecf75": {"doc_hash": "250f1c8a3a208c6909ed0d610e0151ac15e6e513016df186c760f9af86bcdec6", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "25dff030-b29e-4888-b567-2321192d7ead": {"doc_hash": "5cdecb58fe60dc286e865a9ae1fd95b502ca7ca34bd936038a4f9a831c023fd1", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "c96e6bb5-18b2-425d-b277-61bf77277545": {"doc_hash": "f34919197ab287dd43f50b3bf97e02a544a398ae860d92aa2a9f525c1013bb30", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "c88f18dd-26c8-46ef-afd4-b9632b02caab": {"doc_hash": "a155a610f4a9bc7458f2597c5c493791f2027940424ded9cd21193e4d40f9ec4", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "8555986c-bddb-40d5-9883-8ecd3bb2b1b0": {"doc_hash": "22c15247a44356ab779812263e1170b57970b5839f806dee878778dabdd5dad4", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "318b6bd8-c300-4715-a2c8-a2949c2973e5": {"doc_hash": "147ce17a43351fd076524c0adff0e12c8dbcb6bfa4912ef2baaa6ee75c735380", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "8d3f051d-8f08-4cad-8121-f9a6aeda8f8b": {"doc_hash": "93723124892fd1773269e9b42b967837ac7abb41ce108a8a40f123897f8debb5", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "83ef7637-1af7-4a7d-a0c7-bbfb76204433": {"doc_hash": "a31e80777b8ea2e19d8078cc534578dede70a10348136e1a6a3a108de856d07e", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "d5af8599-1a36-43f7-93e7-014faadf5a0f": {"doc_hash": "08bc9bea9887590265df8f8db8add1c17ba7da465c4485044a082494855ccb45", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "f95f4e3c-7da7-455e-b679-83ca5daa71fc": {"doc_hash": "8d7f5e2051fc1007b7a02c056fe0b0cfc1c8c197e8f8eaae22abb29478c9dd17", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "afaa625b-12b0-4621-b5ea-fefcdca49e17": {"doc_hash": "0366ee669bac5d5d4ed83ccfdc819dcbd557971b85650d99933e4f4f93f79139", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "83c4bccb-d8e5-48b6-8465-aade8a02ff5e": {"doc_hash": "150cad9142b1e25fc166bd5f01fa605f0d070c4fc6ef9fbbd5c5f228c2b7d570", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "d45b2642-cfd0-467f-85ab-50f4cdd1b8c5": {"doc_hash": "39971ef71c94cbc5a9ff218155bd7be6248987761d225e72d75cc52c8878b2af", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "686d51e9-53cc-4358-9569-0decee462e14": {"doc_hash": "9fde3d62ce8edcd5fe7c591b6e4b7f7d956349bc8fd1c5331926a01dcc77ffe7", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "b6606255-69d5-434e-9718-006b5038069d": {"doc_hash": "5c5cff23a1c44a7351fab4011850b158fb8bfdbfed5c067e31862ef3113e8313", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "776e2399-4d16-496a-ae37-6293812c0aa7": {"doc_hash": "8ccb3eb3e90a6df55a9ada6d75b5755a5ace5d28d4e20908db1eb9fd3d33ca11", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "f84eacb8-56f7-435e-a192-160e385b8f3a": {"doc_hash": "6ff3842d10d49058e804137e54e7062b9e4a7c777473b4d98f3bfac044309ec8", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "111b2ba6-d6f3-46eb-b2cd-fcb08e1173c3": {"doc_hash": "6d17a4a0c6d861257fca40f941da2d7eef0660d0c7ab698bee244a9be9e14eb0", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "0092c886-f6c2-40a9-814a-498b1bd8b1ff": {"doc_hash": "ac5a3cd10ea6599fd1a03374cc077cf73ee083adc3ee63936dc84d3f6d557298", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "92e92808-bffe-463a-81d9-ff4ae347fed9": {"doc_hash": "afba58b9d6951b4faf49df55ede55d8564a9e51de3d10b83f394ccc63d646b7e", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "1da0ed9f-881e-4323-9ed3-c2fb9a1ff2e1": {"doc_hash": "8685f8378f0c77773fc3f433fd0b43b3e8968b5b38d5f530022d3b937f2efc4a", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "01a9cec3-ae8b-45e3-86df-2de432dae135": {"doc_hash": "557217cb679be6f95a978099694f2cbabc1d5d27df64588e5610a9bf4720c4ce", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "34fc1ecc-efc6-42ef-b75c-306da43e9a43": {"doc_hash": "da91f0b4b910bb9e087a96cf06ca5a6078b2c3f26cc1a2511905b93eee9a346e", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "f0d36da2-9328-47af-9b4a-4b9f02ae76ec": {"doc_hash": "e7afac6374c4a9ee5f8f6eac490f479e6f95a809461d3df543c86710980582f9", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "129c499f-6718-4409-a666-0dc9c1678513": {"doc_hash": "6cf8ee966535f35c4a2f1942697af80e29505bf87a49bf8c0749025c6edf3366", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "70b981ce-d788-42eb-a096-77f20488dc39": {"doc_hash": "3fecc1fbd9ad81495e311a6d144cc745abf300f4082e886cec87512dbacfda57", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "48c996ef-173a-48d0-8f63-5c1b94cc568a": {"doc_hash": "968f99a3bbfe2baa40a42435ce0854d39e288eca71f7474391eed36e7ca602e9", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "ed3c8710-e4da-4eab-a25c-57425ea642ec": {"doc_hash": "b90774cf022d452ea107168bdc23458a29f22dc90777ac52df4ff1303ba68e5a", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "56b40446-0053-4d08-b4a6-dda9318adf30": {"doc_hash": "0654f378189bfc3cec4193c25478587f9c7a8313003a635825571d5c21012fa8", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "26e27990-9b99-4df3-9f9f-b0312d1bcf1b": {"doc_hash": "c6ca865d82782faf95b5f5c7f777e0445056252efe094b69db9c512813ba1364", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "c0488e72-595b-481a-a556-36580943f430": {"doc_hash": "b07a15084a8e62a0ac1c3f477d530718ac7339644f30cfd5e6b1a00ab457e374", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "0e1a68a1-350c-4eaf-afdb-f91a5ecbc9bd": {"doc_hash": "5b2ba468ebd1ba6661ef1b7d93f66f00e69325e6016db6b4291fb2bc9a3a19b5", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "1babe9c5-3d0a-4c4c-be56-02235674699d": {"doc_hash": "d6df0a52b15505a4e92ab8d73ed6b9d511c1bffd54e03b36e3778e5bc0c7d207", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "53c686da-c604-4300-a874-43c9f167fad1": {"doc_hash": "3aa5458ffd71f6935cd96a51fc600dea174386992569b826b9c952e46646d330", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "0c38e4bb-32e3-42a8-b310-51f2e43ed43e": {"doc_hash": "b227ae2631a4fa982948f07cd796d6691c9ad02375cbd6d9df45d79e4434a808", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "4c798e2c-ceca-4a11-ad12-d0f1ef390274": {"doc_hash": "68c6e47da02c2a48dd54718c702ff77b62b183fea0971f198567401c10b8b9cf", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "2fb9f671-7b94-49c7-b67d-33381b9687f4": {"doc_hash": "c69ce3a262d9e723a5657a2552403e7e39e7e2221d12435497b49686ab03e226", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "e523a814-0c35-43b8-a648-ac76fe673583": {"doc_hash": "91ff152e73d0b5fed20beef3f7fafbe5ca0570457b27cedea61a429f7f9e1a7a", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "771aaf0c-15b4-45b5-bbe6-e18793a984cc": {"doc_hash": "827a3c0f749abab7d3982896c61d13998ef834cf962c9bf3e5391995db365c35", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "d532e660-adfd-4fec-8715-7ffb22cb8528": {"doc_hash": "4f797921ecf584e4d0f46c99f2770bd00415e7bece560be1eea949a1883980ea", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "7097d46d-5944-4b03-8834-601745bf7d7f": {"doc_hash": "1821da0bde954dd8c1ecefaa2c65643a9bdceb7d4f40a84e3380edd1fec8e180", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "e331819e-6621-42c7-80d8-993fb11103b1": {"doc_hash": "1026dd992719faa8c4023b343165544f9b111ee8eb062a6282a4b8bd4d396da4", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "84fe1523-aa71-40ee-9f02-284730404b4b": {"doc_hash": "611ffc298a4c5b447d8fe49c9b5f7e2f9b2f9f69fac1dbb11062402361e81ab6", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "83ef953c-4b55-4200-a1a0-36c4c18fa0e7": {"doc_hash": "aa1ee82c5ba2d5ad982b91daaaa6bbb0ead972c32a4e5bee953fb3f4038e276f", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "5dff88aa-0d11-4688-a1a4-605ff622dbd4": {"doc_hash": "3eee98bd172e575ac4da8c410c0e43f64f591c82c812475fc9d12f413320f1c8", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "a299032c-9a01-403d-ae35-126803f5e4e6": {"doc_hash": "b14b61cafc23ec00da9328c9e7a19d2188d73bb62ab6572ff7cbec9c8fbeb6c2", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "d59c3720-e45d-4bd5-bc13-e16de66ce155": {"doc_hash": "d2e867f5c9f215ef453940450d2dcf0ccbfa43928e4fdbc98c79ced59a6a2ad0", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "34063b81-c7fa-4e45-a358-58c98359efe0": {"doc_hash": "528867a203f75fc9aa95d989c8549bcf5076ad7176ec38752e0d3fff3c953250", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "3b202ff2-8981-41f2-bb59-59d0568b22d0": {"doc_hash": "492bc0904f36fd4c92adca1dedf4a7f103e7dd2eeb92b5e203173bbdab5ec63e", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "2e0951a7-b6ff-44ae-907d-de3eab8cd06e": {"doc_hash": "5c8a0057a36848abd87832c06e50225b090dc9d3155cbeb81378d68d28a1153d", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "351e2982-4e08-4376-b2d5-07f6b0fe0ea3": {"doc_hash": "e2165f35cec99adb08e2aab75b0d3f56aaf571fa6114856555e65188e416ae03", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "3a122c60-bfbf-46c9-b35a-13ec08eac9e3": {"doc_hash": "55db1b9672d3545571250a0d2586f3a223bac765416d9b462c047e4196415796", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "3dabc1d5-12a5-495f-9491-958d353cfa83": {"doc_hash": "a4ad305c67c4e24c8b9d477f611c4da58a5dc41320b8269458a5f3ff554a2b1e", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "15b806c2-18b8-4c03-ace2-107df8e142df": {"doc_hash": "46fbf6d1f2337dd87716aa49d53999320467ea3e7de06d75e9e024a6a9376b05", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "c1ffbf9c-cf78-4ec0-bb21-996eee6485b0": {"doc_hash": "9160a7c37e58853be6b1f1eef265a86fea0536d54c258cc6dceabee9ec2f9fb1", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "7c9c8724-b47f-4f88-8b81-e68a4fe0b324": {"doc_hash": "b555a9dcfae0131de7b0121acfb138869c4f4ebbd8e77945a841b8c164c75e8f", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "24fd3e82-2856-4c41-8dda-bbbd673588fe": {"doc_hash": "0fb7a0047c99250ad5b79575556957910a1cc448a52d0d15fbd19237b895c56b", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "446825a5-5453-46bd-9a98-2ab522e790c1": {"doc_hash": "c4d9be4691baa43a2f418cb6a94299d977e7a7541dd76a80a32cd44462695d1d", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "056cd542-3046-419a-ac7b-506e850052c2": {"doc_hash": "96a89af2c7046dc69fe1737609dabf50ca599ef84d5689a2c09b5c397b25da31", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "aa9dfbca-bd79-4ba4-8441-b2395a35d76d": {"doc_hash": "2e3206a05ca0658760b2ce1821f973bc7046b12fbea96f112006f9b16e19ba17", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "1ee20a64-de36-4f10-bd0a-0423682edd98": {"doc_hash": "b212d935c009f73e13b0a95da1cf18921aa1d28395c948cb480530b49b5d25d0", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "b55fc3a0-341e-4295-9f27-625ab38c9103": {"doc_hash": "6cbd650e0c7e0002b51d99e890124843856c207013d892a917f9453cac02c9e4", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "ad29cabf-3f1d-4e78-a57d-dba69871acda": {"doc_hash": "307741ef7f8ae4c27cfd1c3b37015baa13c597d1ef31a2ef1b4d176d217baa9f", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "e501f501-0fdc-4723-ad98-08dbe22853c0": {"doc_hash": "689e5288cab6a6c8c71e4351fe509b1fc240799fe527abcf7b1f00cc8d4d3e0f", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "de2145d2-f6f3-43a7-bb3c-49a286ba46ba": {"doc_hash": "16a42004dc84f8b67ae05e3831052ca5a3b0206590a3c4f32a58513a3a8a5e33", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "434a100d-0c39-450f-88c1-699ead75ede8": {"doc_hash": "4006f9a2d7584f4166e7d231404029b9cc94f59432c3d5c612f11a82abc6c996", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "37b3def5-bfc5-4a0b-9a34-183622b9fd5b": {"doc_hash": "9c13ede85f51cb9463ae94007e2f61246c3ad0f4fc48ad3cc1a146c7ac8f0f0f", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "4d78577a-cf2e-4d33-b256-4ce9ee80b8ef": {"doc_hash": "97c8ec98c45fb053da9437c34164590b3126151ce400371d7627ec0b96c31cc1", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "eaa04127-3117-4679-bf23-b3936537248f": {"doc_hash": "331a40a27680ec5f73fbc80873e30ebf2eac0e0a2092401d8a4ca92a9ff2426c", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "ce9b13d1-3415-4af5-9ecc-835e56370019": {"doc_hash": "0f8d68ab7838290cda75b4bf200a0752bcd1c625c477af954f97141ad4dc730c", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "fed2fb97-023e-4292-824d-9fd90a12ccff": {"doc_hash": "c1c8122cb3d700febe192e27e570b9330878e206304801b38408c40f856c8649", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "4640ee57-adcc-428f-b26f-98898c03b923": {"doc_hash": "a147044186f2e92a54bd85ecd01c1895ab556fe0d5ca2fd98e2a0af80e5a9662", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "a87d2cde-9e07-4bd8-9bab-4f2b7b3515e9": {"doc_hash": "e163f87d5fea848d75e47171d57358c3bcbead3eecc6c451477905bdb4a6d3bf", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "df7326bb-8486-447e-a459-21a7db120eee": {"doc_hash": "f827c78fe302eb5d2c0b5b9498cfd3f8b61610c4f252c20aa5c97cd16dd74fa5", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "d0be6438-6762-4c4c-b539-179149aea4ff": {"doc_hash": "29966fce64be49f7dc77908a7ba95ea690094c07f2fa2b78d10f3b43b9597f57", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "f28ec727-26f4-462c-9052-b05097fcb0b2": {"doc_hash": "2dbc1b27df0235af849c9611819dbf3d18e9e392300b70fbc6ca346d35d7f78c", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "1480a717-409e-436a-9d8f-95ebfcd01aeb": {"doc_hash": "2157a7d1a28e85907355b50ad94c0e6253e3f6c4e54bb19acedea3a752e9403b", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "357c81f4-568e-44a0-8265-07314416218e": {"doc_hash": "6abb5c3b09153802b73987ccc37757f3aabddf2334fd817d91b608e0e23f0af2", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "bf9024cd-66d8-4658-9e3a-00d897dc74b8": {"doc_hash": "c3807a02d15450034964309248feca9a82d485b7f77e426af5b9a689ff2356cf", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "5d355b83-d0d9-4440-b837-aff4af36d149": {"doc_hash": "eed532f4e2650cad0948b59a245ad22d346012f0402699428b5baac17c994e2b", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "372c2e82-3fff-426d-8220-64edfbd37215": {"doc_hash": "fc56d6cdf2579069d2f753d845d46d6870b7b5554a25bee49d3b9edcdbda5d32", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "bcf524a3-cb3c-4227-9d2a-95d14172163b": {"doc_hash": "18567c09b471464127821fd44e41681a031b1d0db42d94ad82355ce092fc32c1", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "276f9601-25aa-4c60-a1e2-b5424d8c3169": {"doc_hash": "ea4b45b86ad505a1958ba53626d82911be5cc657e68cfffb559c02c9ce25bc8c", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "985e16fa-a202-41f6-9339-6dae0eee21c3": {"doc_hash": "787c6401608aca623989964aac057e2170f138e46106d1fdaf3fd9ab0ee4b0d6", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "41986ff7-696a-401f-a4f4-0c5b368e0f47": {"doc_hash": "611ad938bac191d3c34bba7d406e17ce56158e09035770cb02f8ac2366c05ff9", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "afc25fb3-ad93-4495-9177-b8f8f1e1a1d1": {"doc_hash": "8a9756f76682d6af044340320f36efe2aa116c6674587518c105d986777d6943", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "f1bdc863-4a6f-4729-81e8-fb7d7904ce82": {"doc_hash": "1dfbea91fd39ea476c2581ca9199a511a3e9340c217d832d3aa59a9964545bf0", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "1f029326-89fa-4c2e-a4fe-b3c4a7534b52": {"doc_hash": "c24a7522e45ecc5f0fed003843160ae846569e4ec8377e0da32ecca52414fa81", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "4e53197e-0fa3-469f-9361-8e4cc4a35111": {"doc_hash": "513b15025a0e1bee15b39930a631fd61edb72ffd3fb08558c5984d971400adaf", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "52bfe569-5298-4422-85d9-b7787bc6a0f3": {"doc_hash": "5c93197cd82e2335df9a9b6f488677e6c1e943ffe1b67d127dbfb00e4cce33b8", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "b9081599-bf1d-4e52-b896-40c163aeb607": {"doc_hash": "8c1b04916cf7eb537b79729888b5dddc00215808612aae90f05f5a490ff98d2d", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "c7036f8a-86a7-4b13-ac9b-7fc1e6ea9856": {"doc_hash": "f588d8f886a5692715da0b6da98383d05ccee5ce5a2f0072b63fa03fed506081", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "e3e11f3b-9f75-4a4a-a7f5-d39eee24b915": {"doc_hash": "b110bf96f931fa762a4cfeba51af189fb1ce1213702a14721f5e17b87490e717", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "e3756bd1-1403-4a9f-9af8-7e467d081214": {"doc_hash": "2ce910fdf8cdaa3ba16b225619eb56f90155b46c8d4cc0c1c46488c1e20ade60", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "251fc671-9500-4c7a-831b-39dc46b32bd1": {"doc_hash": "04d7fd063b8f32c8aa86c918347d8d4f2418748de85ac29a48f4408ced59bf17", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "d7a17ddc-128f-412e-bcc2-a16e0054c96f": {"doc_hash": "63911471a471fbce4dbc570919a11b78e7180737540e23ca190468f592ac8f39", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "507ac74d-1d52-4b71-b156-c9ddd88a48e3": {"doc_hash": "c9d8ff26f9074d2a74c7369ff4c036bc7a4a8194a9e7afff4574099815e7dc6f", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "f4879e88-a1e1-461a-8bd8-eb7450a49da6": {"doc_hash": "25103997668ffea71c7cf9a874b38abb13fe0d05b289b99adce632e7ad2374f2", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "c63fddf7-e277-49e4-82ae-ce320904ac33": {"doc_hash": "b92fe303d05350cf46b20656b2401e9278f84e50ef833be9d98bb0da2b66efdd", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "eb731752-5e68-463d-9e0f-a61b820a8cb6": {"doc_hash": "2b297ef91893ac3bfef3d43c6de9f93ea4bfe6314bc4494a08f6c32c4bb5b237", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "3467eb37-e970-47ec-be94-65b56d86a240": {"doc_hash": "69e3c3e1763c24491eb14a3cb95ec794ea800b2001a5f1dbd881c4830e609318", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "a9447afe-1f53-44e6-9143-b96e57fcd4f5": {"doc_hash": "a9407395604832cbfd28e33d4810ae6af015f00417a69914b5165b11c7a77462", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "8cd6cfd4-2f6b-4285-8645-5b9b1de2d468": {"doc_hash": "5ddc17f5872c938b14219e4f41903014dbefd03ac200d0752eaa94e222f52561", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "16c6dd83-fbd3-4671-8af8-c45ed743b662": {"doc_hash": "4454c3f694ef1e5c48a7ba90f45c91e40ece3903a03a62deaee4b53b6bb77fc7", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "16d7b1b0-c92d-497d-9234-a6ee77da68b1": {"doc_hash": "7a9e5d1ee74e68f721a095acff87aab32084301ed0302900d103717de790e2a5", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}, "634a7b01-dcd2-403f-b543-f87f8420f2a5": {"doc_hash": "53fd2c199a797e00d4f6eb8f7f391988505764885bd7417401dba21f8398bd6d", "ref_doc_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76"}}, "docstore/ref_doc_info": {"d914f83b-13c7-4aa6-9d99-6c6966cefb76": {"node_ids": ["ace96383-91a8-4ce4-9991-d1f8c9dfca7b", "bbb16dfa-4df8-45e8-8575-c923b0a7a1e6", "c431c262-3bd5-4f52-b353-41fbe544193f", "e60d4c04-26e1-4862-bd16-e862f116c1c8", "f3164624-9d62-483b-adfa-22f953d5d72d", "6fd714dc-8a91-46f3-bfed-e2e99b2231ad", "381b91fd-d618-42b1-a8b0-d7fc717cbe1a", "f02540d1-dd5b-4373-9e43-75d2b653262f", "faa7df87-7b97-48e9-9f1e-9851891fbb60", "03d906b2-0f58-4471-aa57-512d430b1a8d", "fa4cef61-df1f-482f-91cd-afb36144c385", "fc885578-181b-4524-87c2-a6101aad791f", "d0421a7e-b2dd-48a9-922f-1a320b86dc68", "e97a38de-0c61-4e11-a134-cf6a3494c356", "5344fe2a-1777-4a59-8545-83d8a02b0d01", "61c4479f-e0a9-4972-9b5a-45dbbc87098f", "2a9b83a3-dd88-4503-8a0a-ab3c9c4e35d1", "bcdc4f94-f41b-4a25-9118-cff07121a7fd", "da655a22-2362-4fcb-b871-1be34671bb22", "a198df72-c7a9-4eeb-94ae-44010993ee07", "3685dcf2-1b40-4173-b0d4-0b4ab0600d16", "bb6281ce-264e-4d10-903d-945017a9bc91", "74c803eb-152f-477b-82ea-a36750e40a11", "2de4ea91-6cbd-45cf-841b-73895c2b025c", "3267fb5c-e0ca-451e-be15-e12501b36b41", "25bf7ade-a83b-408c-9b6a-9219de556a66", "ea27b600-b6aa-4287-b010-f870bea23948", "48041027-0da9-4fef-9cfb-9abad4b9fa31", "a7a9fe00-0761-420c-aa86-6f09e532e997", "f136077d-cfa8-4e46-9428-70f8a35a3cd9", "c9dbb20c-6eec-4fe9-a790-4f71ffb6f519", "a6d19960-2a44-4207-b169-cfb9751e2e36", "b9d5ed90-13d6-4d84-bf61-245566012bec", "fbcb1b65-21f3-4a94-b7cb-0ffd09e7a4b2", "2966a825-d7a4-44e9-adca-dd45b773d7b5", "aedff500-b446-415a-bd6d-0e47d0c8ede5", "49c78894-9ad2-43d4-aaaa-801a8f0ea6b4", "d06066b2-9f34-4b6d-8b6a-bb5da37bdc09", "9ed8b590-d78d-4563-8eaf-c0b9943087a2", "5eea0768-1899-498f-975b-3539cf6918b6", "7addba30-957c-4b4c-b0dc-45b354c3e054", "18d24414-c932-44fa-a638-367219367557", "60a99625-ae64-4a40-993a-3ea42f7f301a", "3572b7c9-76ec-4f9c-b38c-7f273ffaa74d", "68b97399-8de0-49ed-b969-77332f492ad7", "5ed7643e-deec-4591-a6fe-87c4570b93b0", "231c5d59-5343-4c3d-8f55-4ee499745ddc", "ef26e5f2-a389-4894-a859-71c908852cef", "62acc989-c403-467c-965f-2b12e4713814", "6a83038e-cbba-4fb3-b8b8-b6b689f6a3ed", "0439b1ad-5724-4381-afe3-964117079cdc", "0807d0df-90d4-4507-9156-329dab5637ab", "ded70955-9f7c-49f5-acdd-49cfde908eba", "ad2a9ea5-0f2f-441f-8592-1c9e0226767b", "e4185784-9e89-4d35-9edc-24c10fe9ca06", "6f449a64-d70d-4317-b295-9717ad8b2642", "69b0019f-26cf-4f6b-8aa2-222ce52928e5", "b2d0893b-25ed-4ae7-933b-1971a1b409c6", "b82c1f65-168c-4812-8c00-dbcd4a41f388", "4c24cadc-7500-433d-ad59-c229e4bfda4c", "ec065687-46bd-4cb7-94e5-8f5f89fcdce6", "3d141dfb-9965-464a-b56b-801c6659061d", "aa3a684f-0ec7-4281-9a3b-534b9cdecf75", "25dff030-b29e-4888-b567-2321192d7ead", "c96e6bb5-18b2-425d-b277-61bf77277545", "c88f18dd-26c8-46ef-afd4-b9632b02caab", "8555986c-bddb-40d5-9883-8ecd3bb2b1b0", "318b6bd8-c300-4715-a2c8-a2949c2973e5", "8d3f051d-8f08-4cad-8121-f9a6aeda8f8b", "83ef7637-1af7-4a7d-a0c7-bbfb76204433", "d5af8599-1a36-43f7-93e7-014faadf5a0f", "f95f4e3c-7da7-455e-b679-83ca5daa71fc", "afaa625b-12b0-4621-b5ea-fefcdca49e17", "83c4bccb-d8e5-48b6-8465-aade8a02ff5e", "d45b2642-cfd0-467f-85ab-50f4cdd1b8c5", "686d51e9-53cc-4358-9569-0decee462e14", "b6606255-69d5-434e-9718-006b5038069d", "776e2399-4d16-496a-ae37-6293812c0aa7", "f84eacb8-56f7-435e-a192-160e385b8f3a", "111b2ba6-d6f3-46eb-b2cd-fcb08e1173c3", "0092c886-f6c2-40a9-814a-498b1bd8b1ff", "92e92808-bffe-463a-81d9-ff4ae347fed9", "1da0ed9f-881e-4323-9ed3-c2fb9a1ff2e1", "01a9cec3-ae8b-45e3-86df-2de432dae135", "34fc1ecc-efc6-42ef-b75c-306da43e9a43", "f0d36da2-9328-47af-9b4a-4b9f02ae76ec", "129c499f-6718-4409-a666-0dc9c1678513", "70b981ce-d788-42eb-a096-77f20488dc39", "48c996ef-173a-48d0-8f63-5c1b94cc568a", "ed3c8710-e4da-4eab-a25c-57425ea642ec", "56b40446-0053-4d08-b4a6-dda9318adf30", "26e27990-9b99-4df3-9f9f-b0312d1bcf1b", "c0488e72-595b-481a-a556-36580943f430", "0e1a68a1-350c-4eaf-afdb-f91a5ecbc9bd", "1babe9c5-3d0a-4c4c-be56-02235674699d", "53c686da-c604-4300-a874-43c9f167fad1", "0c38e4bb-32e3-42a8-b310-51f2e43ed43e", "4c798e2c-ceca-4a11-ad12-d0f1ef390274", "2fb9f671-7b94-49c7-b67d-33381b9687f4", "e523a814-0c35-43b8-a648-ac76fe673583", "771aaf0c-15b4-45b5-bbe6-e18793a984cc", "d532e660-adfd-4fec-8715-7ffb22cb8528", "7097d46d-5944-4b03-8834-601745bf7d7f", "e331819e-6621-42c7-80d8-993fb11103b1", "84fe1523-aa71-40ee-9f02-284730404b4b", "83ef953c-4b55-4200-a1a0-36c4c18fa0e7", "5dff88aa-0d11-4688-a1a4-605ff622dbd4", "a299032c-9a01-403d-ae35-126803f5e4e6", "d59c3720-e45d-4bd5-bc13-e16de66ce155", "34063b81-c7fa-4e45-a358-58c98359efe0", "3b202ff2-8981-41f2-bb59-59d0568b22d0", "2e0951a7-b6ff-44ae-907d-de3eab8cd06e", "351e2982-4e08-4376-b2d5-07f6b0fe0ea3", "3a122c60-bfbf-46c9-b35a-13ec08eac9e3", "3dabc1d5-12a5-495f-9491-958d353cfa83", "15b806c2-18b8-4c03-ace2-107df8e142df", "c1ffbf9c-cf78-4ec0-bb21-996eee6485b0", "7c9c8724-b47f-4f88-8b81-e68a4fe0b324", "24fd3e82-2856-4c41-8dda-bbbd673588fe", "446825a5-5453-46bd-9a98-2ab522e790c1", "056cd542-3046-419a-ac7b-506e850052c2", "aa9dfbca-bd79-4ba4-8441-b2395a35d76d", "1ee20a64-de36-4f10-bd0a-0423682edd98", "b55fc3a0-341e-4295-9f27-625ab38c9103", "ad29cabf-3f1d-4e78-a57d-dba69871acda", "e501f501-0fdc-4723-ad98-08dbe22853c0", "de2145d2-f6f3-43a7-bb3c-49a286ba46ba", "434a100d-0c39-450f-88c1-699ead75ede8", "37b3def5-bfc5-4a0b-9a34-183622b9fd5b", "4d78577a-cf2e-4d33-b256-4ce9ee80b8ef", "eaa04127-3117-4679-bf23-b3936537248f", "ce9b13d1-3415-4af5-9ecc-835e56370019", "fed2fb97-023e-4292-824d-9fd90a12ccff", "4640ee57-adcc-428f-b26f-98898c03b923", "a87d2cde-9e07-4bd8-9bab-4f2b7b3515e9", "df7326bb-8486-447e-a459-21a7db120eee", "d0be6438-6762-4c4c-b539-179149aea4ff", "f28ec727-26f4-462c-9052-b05097fcb0b2", "1480a717-409e-436a-9d8f-95ebfcd01aeb", "357c81f4-568e-44a0-8265-07314416218e", "bf9024cd-66d8-4658-9e3a-00d897dc74b8", "5d355b83-d0d9-4440-b837-aff4af36d149", "372c2e82-3fff-426d-8220-64edfbd37215", "bcf524a3-cb3c-4227-9d2a-95d14172163b", "276f9601-25aa-4c60-a1e2-b5424d8c3169", "985e16fa-a202-41f6-9339-6dae0eee21c3", "41986ff7-696a-401f-a4f4-0c5b368e0f47", "afc25fb3-ad93-4495-9177-b8f8f1e1a1d1", "f1bdc863-4a6f-4729-81e8-fb7d7904ce82", "1f029326-89fa-4c2e-a4fe-b3c4a7534b52", "4e53197e-0fa3-469f-9361-8e4cc4a35111", "52bfe569-5298-4422-85d9-b7787bc6a0f3", "b9081599-bf1d-4e52-b896-40c163aeb607", "c7036f8a-86a7-4b13-ac9b-7fc1e6ea9856", "e3e11f3b-9f75-4a4a-a7f5-d39eee24b915", "e3756bd1-1403-4a9f-9af8-7e467d081214", "251fc671-9500-4c7a-831b-39dc46b32bd1", "d7a17ddc-128f-412e-bcc2-a16e0054c96f", "507ac74d-1d52-4b71-b156-c9ddd88a48e3", "f4879e88-a1e1-461a-8bd8-eb7450a49da6", "c63fddf7-e277-49e4-82ae-ce320904ac33", "eb731752-5e68-463d-9e0f-a61b820a8cb6", "3467eb37-e970-47ec-be94-65b56d86a240", "a9447afe-1f53-44e6-9143-b96e57fcd4f5", "8cd6cfd4-2f6b-4285-8645-5b9b1de2d468", "16c6dd83-fbd3-4671-8af8-c45ed743b662", "16d7b1b0-c92d-497d-9234-a6ee77da68b1", "634a7b01-dcd2-403f-b543-f87f8420f2a5"], "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}}}, "docstore/data": {"ace96383-91a8-4ce4-9991-d1f8c9dfca7b": {"__data__": {"id_": "ace96383-91a8-4ce4-9991-d1f8c9dfca7b", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bbb16dfa-4df8-45e8-8575-c923b0a7a1e6", "node_type": "1", "metadata": {}, "hash": "086cb31ce51145d1507592dc58ba47c66de75166ab5508834f1dfba3d9cbd887", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Content from https://docs.baseten.co/concepts/howbasetenworks:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nDeveloping a Chain\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nExporting metrics\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nConcepts\nHow Baseten works\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nIt supports multiple entry points depending on your workflow\u2014whether you\u2019re deploying a dedicated model, calling an open-source LLM via our Model API, or training from scratch.\nAt the core is the Baseten Inference Stack:\nperformant model runtimes on top of Inference optimized infrastructure. Instead of managing infrastructure, scaling policies, and performance optimization, you can focus on building and iterating on your AI-powered applications.\n\u200b\nDedicated deployments\nThis is the primary workflow for teams deploying custom, open-source, or fine-tuned models with full control.\nBaseten\u2019s deployment stack is structured around four key pillars:\n\u200b\nDevelopment\nPackage any model using Truss, our open-source framework for defining dependencies, hardware, and custom logic\u2014no Docker required. For more advanced use cases, build compound inference systems using Chains, orchestrating multiple models, APIs, and processing steps.\nDeveloping a model\nPackage and deploy any AI/ML model as an API with Truss or a Custom Server.\nDeveloping a Chain\nBuild multi-model workflows by chaining models, pre/post-processing, and\nbusiness logic.\n\u200b\nDeployment\nDeploy models to dedicated, autoscaling infrastructure. Use Environments for controlled versioning, rollouts, and promotion between staging and production. Support includes scale-to-zero, canary deploys, and structured model management.\n\u200b\nInference\nServe synchronous, asynchronous, and streaming predictions with configurable execution controls. Optimize for latency, throughput, or cost depending on your application\u2019s needs.\n\u200b\nObservability\nMonitor model health and performance with real-time metrics, logs, and detailed request traces. Export data to observability tools like Datadog or Prometheus. Debug behavior with full visibility into inputs, outputs, and errors.\nThis full-stack infrastructure, from packaging to observability, is powered by the\nBaseten Inference Stack\n: performant model runtimes, cross-cloud availability, and seamless developer workflows.\n\u200b\nModel APIs\nModel APIs\noffer a fast, reliable path to production for LLM-powered features. Use OpenAI-compatible endpoints to call performant open-source models like Llama 4, DeepSeek, and Qwen, with support for structured outputs and tool calling.\nIf your code already works with OpenAI\u2019s SDKs, it\u2019ll work with Baseten\u2014no wrappers or rewrites required.\n\u200b\nTraining\nBaseten Training\nprovides scalable infrastructure for running containerized training jobs. Define your code, environment, and compute resources; manage checkpoints and logs; and transition seamlessly from training to deployment.\nOrganize work with TrainingProjects and track reproducible runs via TrainingJobs. Baseten supports any framework, from PyTorch to custom setups, with centralized artifact and job management.\n\u200b\nSummary\nUse\nDedicated Deployments\nto run and scale production-grade models with full control.\nUse\nModel APIs\nto quickly build LLM-powered features without managing infrastructure.\nUse\nTraining\nto run reproducible training jobs and productionize your own models.\nEach product is built on the same core: reliable infrastructure, strong developer ergonomics, and a focus on operational excellence.\nWas this page helpful?\nYes\nNo\nPrevious\nConcepts\nNext\nOn this page\nDedicated deployments\nDevelopment\nDeployment\nInference\nObservability\nModel APIs\nTraining\nSummary\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4244, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bbb16dfa-4df8-45e8-8575-c923b0a7a1e6": {"__data__": {"id_": "bbb16dfa-4df8-45e8-8575-c923b0a7a1e6", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ace96383-91a8-4ce4-9991-d1f8c9dfca7b", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "b2e8bd6089e40da549103bc5954b4d73306e65baa998ccd4547258c9a9c92672", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c431c262-3bd5-4f52-b353-41fbe544193f", "node_type": "1", "metadata": {}, "hash": "928beae36f4c3b8362cea35b5bed0ba40acee27a6dd2f253e123c5838d5a8a78", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Content from https://docs.baseten.co/concepts/whybaseten:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nDeveloping a Chain\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nExporting metrics\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nConcepts\nWhy Baseten\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\n\u200b\nMission-critical inference\nBuilt for high-performance workloads, our platform optimizes inference performance across modalities, from state-of-the-art transcription to blazing-fast LLMs.\nBuilt-in autoscaling, model performance optimizations, and deep observability tools ensure efficiency without complexity.\nTrusted by top ML teams serving their products to millions of users, Baseten accelerates time to market for AI-driven products by building on four key pillars of inference: performance, infrastructure, tooling, and expertise.\n\u200b\nModel performance\nBaseten\u2019s model performance engineers apply the latest research and custom engine optimizations in production, so you get low latency and high throughput out of the box.\nProduction-grade support for critical features, like speculative decoding and LoRA swapping, is baked into our platform.\n\u200b\nCloud-native infrastructure\nDeploy\nand\nscale models\nacross clusters, regions, and clouds with five nines reliability.\nWe built all the orchestration and optimized the network routing to ensure global scalability without the operational complexity.\n\u200b\nModel management tooling\nLove your development ecosystem, with deep\nobservability\nand easy-to-use tools for deploying, managing, and iterating on models in production.\nQuickly serve open-source and custom models, ultra-low-latency compound AI systems, and custom Docker servers in our cloud or yours.\n\u200b\nForward deployed engineering\nBaseten\u2019s expert engineers work as an extension of your team, customizing deployments for your target performance, quality, and cost-efficiency metrics.\nGet hands-on support with deep inference-specific expertise and 24/7 on-call availability.\nWas this page helpful?\nYes\nNo\nPrevious\nHow Baseten works\nBaseten is a platform for building, serving, and scaling AI models in production.\nNext\nAssistant\nResponses are generated using AI and may contain mistakes.\n\n\nContent from https://docs.baseten.co/deployment/autoscaling:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nDeveloping a Chain\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nExporting metrics\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nDeployment\nAutoscaling\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\n\u200b\nConfiguring autoscaling\nAutoscaling settings are\nper deployment\nand are inherited when promoting a model to production unless overridden.\nConfigure autoscaling through:\nUI\n\u2192 Manage settings in your Baseten workspace.\nAPI\n\u2192 Use the\nautoscaling API\n.\n\u200b\nReplica Scaling\nEach deployment scales within a configured range of replicas:\nMinimum replicas\n\u2192 The lowest number of active replicas.\nDefault:\n0\n(scale to zero).\nMaximum value: Cannot exceed the\nmaximum replica count\n.\nMaximum replicas\n\u2192 The upper limit of active replicas.\nDefault:\n1\n.\nMax:\n10\nby default (contact support to increase).\nWhen first deployed, the model starts with\n1\nreplica (or the\nminimum count\n, if higher). As traffic increases, additional replicas\nscale up\nuntil the\nmaximum count\nis reached. When traffic decreases, replicas\nscale down\nto match demand.", "mimetype": "text/plain", "start_char_idx": 4247, "end_char_idx": 8580, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c431c262-3bd5-4f52-b353-41fbe544193f": {"__data__": {"id_": "c431c262-3bd5-4f52-b353-41fbe544193f", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bbb16dfa-4df8-45e8-8575-c923b0a7a1e6", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ea1ebbdb40e526913dc7d06d63246410f4d71359c5905039265451246304b815", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e60d4c04-26e1-4862-bd16-e862f116c1c8", "node_type": "1", "metadata": {}, "hash": "8ace653bdfab41b402fe8bc2126f7cd6cf4f5e66f182dd3d5004dc3eb50f3e00", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Configure autoscaling through:\nUI\n\u2192 Manage settings in your Baseten workspace.\nAPI\n\u2192 Use the\nautoscaling API\n.\n\u200b\nReplica Scaling\nEach deployment scales within a configured range of replicas:\nMinimum replicas\n\u2192 The lowest number of active replicas.\nDefault:\n0\n(scale to zero).\nMaximum value: Cannot exceed the\nmaximum replica count\n.\nMaximum replicas\n\u2192 The upper limit of active replicas.\nDefault:\n1\n.\nMax:\n10\nby default (contact support to increase).\nWhen first deployed, the model starts with\n1\nreplica (or the\nminimum count\n, if higher). As traffic increases, additional replicas\nscale up\nuntil the\nmaximum count\nis reached. When traffic decreases, replicas\nscale down\nto match demand.\n\u200b\nAutoscaler settings\nThe\nautoscaler logic\nis controlled by three key parameters:\nAutoscaling window\n\u2192 Time window for traffic analysis before scaling up/down. Default: 60 seconds.\nScale down delay\n\u2192 Time before an unused replica is removed. Default: 900 seconds (15 minutes).\nConcurrency target\n\u2192 Number of requests a replica should handle before scaling. Default: 1 request.\nA\nshort autoscaling window\nwith a\nlonger scale-down delay\nis recommended for\nfast upscaling\nwhile maintaining capacity during temporary dips.\n\u200b\nAutoscaling behavior\n\u200b\nScaling Up\nWhen the\naverage requests per active replica\nexceed the\nconcurrency target\nwithin the\nautoscaling window\n, more replicas are created until:\nThe\nconcurrency target is met\n, or\nThe\nmaximum replica count\nis reached.\n\u200b\nScaling Down\nWhen traffic drops below the\nconcurrency target\n, excess replicas are flagged for removal. The\nscale-down delay\nensures that replicas are not removed prematurely:\nIf traffic\nspikes again before the delay ends\n, replicas remain active.\nIf the\nminimum replica count\nis reached, no further scaling down occurs.\n\u200b\nScale to zero\nIf you\u2019re just testing your model or anticipate light and inconsistent traffic, scale to zero can save you substantial amounts of money.\nScale to zero means that when a deployed model is not receiving traffic, it scales down to zero replicas. When the model is called, Baseten spins up a new instance to serve model requests.\nTo turn on scale to zero, just set a deployment\u2019s minimum replica count to zero. Scale to zero is enabled by default in the standard autoscaling config.\nModels that have not received any traffic for more than two weeks will be\nautomatically deactivated. These models will need to be activated manually\nbefore they can serve requests again.\n\u200b\nCold starts\nA\ncold start\nis the time required to\ninitialize a new replica\nwhen scaling up. Cold starts impact:\nScaled-to-zero deployments\n\u2192 The first request must wait for a new replica to start.\nScaling events\n\u2192 When traffic spikes and a deployment requires more replicas.\n\u200b\nCold Start Optimizations\nNetwork accelerator\nBaseten speeds up model loading from\nHugging Face, CloudFront, S3, and OpenAI\nusing parallelized\nbyte-range downloads\n, reducing cold start delays.\nCold start pods\nBaseten pre-warms specialized\ncold start pods\nto accelerate loading times. These pods appear in logs as\n[Coldboost]\n.\nExample coldboost log line\nCopy\nAsk AI\nOct 09 9:20:25pm [\nColdboost\n] Completed model.load() execution in 12650 ms\n\u200b\nAutoscaling for development deployments\nDevelopment deployments have\nfixed autoscaling constraints\nto optimize for\nlive reload workflows\n:\nMin replicas:\n0\nMax replicas:\n1\nAutoscaling window:\n60 seconds\nScale down delay:\n900 seconds (15 min)\nConcurrency target:\n1 request\nTo enable full autoscaling,\npromote the deployment and environment\nlike production.\nWas this page helpful?\nYes\nNo\nPrevious\nConcepts\nNext\nOn this page\nConfiguring autoscaling\nReplica Scaling\nAutoscaler settings\nAutoscaling behavior\nScaling Up\nScaling Down\nScale to zero\nCold starts\nCold Start Optimizations\nAutoscaling for development deployments\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 7893, "end_char_idx": 11761, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e60d4c04-26e1-4862-bd16-e862f116c1c8": {"__data__": {"id_": "e60d4c04-26e1-4862-bd16-e862f116c1c8", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c431c262-3bd5-4f52-b353-41fbe544193f", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "1e79b006f00dbd1ff47c8b847cbb0de8a0d5b3ecc6f07742ed84f26fe921ce9d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f3164624-9d62-483b-adfa-22f953d5d72d", "node_type": "1", "metadata": {}, "hash": "ed147d7fa6c203a84bebed4f340e93fe9a604b5f5ecc0965b53868d812008d15", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Example coldboost log line\nCopy\nAsk AI\nOct 09 9:20:25pm [\nColdboost\n] Completed model.load() execution in 12650 ms\n\u200b\nAutoscaling for development deployments\nDevelopment deployments have\nfixed autoscaling constraints\nto optimize for\nlive reload workflows\n:\nMin replicas:\n0\nMax replicas:\n1\nAutoscaling window:\n60 seconds\nScale down delay:\n900 seconds (15 min)\nConcurrency target:\n1 request\nTo enable full autoscaling,\npromote the deployment and environment\nlike production.\nWas this page helpful?\nYes\nNo\nPrevious\nConcepts\nNext\nOn this page\nConfiguring autoscaling\nReplica Scaling\nAutoscaler settings\nAutoscaling behavior\nScaling Up\nScaling Down\nScale to zero\nCold starts\nCold Start Optimizations\nAutoscaling for development deployments\nAssistant\nResponses are generated using AI and may contain mistakes.\n\n\nContent from https://docs.baseten.co/deployment/concepts:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nDeveloping a Chain\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nExporting metrics\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nDeployment\nConcepts\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nBaseten provides a flexible and scalable infrastructure for deploying and managing machine learning models. This page introduces key concepts -\ndeployments\n,\nenvironments\n,\nresources\n, and\nautoscaling\n\u2014 that shape how models are served, tested, and optimized for performance and cost efficiency.\n\u200b\nDeployments\nDeployments\ndefine how models are served, scaled, and updated. They optimize resource use with autoscaling, scaling to zero, and controlled traffic shifts while ensuring minimal downtime. Deployments can be deactivated to pause resource usage or deleted permanently when no longer needed.\n\u200b\nEnvironments\nEnvironments\ngroup deployments, providing stable endpoints and autoscaling to manage model release cycles. They enable structured testing, controlled rollouts, and seamless transitions between staging and production. Each environment maintains its own settings and metrics, ensuring reliable and scalable deployments.\n\u200b\nResources\nResources\ndefine the hardware allocated to a model server, balancing performance and cost. Choosing the right instance type ensures efficient inference without unnecessary overhead. Resources can be set before deployment in Truss or adjusted later in the model dashboard to match workload demands.\n\u200b\nAutoscaling\nAutoscaling\ndynamically adjusts model resources to handle traffic fluctuations efficiently while minimizing costs. Deployments scale between a defined range of replicas based on demand, with settings for concurrency, scaling speed, and scale-to-zero for low-traffic models. Optimizations like network acceleration and cold start pods ensure fast response times even when scaling up from zero.\nWas this page helpful?\nYes\nNo\nPrevious\nDeployments\nDeploy, manage, and scale machine learning models with Baseten\nNext\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 10959, "end_char_idx": 14404, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f3164624-9d62-483b-adfa-22f953d5d72d": {"__data__": {"id_": "f3164624-9d62-483b-adfa-22f953d5d72d", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e60d4c04-26e1-4862-bd16-e862f116c1c8", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "17796c36ee01649bd7b43f01cc956a9c0f090559aadbce9a659f8b8cc2caea1a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6fd714dc-8a91-46f3-bfed-e2e99b2231ad", "node_type": "1", "metadata": {}, "hash": "8a3a69fa169ee17c59985800dd9d80690125f001a37417f1f04e9b2637dfbb62", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Content from https://docs.baseten.co/deployment/deployments:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nDeveloping a Chain\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nExporting metrics\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nDeployment\nDeployments\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nA\ndeployment\nin Baseten is a\ncontainerized instance of a model\nthat serves inference requests via an API endpoint. Deployments exist independently but can be\npromoted to an environment\nfor structured access and scaling.\nEvery deployment is\nautomatically wrapped in a REST API\n. Once deployed, models can be queried with a simple HTTP request:\nCopy\nAsk AI\nimport\nrequests\nresp\n=\nrequests.post(\n\"https://model-\n{modelID}\n.api.baseten.co/deployment/[\n{deploymentID}\n]/predict\"\n,\nheaders\n=\n{\n\"Authorization\"\n:\n\"Api-Key YOUR_API_KEY\"\n},\njson\n=\n{\n'text'\n:\n'Hello my name is\n{MASK}\n'\n},\n)\nprint\n(resp.json())\nLearn more about running inference on your deployment\n\u200b\nDevelopment deployment\nA\ndevelopment deployment\nis a mutable instance designed for rapid iteration. It is always in the\ndevelopment state\nand cannot be renamed or detached from it.\nKey characteristics:\nLive reload\nenables direct updates without redeployment.\nSingle replica, scales to zero\nwhen idle to conserve compute resources.\nNo autoscaling or zero-downtime updates.\nCan be promoted\nto create a persistent deployment.\nOnce promoted, the development deployment transitions to a\ndeployment\nand can optionally be promoted to an environment.\n\u200b\nEnvironments & Promotion\nEnvironments provide\nlogical isolation\nfor managing deployments but are\nnot required\nfor a deployment to function. A deployment can be executed independently or promoted to an environment for controlled traffic allocation and scaling.\nThe\nproduction environment\nexists by default.\nCustom environments\n(e.g., staging) can be created for specific workflows.\nPromoting a deployment does not modify its behavior\n, only its routing and lifecycle management.\n\u200b\nCanary deployments\nCanary deployments support\nincremental traffic shifting\nto a new deployment, mitigating risk during rollouts.\nTraffic is routed in\n10 evenly distributed stages\nover a configurable time window.\nTraffic only begins to shift once the new deployment reaches the min replica count of the current production model.\nAutoscaling dynamically adjusts to real-time demand.\nCanary rollouts can be enabled or canceled via the UI or\nREST API\n.\n\u200b\nManaging Deployments\n\u200b\nNaming deployments\nBy default, deployments of a model are named\ndeployment-1\n,\ndeployment-2\n, and so forth sequentially. You can instead give deployments custom names via two methods:\nWhile creating the deployment, using a\ncommand line argument in truss push\n.\nAfter creating the deployment, in the model management page within your Baseten dashboard.\nRenaming deployments is purely aesthetic and does not affect model management API paths, which work via model and deployment IDs.\n\u200b\nDeactivating a deployment\nA deployment can be deactivated to suspend inference execution while preserving configuration.\nRemains visible in the dashboard.\nConsumes no compute resources\nbut can be reactivated anytime.\nAPI requests return a 404 error while deactivated.\nFor demand-driven deployments, consider\nautoscaling with scale to zero\n.\n\u200b\nDeleting deployments\nDeployments can be\npermanently deleted\n, but production deployments must be replaced before deletion.\nDeleted deployments are purged from the dashboard\nbut retained in usage logs.\nAll associated compute resources are released.\nAPI requests return a 404 error post-deletion.\nDeletion is irreversible \u2014 use deactivation if retention is required.\nWas this page helpful?\nYes\nNo\nPrevious\nEnvironments\nManage your model\u2019s release cycles with environments.\nNext\nOn this page\nDevelopment deployment\nEnvironments & Promotion\nCanary deployments\nManaging Deployments\nNaming deployments\nDeactivating a deployment\nDeleting deployments\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 14407, "end_char_idx": 18912, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6fd714dc-8a91-46f3-bfed-e2e99b2231ad": {"__data__": {"id_": "6fd714dc-8a91-46f3-bfed-e2e99b2231ad", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f3164624-9d62-483b-adfa-22f953d5d72d", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "228e77e75e5524c64d0ea50740f6168b18e844e3d04dbaf4961ba31f351f24bf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "381b91fd-d618-42b1-a8b0-d7fc717cbe1a", "node_type": "1", "metadata": {}, "hash": "09c37a5ba27115792b6f61c80e1ab5ea595279ef1a8a0e37181fdc8fc867e304", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Content from https://docs.baseten.co/deployment/environments:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nDeveloping a Chain\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nExporting metrics\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nDeployment\nEnvironments\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nEnvironments provide structured management for deployments, ensuring controlled rollouts, stable endpoints, and autoscaling. They help teams stage, test, and release models without affecting production traffic.\nDeployments can be promoted to an environment (e.g., \u201cstaging\u201d) to validate outputs before moving to production, allowing for safer model iteration and evaluation.\n\u200b\nUsing Environments to manage deployments\nEnvironments support\nstructured validation\nbefore promoting a deployment, including:\nAutomated tests & evaluations\nManual testing in pre-production\nGradual traffic shifts with canary deployments\nShadow serving for real-world analysis\nPromoting a deployment ensures it inherits\nenvironment-specific scaling and monitoring settings\n, such as:\nDedicated API endpoint\n\u2192\nPredict API Reference\nAutoscaling controls\n\u2192 Scale behavior is managed per environment.\nTraffic ramp-up\n\u2192 Enable\ncanary rollouts\n.\nMonitoring & Metrics\n\u2192\nExport environment metrics\n.\nA\nproduction environment\noperates like any other environment but has restrictions:\nIt cannot be deleted\nunless the entire model is removed.\nYou cannot create additional environments named \u201cproduction.\u201d\n\u200b\nCreating custom environments\nIn addition to the standard\nproduction\nenvironment, you can create as many custom environments as needed. There are two ways to create a custom environment:\nIn the model management page on the Baseten dashboard.\nVia the\ncreate environment endpoint\nin the model management API.\n\u200b\nPromoting deployments to environments\nWhen a deployment is promoted, Baseten follows a\nthree-step process\n:\nA\nnew deployment\nis created with a unique deployment ID.\nThe deployment\ninitializes resources\nand becomes active.\nThe new deployment\nreplaces the existing deployment\nin that environment.\nIf there was\nno previous deployment, default autoscaling settings\nare applied.\nIf a\nprevious deployment existed\n, the new one\ninherits autoscaling settings\n, and the old deployment is\ndemoted and scales to zero\n.\n\u200b\nPromoting a Published Deployment\nIf a\npublished deployment\n(not a development deployment) is promoted:\nIts\nautoscaling settings are updated\nto match the environment.\nIf\ninactive\n, it must be\nactivated\nbefore promotion.\nPrevious deployments are\ndemoted but remain in the system\n, retaining their\ndeployment ID and scaling behavior\n.\n\u200b\nDeploying directly to an environment\nYou can\nskip development stage\nand deploy directly to an environment by specifying\n--environment\nin\ntruss push\n:\nCopy\nAsk AI\ncd\nmy_model/\ntruss\npush\n--environment\n{environment_name}\nOnly one active promotion per environment is allowed at a time.\n\u200b\nAccessing environments in your code\nThe\nenvironment name\nis available in\nmodel.py\nvia the\nenvironment\nkeyword argument:\nCopy\nAsk AI\ndef\n__init__\n(\nself\n,\n**\nkwargs\n):\nself\n._environment\n=\nkwargs[\n\"environment\"\n]\nTo ensure the\nenvironment variable remains updated\n, enable** \u201cRe-deploy when promoting\u201d **in the UI or via the\nREST API\n. This guarantees the environment is fully initialized after a promotion.\n\u200b\nDeleting environments\nEnvironments can be deleted,\nexcept for production\n. To remove a\nproduction deployment\n, first\npromote another deployment to production\nor delete the entire model.\nDeleted environments are removed from the overview\nbut remain in billing history.\nThey do not consume resources\nafter deletion.\nAPI requests to a deleted environment return a 404 error.\nDeletion is permanent - consider deactivation instead.\nWas this page helpful?", "mimetype": "text/plain", "start_char_idx": 18915, "end_char_idx": 23193, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "381b91fd-d618-42b1-a8b0-d7fc717cbe1a": {"__data__": {"id_": "381b91fd-d618-42b1-a8b0-d7fc717cbe1a", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6fd714dc-8a91-46f3-bfed-e2e99b2231ad", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "1fd509256a99277d570c096e63a85b690dd17ce52e8f458e3b171bde500d69d1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f02540d1-dd5b-4373-9e43-75d2b653262f", "node_type": "1", "metadata": {}, "hash": "2a8a6849b7bc66757bc9b9acee985ac123aa48006fec334f79eb4951ac613c70", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u200b\nAccessing environments in your code\nThe\nenvironment name\nis available in\nmodel.py\nvia the\nenvironment\nkeyword argument:\nCopy\nAsk AI\ndef\n__init__\n(\nself\n,\n**\nkwargs\n):\nself\n._environment\n=\nkwargs[\n\"environment\"\n]\nTo ensure the\nenvironment variable remains updated\n, enable** \u201cRe-deploy when promoting\u201d **in the UI or via the\nREST API\n. This guarantees the environment is fully initialized after a promotion.\n\u200b\nDeleting environments\nEnvironments can be deleted,\nexcept for production\n. To remove a\nproduction deployment\n, first\npromote another deployment to production\nor delete the entire model.\nDeleted environments are removed from the overview\nbut remain in billing history.\nThey do not consume resources\nafter deletion.\nAPI requests to a deleted environment return a 404 error.\nDeletion is permanent - consider deactivation instead.\nWas this page helpful?\nYes\nNo\nPrevious\nResources\nManage and configure model resources\nNext\nOn this page\nUsing Environments to manage deployments\nCreating custom environments\nPromoting deployments to environments\nPromoting a Published Deployment\nDeploying directly to an environment\nAccessing environments in your code\nDeleting environments\nAssistant\nResponses are generated using AI and may contain mistakes.\n\n\nContent from https://docs.baseten.co/deployment/resources:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nDeveloping a Chain\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nExporting metrics\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nDeployment\nResources\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nEvery AI/ML model on Baseten runs on an\ninstance\n, a dedicated set of hardware allocated to the model server. Selecting the right instance type ensures\noptimal performance\nwhile controlling\ncompute costs\n.\nInsufficient resources\n\u2192 Slow inference or failures.\nExcess resources\n\u2192 Higher costs without added benefit.\n\u200b\nInstance type resource components\nInstance\n\u2192 The allocated hardware for inference.\nvCPU\n\u2192 Virtual CPU cores for general computing.\nRAM\n\u2192 Memory available to the CPU.\nGPU\n\u2192 Specialized hardware for accelerated ML workloads.\nVRAM\n\u2192 Dedicated GPU memory for model execution.\n\u200b\nConfiguring model resources\nResources can be defined\nbefore deployment\nin Truss or\nadjusted later\nvia the Baseten UI.\n\u200b\nDefining resources in Truss\nDefine resource requirements in\nconfig.yaml\nbefore running\ntruss push\n. Any changes after deployment will not impact previous deployments. Running\ntruss push\nagain will create a new deployment using the resources specified in the\nconfig.yaml\n.\nThe only exception is the\ndevelopment\ndeployment. It will be redeployed with the new specified resources.\nExample (Stable Diffusion XL):\nconfig.yaml\nCopy\nAsk AI\nresources\n:\naccelerator\n:\nA10G\ncpu\n:\n\"4\"\nmemory\n:\n16Gi\nuse_gpu\n:\ntrue\nBaseten provisions the\nsmallest instance that meets the specified constraints\n:\n**cpu: \u201c3\u201d or \u201c4\u201d \u2192 **Maps to a 4-core instance.\n**cpu: \u201c5\u201d to \u201c8\u201d \u2192 **Maps to an 8-core instance.\nGi\nin\nresources.memory\nrefers to\nGibibytes\n, which are slightly larger\nthan\nGigabytes\n.\n\u200b\nUpdating resources in the Baseten UI\nOnce deployed, resource configurations can only be updated\nthrough the Baseten UI\n. Changing the instance type will deploy a new copy of the deployment using the new specified instance type.\nLike when running\ntruss push\n, the\ndevelopment\ndeployment will be redeployed with the new specified instance type.\nFor a list of available instance types, see the\ninstance type reference\n.\n\u200b\nInstance Type Reference\nSpecs and benchmarks for every Baseten instance type.\nChoosing the right instance for model inference means balancing performance and cost. This page lists all available instance types on Baseten to help you deploy and serve models effectively.", "mimetype": "text/plain", "start_char_idx": 22333, "end_char_idx": 26557, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f02540d1-dd5b-4373-9e43-75d2b653262f": {"__data__": {"id_": "f02540d1-dd5b-4373-9e43-75d2b653262f", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "381b91fd-d618-42b1-a8b0-d7fc717cbe1a", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ebc382906565871e1171706ba4dbcad53059445c60f9e998773eadb524bab538", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "faa7df87-7b97-48e9-9f1e-9851891fbb60", "node_type": "1", "metadata": {}, "hash": "84bf0a3ff32064c6f3643a560ca3007b5b104f6384a757a9d3b807320d4fba47", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**cpu: \u201c5\u201d to \u201c8\u201d \u2192 **Maps to an 8-core instance.\nGi\nin\nresources.memory\nrefers to\nGibibytes\n, which are slightly larger\nthan\nGigabytes\n.\n\u200b\nUpdating resources in the Baseten UI\nOnce deployed, resource configurations can only be updated\nthrough the Baseten UI\n. Changing the instance type will deploy a new copy of the deployment using the new specified instance type.\nLike when running\ntruss push\n, the\ndevelopment\ndeployment will be redeployed with the new specified instance type.\nFor a list of available instance types, see the\ninstance type reference\n.\n\u200b\nInstance Type Reference\nSpecs and benchmarks for every Baseten instance type.\nChoosing the right instance for model inference means balancing performance and cost. This page lists all available instance types on Baseten to help you deploy and serve models effectively.\n\u200b\nCPU-only Instances\nCost-effective options for lighter workloads. No GPU.\nStarts at\n: $0.00058/min\nBest for\n: Transformers pipelines, small QA models, text embeddings\nInstance\n$/min\nvCPU\nRAM\n1\u00d72\n$0.00058\n1\n2 GiB\n1\u00d74\n$0.00086\n1\n4 GiB\n2\u00d78\n$0.00173\n2\n8 GiB\n4\u00d716\n$0.00346\n4\n16 GiB\n8\u00d732\n$0.00691\n8\n32 GiB\n16\u00d764\n$0.01382\n16\n64 GiB\nExample workloads:\n1x2\n: Text classification (e.g., Truss quickstart)\n4x16\n: LayoutLM Document QA\n4x16+\n: Sentence Transformers embeddings on larger corpora\n\u200b\nGPU Instances\nAccelerated inference for LLMs, diffusion models, and Whisper.\nInstance\n$/min\nvCPU\nRAM\nGPU\nVRAM\nT4x4x16\n$0.01052\n4\n16 GiB\nNVIDIA T4\n16 GiB\nT4x8x32\n$0.01504\n8\n32 GiB\nNVIDIA T4\n16 GiB\nT4x16x64\n$0.02408\n16\n64 GiB\nNVIDIA T4\n16 GiB\nL4x4x16\n$0.01414\n4\n16 GiB\nNVIDIA L4\n24 GiB\nL4:2x4x16\n$0.04002\n24\n96 GiB\n2 NVIDIA L4s\n48 GiB\nL4:4x48x192\n$0.08003\n48\n192 GiB\n4 NVIDIA L4s\n96 GiB\nA10Gx4x16\n$0.02012\n4\n16 GiB\nNVIDIA A10G\n24 GiB\nA10Gx8x32\n$0.02424\n8\n32 GiB\nNVIDIA A10G\n24 GiB\nA10Gx16x64\n$0.03248\n16\n64 GiB\nNVIDIA A10G\n24 GiB\nA10G:2x24x96\n$0.05672\n24\n96 GiB\n2 NVIDIA A10Gs\n48 GiB\nA10G:4x48x192\n$0.11344\n48\n192 GiB\n4 NVIDIA A10Gs\n96 GiB\nA10G:8x192x768\n$0.32576\n192\n768 GiB\n8 NVIDIA A10Gs\n188 GiB\nV100x8x61\n$0.06120\n16\n61 GiB\nNVIDIA V100\n16 GiB\nA100x12x144\n$0.10240\n12\n144 GiB\n1 NVIDIA A100\n80 GiB\nA100:2x24x288\n$0.20480\n24\n288 GiB\n2 NVIDIA A100s\n160 GiB\nA100:3x36x432\n$0.30720\n36\n432 GiB\n3 NVIDIA A100s\n240 GiB\nA100:4x48x576\n$0.40960\n48\n576 GiB\n4 NVIDIA A100s\n320 GiB\nA100:5x60x720\n$0.51200\n60\n720 GiB\n5 NVIDIA A100s\n400 GiB\nA100:6x72x864\n$0.", "mimetype": "text/plain", "start_char_idx": 25730, "end_char_idx": 28086, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "faa7df87-7b97-48e9-9f1e-9851891fbb60": {"__data__": {"id_": "faa7df87-7b97-48e9-9f1e-9851891fbb60", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f02540d1-dd5b-4373-9e43-75d2b653262f", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "0c69a4c77e59d7a57f5455ddc8b151fe5c04bf54c5b24ad121b74a17ae355fa6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "03d906b2-0f58-4471-aa57-512d430b1a8d", "node_type": "1", "metadata": {}, "hash": "805815e1d8eaf7cbf66b5f3f91c2e1652ad0da922de9475e105c013e02486781", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "06120\n16\n61 GiB\nNVIDIA V100\n16 GiB\nA100x12x144\n$0.10240\n12\n144 GiB\n1 NVIDIA A100\n80 GiB\nA100:2x24x288\n$0.20480\n24\n288 GiB\n2 NVIDIA A100s\n160 GiB\nA100:3x36x432\n$0.30720\n36\n432 GiB\n3 NVIDIA A100s\n240 GiB\nA100:4x48x576\n$0.40960\n48\n576 GiB\n4 NVIDIA A100s\n320 GiB\nA100:5x60x720\n$0.51200\n60\n720 GiB\n5 NVIDIA A100s\n400 GiB\nA100:6x72x864\n$0.61440\n72\n864 GiB\n6 NVIDIA A100s\n480 GiB\nA100:7x84x1008\n$0.71680\n84\n1008 GiB\n7 NVIDIA A100s\n560 GiB\nA100:8x96x1152\n$0.81920\n96\n1152 GiB\n8 NVIDIA A100s\n640 GiB\nH100x26x234\n$0.16640\n26\n234 GiB\n1 NVIDIA H100\n80 GiB\nH100:2x52x468\n$0.33280\n52\n468 GiB\n2 NVIDIA H100s\n160 GiB\nH100:4x104x936\n$0.66560\n104\n936 GiB\n4 NVIDIA H100s\n320 GiB\nH100:8x208x1872\n$1.33120\n208\n1872 GiB\n8 NVIDIA H100s\n640 GiB\nH100MIG:3gx13x117\n$0.08250\n13\n117 GiB\nFractional NVIDIA H100\n40 GiB\n\u200b\nGPU Details & Workloads\n\u200b\nT4\nTuring-series GPU\n2,560 CUDA / 320 Tensor cores\n16 GiB VRAM\nBest for:\nWhisper, small LLMs like StableLM 3B\n\u200b\nL4\nAda Lovelace-series GPU\n7,680 CUDA / 240 Tensor cores\n24 GiB VRAM, 300 GiB/s\n24 GiB VRAM, 300 GiB/s\n121 TFLOPS (fp16)\nBest for\n: Stable Diffusion XL\nLimit\n: Not suitable for LLMs due to bandwidth\n\u200b\nA10G\nAmpere-series GPU\n9,216 CUDA / 288 Tensor cores\n24 GiB VRAM, 600 GiB/s\n70 TFLOPS (fp16)\nBest for\n: Mistral 7B, Whisper, Stable Diffusion/SDXL\n\u200b\nV100\nVolta-series GPU\n16 GiB VRAM\nBest for\n: Legacy workloads needing V100-specific support\n\u200b\nA100\nAmpere-series GPU\n6,912 CUDA / 432 Tensor cores\n80 GiB VRAM, 1.94 TB/s\n312 TFLOPS (fp16)\nBest for\n: Mixtral, Llama 2 70B (2 A100s), Falcon 180B (5 A100s), SDXL\n\u200b\nH100\nHopper-series GPU\n16,896 CUDA / 640 Tensor cores\n80 GiB VRAM, 3.35 TB/s\n990 TFLOPS (fp16)\nBest for\n: Mixtral 8x7B, Llama 2 70B (2\u00d7H100), SDXL\n\u200b\nH100MIG\nFractional H100 (3/7 compute, \u00bd memory)\n7,242 CUDA cores, 40 GiB VRAM\n1.675 TB/s bandwidth\nBest for\n: Efficient LLM inference at lower cost than A100\nWas this page helpful?\nYes\nNo\nPrevious\nAutoscaling\nAutoscaling dynamically adjusts the number of active replicas to **handle variable traffic** while minimizing idle compute costs.\nNext\nOn this page\nInstance type resource components\nConfiguring model resources\nDefining resources in Truss\nUpdating resources in the Baseten UI\nInstance Type Reference\nCPU-only Instances\nGPU Instances\nGPU Details & Workloads\nT4\nL4\nA10G\nV100\nA100\nH100\nH100MIG\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 27753, "end_char_idx": 30109, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "03d906b2-0f58-4471-aa57-512d430b1a8d": {"__data__": {"id_": "03d906b2-0f58-4471-aa57-512d430b1a8d", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "faa7df87-7b97-48e9-9f1e-9851891fbb60", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "d5a0e6fabe1ff6f67e90c5cc7484a7f779ceb155c9df19c0eea5430edca6497d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fa4cef61-df1f-482f-91cd-afb36144c385", "node_type": "1", "metadata": {}, "hash": "8b3eab95fb15edcef5be4f288a47b09f602b4476bb35f3cd5e1389b5bfaf1628", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Llama 2 70B (2\u00d7H100), SDXL\n\u200b\nH100MIG\nFractional H100 (3/7 compute, \u00bd memory)\n7,242 CUDA cores, 40 GiB VRAM\n1.675 TB/s bandwidth\nBest for\n: Efficient LLM inference at lower cost than A100\nWas this page helpful?\nYes\nNo\nPrevious\nAutoscaling\nAutoscaling dynamically adjusts the number of active replicas to **handle variable traffic** while minimizing idle compute costs.\nNext\nOn this page\nInstance type resource components\nConfiguring model resources\nDefining resources in Truss\nUpdating resources in the Baseten UI\nInstance Type Reference\nCPU-only Instances\nGPU Instances\nGPU Details & Workloads\nT4\nL4\nA10G\nV100\nA100\nH100\nH100MIG\nAssistant\nResponses are generated using AI and may contain mistakes.\n\n\nContent from https://docs.baseten.co/development/chain/binaryio:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nDeveloping a Chain\nOverview\nConcepts\nYour first Chain\nArchitecture & Design\nLocal Development\nDeploy\nInvocation\nWatch\nSubclassing\nStreaming\nBinary IO\nError Handling\nTruss Integration\nEngine Builder Models\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nExporting metrics\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nDeveloping a Chain\nBinary IO\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nNumeric data or audio/video are most efficiently transmitted as bytes.\nOther representations such as JSON or base64 encoding loose precision, add\nsignificant parsing overhead and increase message sizes (e.g. ~33% increase\nfor base64 encoding).\nChains extends the JSON-centred pydantic ecosystem with two ways how you can\ninclude binary data: numpy array support and raw bytes.\n\u200b\nNumpy\nndarray\nsupport\nOnce you have your data represented as a numpy array, you can easily (and\noften without copying) convert it to\ntorch\n,\ntensorflow\nor other common\nnumeric library\u2019s objects.\nTo include numpy arrays in a pydantic model, chains has a special field type\nimplementation\nNumpyArrayField\n. For example:\nCopy\nAsk AI\nimport\nnumpy\nas\nnp\nimport\npydantic\nfrom\ntruss_chains\nimport\npydantic_numpy\nclass\nDataModel\n(\npydantic\n.\nBaseModel\n):\nsome_numbers: pydantic_numpy.NumpyArrayField\nother_field:\nstr\n...\nnumbers\n=\nnp.random.random((\n3\n,\n2\n))\ndata\n=\nDataModel(\nsome_numbers\n=\nnumbers,\nother_field\n=\n\"Example\"\n)\nprint\n(data)\n# some_numbers=NumpyArrayField(shape=(3, 2), dtype=float64, data=[\n#   [0.39595027 0.23837526]\n#   [0.56714894 0.61244946]\n#   [0.45821942 0.42464844]])\n# other_field='Example'\nNumpyArrayField\nis a wrapper around the actual numpy array. Inside your\npython code, you can work with its\narray\nattribute:\nCopy\nAsk AI\ndata.some_numbers.array\n+=\n10\n# some_numbers=NumpyArrayField(shape=(3, 2), dtype=float64, data=[\n#   [10.39595027 10.23837526]\n#   [10.56714894 10.61244946]\n#   [10.45821942 10.42464844]])\n# other_field='Example'\nThe interesting part is, how it serializes when making communicating between\nChainlets or with a client.\nIt can work in two modes: JSON and binary.\n\u200b\nBinary\nAs an JSON alternative that supports byte data, Chains uses\nmsgpack\n(with\nmsgpack_numpy\n) to serialize the dict representation.", "mimetype": "text/plain", "start_char_idx": 29413, "end_char_idx": 32939, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fa4cef61-df1f-482f-91cd-afb36144c385": {"__data__": {"id_": "fa4cef61-df1f-482f-91cd-afb36144c385", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "03d906b2-0f58-4471-aa57-512d430b1a8d", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "7b976f9c224c426dacaf3086bbc8c926a019895989d51642227829b47301e287", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fc885578-181b-4524-87c2-a6101aad791f", "node_type": "1", "metadata": {}, "hash": "f40c413edc98b1049dd91525dfb86ecfbd2290228b602a18a21a008c2a1b5d45", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Inside your\npython code, you can work with its\narray\nattribute:\nCopy\nAsk AI\ndata.some_numbers.array\n+=\n10\n# some_numbers=NumpyArrayField(shape=(3, 2), dtype=float64, data=[\n#   [10.39595027 10.23837526]\n#   [10.56714894 10.61244946]\n#   [10.45821942 10.42464844]])\n# other_field='Example'\nThe interesting part is, how it serializes when making communicating between\nChainlets or with a client.\nIt can work in two modes: JSON and binary.\n\u200b\nBinary\nAs an JSON alternative that supports byte data, Chains uses\nmsgpack\n(with\nmsgpack_numpy\n) to serialize the dict representation.\nFor Chainlet-Chainlet RPCs this is done automatically for you by enabling binary\nmode of the dependency Chainlets, see\nall options\n:\nCopy\nAsk AI\nimport\ntruss_chains\nas\nchains\nclass\nWorker\n(\nchains\n.\nChainletBase\n):\nasync\ndef\nrun_remote\n(\nself\n,\ndata\n: DataModel) -> DataModel:\ndata.some_numbers.array\n+=\n10\nreturn\ndata\nclass\nConsumer\n(\nchains\n.\nChainletBase\n):\ndef\n__init__\n(\nself\n,\nworker\n=\nchains.depends(Worker,\nuse_binary\n=\nTrue\n)):\nself\n._worker\n=\nworker\nasync\ndef\nrun_remote\n(\nself\n):\nnumbers\n=\nnp.random.random((\n3\n,\n2\n))\ndata\n=\nDataModel(\nsome_numbers\n=\nnumbers,\nother_field\n=\n\"Example\"\n)\nresult\n=\nawait\nself\n._worker.run_remote(data)\nNow the data is transmitted in a fast and compact way between Chainlets\nwhich often gives performance increases.\n\u200b\nBinary client\nIf you want to send such data as input to a chain or parse binary output\nfrom a chain, you have to add the\nmsgpack\nserialization client-side:\nCopy\nAsk AI\nimport\nrequests\nimport\nmsgpack\nimport\nmsgpack_numpy\nmsgpack_numpy.patch()\n# Register hook for numpy.\n# Dump to \"python\" dict and then to binary.\ndata_dict\n=\ndata.model_dump(\nmode\n=\n\"python\"\n)\ndata_bytes\n=\nmsgpack.dumps(data_dict)\n# Set binary content type in request header.\nheaders\n=\n{\n\"Content-Type\"\n:\n\"application/octet-stream\"\n,\n\"Authorization\"\n:\n...\n}\nresponse\n=\nrequests.post(url,\ndata\n=\ndata_bytes,\nheaders\n=\nheaders)\nresponse_dict\n=\nmsgpack.loads(response.content)\nresponse_model\n=\nResponseModel.model_validate(response_dict)\nThe steps of dumping from a pydantic model and validating the response dict\ninto a pydantic model can be skipped, if you prefer working with raw dicts\non the client.\nThe implementation of\nNumpyArrayField\nonly needs\npydantic\n, no other Chains\ndependencies. So you can take that implementation code in isolation and\nintegrated it in your client code.\nSome version combinations of\nmsgpack\nand\nmsgpack_numpy\ngive errors, we\nknow that\nmsgpack = \">=1.0.2\"\nand\nmsgpack-numpy = \">=0.4.8\"\nwork.\n\u200b\nJSON\nThe JSON-schema to represent the array is a dict of\nshape (tuple[int]),  dtype (str), data_b64 (str)\n. E.g.\nCopy\nAsk AI\nprint\n(data.model_dump_json())\n'{\"some_numbers\":{\"shape\":[3,2],\"dtype\":\"float64\", \"data_b64\":\"30d4/rnKJEAsvm...'\nThe base64 data corresponds to\nnp.ndarray.tobytes()\n.\nTo get back to the array from the JSON string, use the model\u2019s\nmodel_validate_json\nmethod.\nAs discussed in the beginning, this schema is not performant for numeric data\nand only offered as a compatibility layer (JSON does not allow bytes) -\ngenerally prefer the binary format.\n\u200b\nSimple\nbytes\nfields\nIt is possible to add a\nbytes\nfield to a pydantic model used in a chain,\nor as a plain argument to\nrun_remote\n. This can be useful to include\nnon-numpy data formats such as images or audio/video snippets.\nIn this case, the \u201cnormal\u201d JSON representation does not work and all\ninvolved requests or Chainlet-Chainlet-invocations must use binary mode.\nThe same steps as for arrays\nabove\napply: construct dicts\nwith\nbytes\nvalues and keys corresponding to the\nrun_remote\nargument\nnames or the field names in the pydantic model. Then use\nmsgpack\nto\nserialize and deserialize those dicts.", "mimetype": "text/plain", "start_char_idx": 32366, "end_char_idx": 36058, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fc885578-181b-4524-87c2-a6101aad791f": {"__data__": {"id_": "fc885578-181b-4524-87c2-a6101aad791f", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fa4cef61-df1f-482f-91cd-afb36144c385", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "bcc4601b7e735581c0784e3a1d82554a0388b51ec390f441a5ad1f76c46d238f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d0421a7e-b2dd-48a9-922f-1a320b86dc68", "node_type": "1", "metadata": {}, "hash": "9009f6d70974ee9b6d872a922961131b86e65ff2dc6b0199469806354b27052f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As discussed in the beginning, this schema is not performant for numeric data\nand only offered as a compatibility layer (JSON does not allow bytes) -\ngenerally prefer the binary format.\n\u200b\nSimple\nbytes\nfields\nIt is possible to add a\nbytes\nfield to a pydantic model used in a chain,\nor as a plain argument to\nrun_remote\n. This can be useful to include\nnon-numpy data formats such as images or audio/video snippets.\nIn this case, the \u201cnormal\u201d JSON representation does not work and all\ninvolved requests or Chainlet-Chainlet-invocations must use binary mode.\nThe same steps as for arrays\nabove\napply: construct dicts\nwith\nbytes\nvalues and keys corresponding to the\nrun_remote\nargument\nnames or the field names in the pydantic model. Then use\nmsgpack\nto\nserialize and deserialize those dicts.\nDon\u2019t forget to add\nContent-type\nheaders and that\nresponse.json()\nwill\nnot work.\nWas this page helpful?\nYes\nNo\nPrevious\nError Handling\nUnderstanding and handling Chains errors\nNext\nOn this page\nNumpy ndarray support\nBinary\nBinary client\nJSON\nSimple bytes fields\nAssistant\nResponses are generated using AI and may contain mistakes.\n\n\nContent from https://docs.baseten.co/development/chain/concepts:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nDeveloping a Chain\nOverview\nConcepts\nYour first Chain\nArchitecture & Design\nLocal Development\nDeploy\nInvocation\nWatch\nSubclassing\nStreaming\nBinary IO\nError Handling\nTruss Integration\nEngine Builder Models\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nExporting metrics\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nDeveloping a Chain\nConcepts\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\n\u200b\nChainlet\nA Chainlet is the basic building block of Chains. A Chainlet is a Python class\nthat specifies:\nA set of compute resources.\nA Python environment with software dependencies.\nA typed interface\nrun_remote()\nfor other Chainlets to call.\nThis is the simplest possible Chainlet \u2014 only the\nrun_remote()\nmethod is\nrequired \u2014 and we can layer in other concepts to create a more capable Chainlet.\nCopy\nAsk AI\nimport\ntruss_chains\nas\nchains\nclass\nSayHello\n(\nchains\n.\nChainletBase\n):\nasync\ndef\nrun_remote\n(\nself\n,\nname\n:\nstr\n) ->\nstr\n:\nreturn\nf\n\"Hello,\n{\nname\n}\n\"\nYou can modularize your code by creating your own chainlet sub-classes,\nrefer to our\nsubclassing guide\n.\n\u200b\nRemote configuration\nChainlets are meant for deployment as remote services. Each Chainlet specifies\nits own requirements for compute hardware (CPU count, GPU type and count, etc)\nand software dependencies (Python libraries or system packages). This\nconfiguration is built into a Docker image automatically as part of the\ndeployment process.\nWhen no configuration is provided, the Chainlet will be deployed on a basic\ninstance with one vCPU, 2GB of RAM, no GPU, and a standard set of Python and\nsystem packages.\nConfiguration is set using the\nremote_config\nclass variable\nwithin the Chainlet:\nCopy\nAsk AI\nimport\ntruss_chains\nas\nchains\nclass\nMyChainlet\n(\nchains\n.\nChainletBase\n):\nremote_config\n=\nchains.RemoteConfig(\ndocker_image\n=\nchains.DockerImage(\npip_requirements\n=\n[\n\"torch==2.3.0\"\n,\n...\n]\n),\ncompute\n=\nchains.Compute(\ngpu\n=\n\"H100\"\n,\n...\n),\nassets\n=\nchains.Assets(\nsecret_keys\n=\n[\n\"hf_access_token\"\n],\n...\n),\n)\nSee the\nremote configuration reference\nfor a complete list of options.\n\u200b\nInitialization\nChainlets are implemented as classes because we often want to set up expensive\nstatic resources once at startup and then re-use it with each invocation of the\nChainlet.", "mimetype": "text/plain", "start_char_idx": 35271, "end_char_idx": 39236, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d0421a7e-b2dd-48a9-922f-1a320b86dc68": {"__data__": {"id_": "d0421a7e-b2dd-48a9-922f-1a320b86dc68", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fc885578-181b-4524-87c2-a6101aad791f", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "4fe28bd06a7bd003bf0ef1e31520ffdfb9414568d99fe1c1e37be0058b85ff94", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e97a38de-0c61-4e11-a134-cf6a3494c356", "node_type": "1", "metadata": {}, "hash": "5628bbcad8ee98fd8c0de2827524d764abd42da0245aefaf7d4abfc0bdfc338d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Configuration is set using the\nremote_config\nclass variable\nwithin the Chainlet:\nCopy\nAsk AI\nimport\ntruss_chains\nas\nchains\nclass\nMyChainlet\n(\nchains\n.\nChainletBase\n):\nremote_config\n=\nchains.RemoteConfig(\ndocker_image\n=\nchains.DockerImage(\npip_requirements\n=\n[\n\"torch==2.3.0\"\n,\n...\n]\n),\ncompute\n=\nchains.Compute(\ngpu\n=\n\"H100\"\n,\n...\n),\nassets\n=\nchains.Assets(\nsecret_keys\n=\n[\n\"hf_access_token\"\n],\n...\n),\n)\nSee the\nremote configuration reference\nfor a complete list of options.\n\u200b\nInitialization\nChainlets are implemented as classes because we often want to set up expensive\nstatic resources once at startup and then re-use it with each invocation of the\nChainlet. For example, we only want to initialize an AI model and download its\nweights once then re-use it every time we run inference.\nWe do this setup in\n__init__()\n, which is run exactly once when the Chainlet is\ndeployed or scaled up.\nCopy\nAsk AI\nimport\ntruss_chains\nas\nchains\nclass\nPhiLLM\n(\nchains\n.\nChainletBase\n):\ndef\n__init__\n(\nself\n) ->\nNone\n:\nimport\ntorch\nimport\ntransformers\nself\n._model\n=\ntransformers.AutoModelForCausalLM.from_pretrained(\nPHI_HF_MODEL\n,\ntorch_dtype\n=\ntorch.float16,\ndevice_map\n=\n\"auto\"\n,\n)\nself\n._tokenizer\n=\ntransformers.AutoTokenizer.from_pretrained(\nPHI_HF_MODEL\n,\n)\nChainlet initialization also has two important features: context and dependency\ninjection of other Chainlets, explained below.\n\u200b\nContext (access information)\nYou can add\nDeploymentContext\nobject as an optional argument to the\n__init__\n-method of a Chainlet.\nThis allows you to use secrets within your Chainlet, such as using\na\nhf_access_token\nto access a gated model on Hugging Face (note that when\nusing secrets, they also need to be added to the\nassets\n).\nCopy\nAsk AI\nimport\ntruss_chains\nas\nchains\nclass\nMistralLLM\n(\nchains\n.\nChainletBase\n):\nremote_config\n=\nchains.RemoteConfig(\n...\nassets\n=\nchains.Assets(\nsecret_keys\n=\n[\n\"hf_access_token\"\n],\n...\n),\n)\ndef\n__init__\n(\nself\n,\n# Adding the `context` argument, allows us to access secrets\ncontext\n: chains.DeploymentContext\n=\nchains.depends_context(),\n) ->\nNone\n:\nimport\ntransformers\n# Using the secret from context to access a gated model on HF\nself\n._model\n=\ntransformers.AutoModelForCausalLM.from_pretrained(\n\"mistralai/Mistral-7B-Instruct-v0.2\"\n,\nuse_auth_token\n=\ncontext.secrets[\n\"hf_access_token\"\n],\n)\n\u200b\nDepends (call other Chainlets)\nThe Chains framework uses the\nchains.depends()\nfunction in\nChainlets\u2019\n__init__()\nmethod to track the dependency relationship between\ndifferent Chainlets within a Chain.\nThis syntax, inspired by dependency injection, is used to translate local Python\nfunction calls into calls to the remote Chainlets in production.\nOnce a dependency Chainlet is added with\nchains.depends()\n, its\nrun_remote()\nmethod can\ncall this dependency Chainlet, e.g. below\nHelloAll\nwe can make calls to\nSayHello\n:\nCopy\nAsk AI\nimport\ntruss_chains\nas\nchains\nclass\nHelloAll\n(\nchains\n.\nChainletBase\n):\ndef\n__init__\n(\nself\n,\nsay_hello_chainlet\n=\nchains.depends(SayHello)) ->\nNone\n:\nself\n._say_hello\n=\nsay_hello_chainlet\nasync\ndef\nrun_remote\n(\nself\n,\nnames\n: list[\nstr\n]) ->\nstr\n:\noutput\n=\n[]\nfor\nname\nin\nnames:\noutput.append(\nself\n._say_hello.run_remote(name))\nreturn\n\"\n\\n\n\"\n.join(output)\n\u200b\nRun remote (chaining Chainlets)\nThe\nrun_remote()\nmethod is run each time the Chainlet is called. It is the\nsole public interface for the Chainlet (though you can have as many private\nhelper functions as you want) and its inputs and outputs must have type\nannotations.\nIn\nrun_remote()\nyou implement the actual work of the Chainlet, such as model\ninference or data chunking:\nCopy\nAsk AI\nimport\ntruss_chains\nas\nchains\nclass\nPhiLLM\n(\nchains\n.", "mimetype": "text/plain", "start_char_idx": 38576, "end_char_idx": 42213, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e97a38de-0c61-4e11-a134-cf6a3494c356": {"__data__": {"id_": "e97a38de-0c61-4e11-a134-cf6a3494c356", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d0421a7e-b2dd-48a9-922f-1a320b86dc68", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "37c9eefa03e6fbc882a330552584d08eb26798a20551d10ef9bae97ba9484264", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5344fe2a-1777-4a59-8545-83d8a02b0d01", "node_type": "1", "metadata": {}, "hash": "dfd65b9b98b2f73643aa6db52a1334c0e07d6530a9e25079acff8ba0597c35e3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It is the\nsole public interface for the Chainlet (though you can have as many private\nhelper functions as you want) and its inputs and outputs must have type\nannotations.\nIn\nrun_remote()\nyou implement the actual work of the Chainlet, such as model\ninference or data chunking:\nCopy\nAsk AI\nimport\ntruss_chains\nas\nchains\nclass\nPhiLLM\n(\nchains\n.\nChainletBase\n):\nasync\ndef\nrun_remote\n(\nself\n,\nmessages\n: Messages) ->\nstr\n:\nimport\ntorch\nmodel_inputs\n=\nawait\nself\n._tokenizer.apply_chat_template(\nmessages,\ntokenize\n=\nFalse\n,\nadd_generation_prompt\n=\nTrue\n)\ninputs\n=\nawait\nself\n._tokenizer(model_inputs,\nreturn_tensors\n=\n\"pt\"\n)\ninput_ids\n=\ninputs[\n\"input_ids\"\n].to(\n\"cuda\"\n)\nwith\ntorch.no_grad():\noutputs\n=\nawait\nself\n._model.generate(\ninput_ids\n=\ninput_ids,\n**\nself\n._generate_args)\noutput_text\n=\nawait\nself\n._tokenizer.decode(\noutputs[\n0\n],\nskip_special_tokens\n=\nTrue\n)\nreturn\noutput_text\nWe recommend implementing this as an\nasync\nmethod and using async APIs for\ndoing all the work (e.g. downloads, vLLM or TRT inference).\nIt is possible to stream results back, see our\nstreaming guide\n.\nIf\nrun_remote()\nmakes calls to other Chainlets, e.g. invoking a dependency\nChainlet for each element in a list, you can benefit from concurrent\nexecution, by making the\nrun_remote()\nan\nasync\nmethod and starting the\ncalls as concurrent tasks\nasyncio.ensure_future(self._dep_chainlet.run_remote(...))\n.\n\u200b\nEntrypoint\nThe entrypoint is called directly from the deployed Chain\u2019s API endpoint and\nkicks off the entire chain. The entrypoint is also responsible for returning the\nfinal result back to the client.\nUsing the\n@chains.mark_entrypoint\ndecorator, one Chainlet within a file is set as the entrypoint to the chain.\nCopy\nAsk AI\n@chains.mark_entrypoint\nclass\nHelloAll\n(\nchains\n.\nChainletBase\n):\nOptionally you can also set a Chain display name (not to be confused with\nChainlet display name) with this decorator:\nCopy\nAsk AI\n@chains.mark_entrypoint\n(\n\"My Awesome Chain\"\n)\nclass\nHelloAll\n(\nchains\n.\nChainletBase\n):\n\u200b\nI/O and\npydantic\ndata types\nTo make orchestrating multiple remotely deployed services possible, Chains\nrelies heavily on typed inputs and outputs. Values must be serialized to a safe\nexchange format to be sent over the network.\nThe Chains framework uses the type annotations to infer how data should be\nserialized and currently is restricted to types that are JSON compatible. Types\ncan be:\nDirect type annotations for simple types such as\nint\n,\nfloat\n,\nor\nlist[str]\n.\nPydantic models to define a schema for nested data structures or multiple\narguments.\nAn example of pydantic input and output types for a Chainlet is given below:\nCopy\nAsk AI\nimport\nenum\nimport\npydantic\nclass\nModes\n(\nenum\n.\nEnum\n):\nMODE_0\n=\n\"MODE_0\"\nMODE_1\n=\n\"MODE_1\"\nclass\nSplitTextInput\n(\npydantic\n.\nBaseModel\n):\ndata:\nstr\nnum_partitions:\nint\nmode: Modes\nclass\nSplitTextOutput\n(\npydantic\n.\nBaseModel\n):\nparts: list[\nstr\n]\npart_lens: list[\nint\n]\nRefer to the\npydantic docs\nfor more\ndetails on how\nto define custom pydantic data models.\nAlso refer to the\nguide\nabout efficient integration\nof binary and numeric data.\n\u200b\nChains compared to Truss\nTips for Truss users\nChains is an alternate SDK for packaging and deploying AI models. It carries over many features and concepts from Truss and gives you access to the benefits of Baseten (resource provisioning, autoscaling, fast cold starts, etc), but it is not a 1-1 replacement for Truss.\nHere are some key differences:\nRather than running\ntruss init\nand creating a Truss in a directory, a Chain\nis a single file, giving you more flexibility for implementing multi-step\nmodel inference. Create an example with\ntruss chains init\n.\nConfiguration is done inline in typed Python code rather than in a\nconfig.yaml\nfile.\nWhile Chainlets are converted to Truss models when run on Baseten,\nChainlet != TrussModel\n.", "mimetype": "text/plain", "start_char_idx": 41872, "end_char_idx": 45692, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5344fe2a-1777-4a59-8545-83d8a02b0d01": {"__data__": {"id_": "5344fe2a-1777-4a59-8545-83d8a02b0d01", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e97a38de-0c61-4e11-a134-cf6a3494c356", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "eee7c1c7005b61fce56a2a951ba13d3d568c19cb4c0eff505fe8c4d8ca2f82ae", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "61c4479f-e0a9-4972-9b5a-45dbbc87098f", "node_type": "1", "metadata": {}, "hash": "2a2ff85a1a05ce328784c89ac9fb603d5da5bc41985982b4d76d451ae515b963", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Also refer to the\nguide\nabout efficient integration\nof binary and numeric data.\n\u200b\nChains compared to Truss\nTips for Truss users\nChains is an alternate SDK for packaging and deploying AI models. It carries over many features and concepts from Truss and gives you access to the benefits of Baseten (resource provisioning, autoscaling, fast cold starts, etc), but it is not a 1-1 replacement for Truss.\nHere are some key differences:\nRather than running\ntruss init\nand creating a Truss in a directory, a Chain\nis a single file, giving you more flexibility for implementing multi-step\nmodel inference. Create an example with\ntruss chains init\n.\nConfiguration is done inline in typed Python code rather than in a\nconfig.yaml\nfile.\nWhile Chainlets are converted to Truss models when run on Baseten,\nChainlet != TrussModel\n.\nChains is designed for compatibility and incremental adoption, with a stub\nfunction for wrapping existing deployed models.\nWas this page helpful?\nYes\nNo\nPrevious\nYour first Chain\nBuild and deploy two example Chains\nNext\nOn this page\nChainlet\nRemote configuration\nInitialization\nContext (access information)\nDepends (call other Chainlets)\nRun remote (chaining Chainlets)\nEntrypoint\nI/O and pydantic data types\nChains compared to Truss\nAssistant\nResponses are generated using AI and may contain mistakes.\n\n\nContent from https://docs.baseten.co/development/chain/deploy:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nDeveloping a Chain\nOverview\nConcepts\nYour first Chain\nArchitecture & Design\nLocal Development\nDeploy\nInvocation\nWatch\nSubclassing\nStreaming\nBinary IO\nError Handling\nTruss Integration\nEngine Builder Models\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nExporting metrics\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nDeveloping a Chain\nDeploy\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nDeploying a Chain is an atomic action that deploys every Chainlet\nwithin the Chain. Each Chainlet specifies its own remote\nenvironment \u2014 hardware resources, Python and system dependencies, autoscaling\nsettings.\n\u200b\nDevelopment\nThe default behavior for pushing a chain is to create a development deployment:\nCopy\nAsk AI\ntruss\nchains\npush\n./my_chain.py\nWhere\nmy_chain.py\ncontains the entrypoint Chainlet for your Chain.\nDevelopment deployments are intended for testing and can\u2019t scale past one\nreplica. Each time you make a development deployment, it overwrites the existing\ndevelopment deployment.\nDevelopment deployments support rapid iteration with\nwatch\n- see\nabove\nguide\n.\n\u200b\n\ud83c\udd95 Environments\nTo deploy a Chain to an environment, run:\nCopy\nAsk AI\ntruss\nchains\npush\n./my_chain.py\n--environment\n{env_name}\nEnvironments are intended for live traffic and have access to full\nautoscaling settings. Each time you deploy to an environment, a new deployment is\ncreated. Once the new deployment is live, it replaces the previous deployment,\nwhich is relegated to the published deployments list.\nLearn more\nabout environments.\nWas this page helpful?\nYes\nNo\nPrevious\nInvocation\nCall your deployed Chain\nNext\nOn this page\nDevelopment\n\ud83c\udd95 Environments\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 44875, "end_char_idx": 48502, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "61c4479f-e0a9-4972-9b5a-45dbbc87098f": {"__data__": {"id_": "61c4479f-e0a9-4972-9b5a-45dbbc87098f", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5344fe2a-1777-4a59-8545-83d8a02b0d01", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "9e14a16c7a516addfae74dbebd0b7e12932d2ed8ca1d15fd8fd40eb2e6467f78", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2a9b83a3-dd88-4503-8a0a-ab3c9c4e35d1", "node_type": "1", "metadata": {}, "hash": "cab3487cb63d9d3e64d24f8fb6ee647cbd5b01d2cf166bd077cf701838e507b3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Content from https://docs.baseten.co/development/chain/design:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nDeveloping a Chain\nOverview\nConcepts\nYour first Chain\nArchitecture & Design\nLocal Development\nDeploy\nInvocation\nWatch\nSubclassing\nStreaming\nBinary IO\nError Handling\nTruss Integration\nEngine Builder Models\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nExporting metrics\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nDeveloping a Chain\nArchitecture & Design\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nA Chain is composed of multiple connected Chainlets working together to perform\na task.\nFor example, the Chain in the diagram below takes a large audio file as input.\nThen it splits it into smaller chunks, transcribes each chunk in parallel\n(reducing the end-to-end latency), and finally aggregates and returns the\nresults.\nTo build an efficient Chain, we recommend drafting your high level\nstructure as a flowchart or diagram. This can help you identifying\nparallelizable units of work and steps that need different (model/hardware)\nresources.\nIf one Chainlet creates many \u201csub-tasks\u201d by calling other dependency\nChainlets (e.g. in a loop over partial work items),\nthese calls should be done as\naynscio\n-tasks that run concurrently.\nThat way you get the most out of the parallelism that Chains offers. This\ndesign pattern is extensively used in the\naudio transcription example\n.\nWhile using\nasyncio\nis essential for performance, it can also be tricky.\nHere are a few caveats to look out for:\nExecuting operations in an async function that block the event loop for\nmore than a fraction of a second. This hinders the \u201cflow\u201d of processing\nrequests concurrently and starting RPCs to other Chainlets. Ideally use\nnative async APIs. Frameworks like vLLM or triton server offer such APIs,\nsimilarly file downloads can be made async and you might find\nAsyncBatcher\nuseful.\nIf there is no async support, consider running blocking code in a\nthread/process pool (as an attributed of a Chainlet).\nCreating async tasks (e.g. with\nasyncio.ensure_future\n) does not start\nthe task\nimmediately\n. In particular, when starting several tasks in a loop,\nensure_future\nmust be alternated with operations that yield to the event\nloop that, so the task can be started. If the loop is not\nasync for\nor\ncontains other\nawait\nstatements, a \u201cdummy\u201d await can be added, for example\nawait asyncio.sleep(0)\n. This allows the tasks to be started concurrently.\nWas this page helpful?\nYes\nNo\nPrevious\nLocal Development\nIterating, Debugging, Testing, Mocking\nNext\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 48505, "end_char_idx": 51618, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2a9b83a3-dd88-4503-8a0a-ab3c9c4e35d1": {"__data__": {"id_": "2a9b83a3-dd88-4503-8a0a-ab3c9c4e35d1", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "61c4479f-e0a9-4972-9b5a-45dbbc87098f", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "1970ce65a5f7faf68ed576cf2a2ea95a4d50a09c5e24e01325f5f378809b056c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bcdc4f94-f41b-4a25-9118-cff07121a7fd", "node_type": "1", "metadata": {}, "hash": "e3dfbddb303ae986e5e78b2f87c508c114b2b054518024a84a07a2d7cf35c223", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Content from https://docs.baseten.co/development/chain/engine-builder-models:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nDeveloping a Chain\nOverview\nConcepts\nYour first Chain\nArchitecture & Design\nLocal Development\nDeploy\nInvocation\nWatch\nSubclassing\nStreaming\nBinary IO\nError Handling\nTruss Integration\nEngine Builder Models\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nExporting metrics\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nDeveloping a Chain\nEngine Builder Models\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nBaseten\u2019s\nEngine Builder\nenables the deployment of optimized model inference engines. Currently, it supports TensorRT-LLM. Truss Chains allows seamless integration of these engines into structured workflows. This guide provides a quick entry point for Chains users.\n\u200b\nLLama 7B Example\nUse the\nEngineBuilderLLMChainlet\nbaseclass to configure an LLM engine. The additional\nengine_builder_config\nfield specifies model architecture, repository, and runtime parameters and more, the full options are detailed in the\nEngine Builder configuration guide\n.\nCopy\nAsk AI\nimport\ntruss_chains\nas\nchains\nfrom\ntruss.base\nimport\ntrt_llm_config, truss_config\nclass\nLlama7BChainlet\n(\nchains\n.\nEngineBuilderLLMChainlet\n):\nremote_config\n=\nchains.RemoteConfig(\ncompute\n=\nchains.Compute(\ngpu\n=\ntruss_config.Accelerator.H100),\nassets\n=\nchains.Assets(\nsecret_keys\n=\n[\n\"hf_access_token\"\n]),\n)\nengine_builder_config\n=\ntruss_config.TRTLLMConfiguration(\nbuild\n=\ntrt_llm_config.TrussTRTLLMBuildConfiguration(\nbase_model\n=\ntrt_llm_config.TrussTRTLLMModel.\nLLAMA\n,\ncheckpoint_repository\n=\ntrt_llm_config.CheckpointRepository(\nsource\n=\ntrt_llm_config.CheckpointSource.\nHF\n,\nrepo\n=\n\"meta-llama/Llama-3.1-8B-Instruct\"\n,\n),\nmax_batch_size\n=\n8\n,\nmax_seq_len\n=\n4096\n,\ntensor_parallel_count\n=\n1\n,\n)\n)\n\u200b\nDifferences from Standard Chainlets\nNo\nrun_remote\nimplementation: Unlike regular Chainlets,\nEngineBuilderLLMChainlet\ndoes not require users to implement\nrun_remote()\n. Instead, it automatically wires into the deployed engine\u2019s API. All LLM Chainlets have the same function signature:\nchains.EngineBuilderLLMInput\nas input and a stream (\nAsyncIterator\n) of strings as output. Likewise\nEngineBuilderLLMChainlet\ns can only be used as dependencies, but not have dependencies themselves.\nNo\nrun_local\n(\nguide\n) and\nwatch\n(\nguide\n) Standard Chains support a local debugging mode and watch. However, when using\nEngineBuilderLLMChainlet\n, local execution is not available, and testing must be done after deployment.\nFor a faster dev loop of the rest of your chain (everything except the engine builder chainlet) you can substitute those chainlets with stubs like you can do for an already deployed truss model [\nguide\n].\n\u200b\nIntegrate the Engine Builder Chainlet\nAfter defining an\nEngineBuilderLLMInput\nlike\nLlama7BChainlet\nabove, you can use it as a dependency in other conventional chainlets:\nCopy\nAsk AI\nfrom\ntyping\nimport\nAsyncIterator\nimport\ntruss_chains\nas\nchains\n@chains.mark_entrypoint\nclass\nTestController\n(\nchains\n.\nChainletBase\n):\n\"\"\"Example using the Engine Builder Chainlet in another Chainlet.\"\"\"", "mimetype": "text/plain", "start_char_idx": 51621, "end_char_idx": 55217, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bcdc4f94-f41b-4a25-9118-cff07121a7fd": {"__data__": {"id_": "bcdc4f94-f41b-4a25-9118-cff07121a7fd", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2a9b83a3-dd88-4503-8a0a-ab3c9c4e35d1", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "9bfce298c6fd87e0e4aeb79697f081a0c7bd683108a67ae6881c362d6f338516", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "da655a22-2362-4fcb-b871-1be34671bb22", "node_type": "1", "metadata": {}, "hash": "a06648ad9b1dc4e7498339b7d183d56f659d3de41ae2ec0866df1d36f42b9aef", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, when using\nEngineBuilderLLMChainlet\n, local execution is not available, and testing must be done after deployment.\nFor a faster dev loop of the rest of your chain (everything except the engine builder chainlet) you can substitute those chainlets with stubs like you can do for an already deployed truss model [\nguide\n].\n\u200b\nIntegrate the Engine Builder Chainlet\nAfter defining an\nEngineBuilderLLMInput\nlike\nLlama7BChainlet\nabove, you can use it as a dependency in other conventional chainlets:\nCopy\nAsk AI\nfrom\ntyping\nimport\nAsyncIterator\nimport\ntruss_chains\nas\nchains\n@chains.mark_entrypoint\nclass\nTestController\n(\nchains\n.\nChainletBase\n):\n\"\"\"Example using the Engine Builder Chainlet in another Chainlet.\"\"\"\ndef\n__init__\n(\nself\n,\nllm\n=\nchains.depends(Llama7BChainlet)) ->\nNone\n:\nself\n._llm\n=\nllm\nasync\ndef\nrun_remote\n(\nself\n,\nprompt\n:\nstr\n) -> AsyncIterator[\nstr\n]:\nmessages\n=\n[{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: prompt}]\nllm_input\n=\nchains.EngineBuilderLLMInput(\nmessages\n=\nmessages)\nasync\nfor\nchunk\nin\nself\n._llm.run_remote(llm_input):\nyield\nchunk\nWas this page helpful?\nYes\nNo\nPrevious\nConcepts\nNext\nOn this page\nLLama 7B Example\nDifferences from Standard Chainlets\nIntegrate the Engine Builder Chainlet\nAssistant\nResponses are generated using AI and may contain mistakes.\n\n\nContent from https://docs.baseten.co/development/chain/errorhandling:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nDeveloping a Chain\nOverview\nConcepts\nYour first Chain\nArchitecture & Design\nLocal Development\nDeploy\nInvocation\nWatch\nSubclassing\nStreaming\nBinary IO\nError Handling\nTruss Integration\nEngine Builder Models\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nExporting metrics\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nDeveloping a Chain\nError Handling\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nError handling in Chains follows the principle that the root cause \u201cbubbles\nup\u201d until the entrypoint - which returns an error response. Similarly to how\npython stack traces contain all the layers from where an exception was raised\nup until the main function.\nConsider the case of a Chain where the entrypoint calls\nrun_remote\nof a\nChainlet named\nTextToNum\nand this in turn invokes\nTextReplicator\n. The\nrespective\nrun_remote\nmethods might also use other helper functions that\nappear in the call stack.", "mimetype": "text/plain", "start_char_idx": 54501, "end_char_idx": 57301, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "da655a22-2362-4fcb-b871-1be34671bb22": {"__data__": {"id_": "da655a22-2362-4fcb-b871-1be34671bb22", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bcdc4f94-f41b-4a25-9118-cff07121a7fd", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "f12b4ec3bc5c2ca569b03cdd1ab4cbbee58b668306ca9cedaa355042070dfc60", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a198df72-c7a9-4eeb-94ae-44010993ee07", "node_type": "1", "metadata": {}, "hash": "1a71a4d7b01d16a2feb827a2434f10f9d91fe67f386a34d43d531493ae87a711", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Similarly to how\npython stack traces contain all the layers from where an exception was raised\nup until the main function.\nConsider the case of a Chain where the entrypoint calls\nrun_remote\nof a\nChainlet named\nTextToNum\nand this in turn invokes\nTextReplicator\n. The\nrespective\nrun_remote\nmethods might also use other helper functions that\nappear in the call stack.\nBelow is an example stack trace that shows how the root cause (a\nValueError\n) is propagated up to the entrypoint\u2019s\nrun_remote\nmethod (this\nis what you would see as an error log):\nCopy\nAsk AI\nChainlet-Traceback (most recent call last):\nFile \"/packages/itest_chain.py\", line 132, in run_remote\nvalue = self._accumulate_parts(text_parts.parts)\nFile \"/packages/itest_chain.py\", line 144, in _accumulate_parts\nvalue += self._text_to_num.run_remote(part)\nValueError: (showing chained remote errors, root error at the bottom)\n\u251c\u2500 Error in dependency Chainlet `TextToNum`:\n\u2502   Chainlet-Traceback (most recent call last):\n\u2502     File \"/packages/itest_chain.py\", line 87, in run_remote\n\u2502       generated_text = self._replicator.run_remote(data)\n\u2502   ValueError: (showing chained remote errors, root error at the bottom)\n\u2502   \u251c\u2500 Error in dependency Chainlet `TextReplicator`:\n\u2502   \u2502   Chainlet-Traceback (most recent call last):\n\u2502   \u2502     File \"/packages/itest_chain.py\", line 52, in run_remote\n\u2502   \u2502       validate_data(data)\n\u2502   \u2502     File \"/packages/itest_chain.py\", line 36, in validate_data\n\u2502   \u2502       raise ValueError(f\"This input is too long: {len(data)}.\")\n\u2570   \u2570   ValueError: This input is too long: 100.\n\u200b\nException handling and retries\nAbove stack trace is what you see if you don\u2019t catch the exception. It is\npossible to add error handling around each remote Chainlet invocation.\nChains tries to raise the same exception class on the\ncaller\nChainlet as was\nraised in the\ndependency\nChainlet.\nBuiltin exceptions (e.g.\nValueError\n) always work.\nCustom or third-party exceptions (e.g. from\ntorch\n) can be only raised\nin the caller if they are included in the dependencies of the caller as\nwell. If the exception class cannot be resolved, a\nGenericRemoteException\nis raised instead.\nNote that the\nmessage\nof re-raised exceptions is the concatenation\nof the original message and the formatted stack trace of the dependency\nChainlet.\nIn some cases it might make sense to simply retry a remote invocation (e.g.\nif it failed due to some transient problems like networking or any \u201cflaky\u201d\nparts).\ndepends\ncan be configured with additional\noptions\nfor that.\nBelow example shows how you can add automatic retries and error handling for\nthe call to\nTextReplicator\nin\nTextToNum\n:\nCopy\nAsk AI\nimport\ntruss_chains\nas\nchains\nclass\nTextToNum\n(\nchains\n.\nChainletBase\n):\ndef\n__init__\n(\nself\n,\nreplicator\n: TextReplicator\n=\nchains.depends(TextReplicator,\nretries\n=\n3\n),\n) ->\nNone\n:\nself\n._replicator\n=\nreplicator\nasync\ndef\nrun_remote\n(\nself\n,\ndata\n:\n...\n):\ntry\n:\ngenerated_text\n=\nawait\nself\n._replicator.run_remote(data)\nexcept\nValueError\n:\n...\n# Handle error.\n\u200b\nStack filtering\nThe stack trace is intended to show the user implemented code in\nrun_remote\n(and user implemented helper functions). Under the\nhood, the calls from one Chainlet to another go through an HTTP\nconnection, managed by the Chains framework. And each Chainlet itself is\nrun as a FastAPI server with several layers of request handling code \u201cabove\u201d.\nIn order to provide concise, readable stacks, all of this non-user code is\nfiltered out.\nWas this page helpful?\nYes\nNo\nPrevious\nTruss Integration\nIntegrate deployed Truss models with stubs\nNext\nOn this page\nException handling and retries\nStack filtering\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 56937, "end_char_idx": 60621, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a198df72-c7a9-4eeb-94ae-44010993ee07": {"__data__": {"id_": "a198df72-c7a9-4eeb-94ae-44010993ee07", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "da655a22-2362-4fcb-b871-1be34671bb22", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "365f36f6586d6597edb769edf8a21ec42a2204959b8445bb56cf05f1d1b789ce", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3685dcf2-1b40-4173-b0d4-0b4ab0600d16", "node_type": "1", "metadata": {}, "hash": "94bd29237da203ba01caa2d108d6e712a970531eed77994a90e3bfb1daf28e4d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u200b\nStack filtering\nThe stack trace is intended to show the user implemented code in\nrun_remote\n(and user implemented helper functions). Under the\nhood, the calls from one Chainlet to another go through an HTTP\nconnection, managed by the Chains framework. And each Chainlet itself is\nrun as a FastAPI server with several layers of request handling code \u201cabove\u201d.\nIn order to provide concise, readable stacks, all of this non-user code is\nfiltered out.\nWas this page helpful?\nYes\nNo\nPrevious\nTruss Integration\nIntegrate deployed Truss models with stubs\nNext\nOn this page\nException handling and retries\nStack filtering\nAssistant\nResponses are generated using AI and may contain mistakes.\n\n\nContent from https://docs.baseten.co/development/chain/getting-started:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nDeveloping a Chain\nOverview\nConcepts\nYour first Chain\nArchitecture & Design\nLocal Development\nDeploy\nInvocation\nWatch\nSubclassing\nStreaming\nBinary IO\nError Handling\nTruss Integration\nEngine Builder Models\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nExporting metrics\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nDeveloping a Chain\nYour first Chain\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nThis quickstart guide contains instructions for creating two Chains:\nA simple CPU-only \u201chello world\u201d-Chain.\nA Chain that implements Phi-3 Mini and uses it to write poems.\n\u200b\nPrerequisites\nTo use Chains, install a recent Truss version and ensure pydantic is v2:\nCopy\nAsk AI\npip\ninstall\n--upgrade\ntruss\n'pydantic>=2.0.0'\nHelp for setting up a clean development environment\nTruss requires python\n>=3.8,<3.13\n. To set up a fresh development environment,\nyou can use the following commands, creating a environment named\nchains_env\nusing\npyenv\n:\nCopy\nAsk AI\ncurl\nhttps://pyenv.run\n|\nbash\necho\n'export PYENV_ROOT=\"$HOME/.pyenv\"'\n>>\n~/.bashrc\necho\n'[[ -d $PYENV_ROOT/bin ]] && export PATH=\"$PYENV_ROOT/bin:$PATH\"'\n>>\n~/.bashrc\necho\n'eval \"$(pyenv init -)\"'\n>>\n~/.bashrc\nsource\n~/.bashrc\npyenv\ninstall\n3.11.0\nENV_NAME\n=\n\"chains_env\"\npyenv\nvirtualenv\n3.11.0\n$ENV_NAME\npyenv\nactivate\n$ENV_NAME\npip\ninstall\n--upgrade\ntruss\n'pydantic>=2.0.0'\nTo deploy Chains remotely, you also need a\nBaseten account\n.\nIt is handy to export your API key to the current shell session or permanently in your\n.bashrc\n:\n~/.bashrc\nCopy\nAsk AI\nexport\nBASETEN_API_KEY\n=\n\"nPh8...\"\n\u200b\nExample: Hello World\nChains are written in Python files. In your working directory,\ncreate\nhello_chain/hello.py\n:\nCopy\nAsk AI\nmkdir\nhello_chain\ncd\nhello_chain\ntouch\nhello.py\nIn the file, we\u2019ll specify a basic Chain. It has two Chainlets:\nHelloWorld\n, the entrypoint, which handles the input and output.\nRandInt\n, which generates a random integer. It is used a as a dependency\nby\nHelloWorld\n.\nVia the entrypoint, the Chain takes a maximum value and returns the string \u201d\nHello World!\u201d repeated a\nvariable number of times.\nhello.py\nCopy\nAsk AI\nimport\nrandom\nimport\ntruss_chains\nas\nchains\nclass\nRandInt\n(\nchains\n.\nChainletBase\n):\nasync\ndef\nrun_remote\n(\nself\n,\nmax_value\n:\nint\n) ->\nint\n:\nreturn\nrandom.randint(\n1\n, max_value)\n@chains.mark_entrypoint\nclass\nHelloWorld\n(\nchains\n.", "mimetype": "text/plain", "start_char_idx": 59939, "end_char_idx": 63561, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3685dcf2-1b40-4173-b0d4-0b4ab0600d16": {"__data__": {"id_": "3685dcf2-1b40-4173-b0d4-0b4ab0600d16", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a198df72-c7a9-4eeb-94ae-44010993ee07", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "7fa8ce16c6ba0cc199e87fca6b565dd10ff692fc360c9b9c72a89163f32f194c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bb6281ce-264e-4d10-903d-945017a9bc91", "node_type": "1", "metadata": {}, "hash": "df9c002363fb3a77dbadc542a918304147c1a3a12c875a083bf9998e2cb3889c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It has two Chainlets:\nHelloWorld\n, the entrypoint, which handles the input and output.\nRandInt\n, which generates a random integer. It is used a as a dependency\nby\nHelloWorld\n.\nVia the entrypoint, the Chain takes a maximum value and returns the string \u201d\nHello World!\u201d repeated a\nvariable number of times.\nhello.py\nCopy\nAsk AI\nimport\nrandom\nimport\ntruss_chains\nas\nchains\nclass\nRandInt\n(\nchains\n.\nChainletBase\n):\nasync\ndef\nrun_remote\n(\nself\n,\nmax_value\n:\nint\n) ->\nint\n:\nreturn\nrandom.randint(\n1\n, max_value)\n@chains.mark_entrypoint\nclass\nHelloWorld\n(\nchains\n.\nChainletBase\n):\ndef\n__init__\n(\nself\n,\nrand_int\n=\nchains.depends(RandInt,\nretries\n=\n3\n)) ->\nNone\n:\nself\n._rand_int\n=\nrand_int\nasync\ndef\nrun_remote\n(\nself\n,\nmax_value\n:\nint\n) ->\nstr\n:\nnum_repetitions\n=\nawait\nself\n._rand_int.run_remote(max_value)\nreturn\n\"Hello World! \"\n*\nnum_repetitions\n\u200b\nThe Chainlet class-contract\nExactly one Chainlet must be marked as the entrypoint with\nthe\n@chains.mark_entrypoint\ndecorator. This Chainlet is responsible for\nhandling public-facing input and output for the whole Chain in response to an\nAPI call.\nA Chainlet class has a single public method,\nrun_remote()\n, which is\nthe API\nendpoint for the entrypoint Chainlet and the function that other Chainlets can\nuse as a dependency. The\nrun_remote()\nmethod must be fully type-annotated\nwith\nprimitive python\ntypes\nor\npydantic models\n.\nChainlets cannot be\nnaively\ninstantiated. The only correct usages are:\nMake one Chainlet depend on another one via the\nchains.depends()\ndirective\nas an\n__init__\n-argument as shown above for the\nRandInt\nChainlet.\nIn the\nlocal debugging mode\n.\nBeyond that, you can structure your code as you like, with private methods,\nimports from other files, and so forth.\nKeep in mind that Chainlets are intended for distributed, replicated, remote\nexecution, so using global variables, global state, and certain Python\nfeatures like importing modules dynamically at runtime should be avoided as\nthey may not work as intended.\n\u200b\nDeploy your Chain to Baseten\nTo deploy your Chain to Baseten, run:\nCopy\nAsk AI\ntruss\nchains\npush\nhello.py\nThe deploy command results in an output like this:\nCopy\nAsk AI\n\u26d3\ufe0f   HelloWorld - Chainlets  \u26d3\ufe0f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Status               \u2502 Name                    \u2502 Logs URL    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \ud83d\udc9a ACTIVE           \u2502 HelloWorld (entrypoint) \u2502 https://... \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \ud83d\udc9a ACTIVE           \u2502 RandInt (dep)           \u2502 https://... \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\nDeployment succeeded.\nYou can run the chain with:\ncurl -X POST 'https://chain-.../run_remote' \\\n-H \"Authorization: Api-Key $BASETEN_API_KEY\" \\\n-d '<JSON_INPUT>'\nWait for the status to turn to\nACTIVE\nand test invoking your Chain (replace\n$INVOCATION_URL\nin below command):\nCopy\nAsk AI\ncurl\n-X\nPOST\n$INVOCATION_URL\n\\\n-H\n\"Authorization: Api-Key\n$BASETEN_API_KEY\n\"\n\\\n-d\n'{\"max_value\": 10}'\n# \"Hello World! Hello World! Hello World! \"\n\u200b\nExample: Poetry with LLMs\nOur second example also has two Chainlets, but is somewhat more complex and\nrealistic. The Chainlets are:\nPoemGenerator\n, the entrypoint, which handles the input and output and\norchestrates calls to the LLM.\nPhiLLM\n, which runs inference on Phi-3 Mini.\nThis Chain takes a list of words and returns a poem about each word, written by\nPhi-3.", "mimetype": "text/plain", "start_char_idx": 63005, "end_char_idx": 66441, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bb6281ce-264e-4d10-903d-945017a9bc91": {"__data__": {"id_": "bb6281ce-264e-4d10-903d-945017a9bc91", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3685dcf2-1b40-4173-b0d4-0b4ab0600d16", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "56786d6e57ddc877f10ce91a96bb4875fa69fe2999eef57d4894efec12e7adcd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "74c803eb-152f-477b-82ea-a36750e40a11", "node_type": "1", "metadata": {}, "hash": "4e9dac85b29ebafc3eb8a8c54f896dc9309539921d626cb0d97df8e7ce4a794c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Hello World! Hello World! \"\n\u200b\nExample: Poetry with LLMs\nOur second example also has two Chainlets, but is somewhat more complex and\nrealistic. The Chainlets are:\nPoemGenerator\n, the entrypoint, which handles the input and output and\norchestrates calls to the LLM.\nPhiLLM\n, which runs inference on Phi-3 Mini.\nThis Chain takes a list of words and returns a poem about each word, written by\nPhi-3. Here\u2019s the architecture:\nWe build this Chain in a new working directory (if you are still inside\nhello_chain/\n, go up one level with\ncd ..\nfirst):\nCopy\nAsk AI\nmkdir\npoetry_chain\ncd\npoetry_chain\ntouch\npoems.py\nA similar ent-to-end code example, using Mistral as an LLM, is available in\nthe\nexamples\nrepo\n.\n\u200b\nBuilding the LLM Chainlet\nThe main difference between this Chain and the previous one is that we now have\nan LLM that needs a GPU and more complex dependencies.\nCopy the following code into\npoems.py\n:\npoems.py\nCopy\nAsk AI\nimport\nasyncio\nfrom\ntyping\nimport\nList\nimport\npydantic\nimport\ntruss_chains\nas\nchains\nfrom\ntruss\nimport\ntruss_config\nPHI_HF_MODEL\n=\n\"microsoft/Phi-3-mini-4k-instruct\"\n# This configures to cache model weights from the hunggingface repo\n# in the docker image that is used for deploying the Chainlet.\nPHI_CACHE\n=\ntruss_config.ModelRepo(\nrepo_id\n=\nPHI_HF_MODEL\n,\nallow_patterns\n=\n[\n\"*.json\"\n,\n\"*.safetensors\"\n,\n\".model\"\n]\n)\nclass\nMessages\n(\npydantic\n.\nBaseModel\n):\nmessages: List[dict[\nstr\n,\nstr\n]]\nclass\nPhiLLM\n(\nchains\n.\nChainletBase\n):\n# `remote_config` defines the resources required for this chainlet.\nremote_config\n=\nchains.RemoteConfig(\ndocker_image\n=\nchains.DockerImage(\n# The phi model needs some extra python packages.\npip_requirements\n=\n[\n\"accelerate==0.30.1\"\n,\n\"einops==0.8.0\"\n,\n\"transformers==4.41.2\"\n,\n\"torch==2.3.0\"\n,\n]\n),\n# The phi model needs a GPU and more CPUs.\ncompute\n=\nchains.Compute(\ncpu_count\n=\n2\n,\ngpu\n=\n\"T4\"\n),\n# Cache the model weights in the image\nassets\n=\nchains.Assets(\ncached\n=\n[\nPHI_CACHE\n]),\n)\ndef\n__init__\n(\nself\n) ->\nNone\n:\n# Note the imports of the *specific* python requirements are\n# pushed down to here. This code will only be executed on the\n# remotely deployed Chainlet, not in the local environment,\n# so we don't need to install these packages in the local\n# dev environment.\nimport\ntorch\nimport\ntransformers\nself\n._model\n=\ntransformers.AutoModelForCausalLM.from_pretrained(\nPHI_HF_MODEL\n,\ntorch_dtype\n=\ntorch.float16,\ndevice_map\n=\n\"auto\"\n,\n)\nself\n._tokenizer\n=\ntransformers.AutoTokenizer.from_pretrained(\nPHI_HF_MODEL\n,\n)\nself\n._generate_args\n=\n{\n\"max_new_tokens\"\n:\n512\n,\n\"temperature\"\n:\n1.0\n,\n\"top_p\"\n:\n0.95\n,\n\"top_k\"\n:\n50\n,\n\"repetition_penalty\"\n:\n1.0\n,\n\"no_repeat_ngram_size\"\n:\n0\n,\n\"use_cache\"\n:\nTrue\n,\n\"do_sample\"\n:\nTrue\n,\n\"eos_token_id\"\n:\nself\n._tokenizer.eos_token_id,\n\"pad_token_id\"\n:\nself\n._tokenizer.pad_token_id,\n}\nasync\ndef\nrun_remote\n(\nself\n,\nmessages\n: Messages) ->\nstr\n:\nimport\ntorch\nmodel_inputs\n=\nself\n._tokenizer.apply_chat_template(\nmessages,\ntokenize\n=\nFalse\n,\nadd_generation_prompt\n=\nTrue\n)\ninputs\n=\nself\n._tokenizer(model_inputs,\nreturn_tensors\n=\n\"pt\"\n)\ninput_ids\n=\ninputs[\n\"input_ids\"\n].to(\n\"cuda\"\n)\nwith\ntorch.no_grad():\noutputs\n=\nself\n._model.generate(\ninput_ids\n=\ninput_ids,\n**\nself\n._generate_args)\noutput_text\n=\nself\n._tokenizer.decode(\noutputs[\n0\n],\nskip_special_tokens\n=\nTrue\n)\nreturn\noutput_text\n\u200b\nBuilding the entrypoint\nNow that we have an LLM, we can use it in a poem generator Chainlet.", "mimetype": "text/plain", "start_char_idx": 66046, "end_char_idx": 69429, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "74c803eb-152f-477b-82ea-a36750e40a11": {"__data__": {"id_": "74c803eb-152f-477b-82ea-a36750e40a11", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bb6281ce-264e-4d10-903d-945017a9bc91", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "8cd754d58d654441ece7ef3cdb8fe35aaf6a61d0cd530a049c070c36cecb34f7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2de4ea91-6cbd-45cf-841b-73895c2b025c", "node_type": "1", "metadata": {}, "hash": "7c6c009a81ca837a79ba888a47c151098bfc956fd988d10836bbd6d1c6cc3f74", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Add the\nfollowing code to\npoems.py\n:\npoems.py\nCopy\nAsk AI\nimport\nasyncio\n@chains.mark_entrypoint\nclass\nPoemGenerator\n(\nchains\n.\nChainletBase\n):\ndef\n__init__\n(\nself\n,\nphi_llm\n: PhiLLM\n=\nchains.depends(PhiLLM)) ->\nNone\n:\nself\n._phi_llm\n=\nphi_llm\nasync\ndef\nrun_remote\n(\nself\n,\nwords\n: list[\nstr\n]) -> list[\nstr\n]:\ntasks\n=\n[]\nfor\nword\nin\nwords:\nmessages\n=\nMessages(\nmessages\n=\n[\n{\n\"role\"\n:\n\"system\"\n,\n\"content\"\n: (\n\"You are poet who writes short, \"\n\"lighthearted, amusing poetry.\"\n),\n},\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\nf\n\"Write a poem about\n{\nword\n}\n\"\n},\n]\n)\ntasks.append(\nasyncio.ensure_future(\nself\n._phi_llm.run_remote(messages)))\nawait\nasyncio.sleep(\n0\n)\n# Yield to event loop, to allow starting tasks.\nreturn\nlist\n(\nawait\nasyncio.gather(\n*\ntasks))\nNote that we use\nasyncio.ensure_future\naround each RPC to the LLM chainlet.\nThis makes the current python process start these remote calls concurrently,\ni.e. the next call is started before the previous one has finished and we can\nminimize our overall runtime. In order to await the results of all calls,\nasyncio.gather\nis used which gives us back normal python objects.\nIf the LLM is hit with many concurrent requests, it can auto-scale up (if\nautoscaling is configure). More advanced LLM models have batching capabilities,\nso for those even a single instance can serve concurrent request.\n\u200b\nDeploy your Chain to Baseten\nTo deploy your Chain to Baseten, run:\nCopy\nAsk AI\ntruss\nchains\npush\npoems.py\nWait for the status to turn to\nACTIVE\nand test invoking your Chain (replace\n$INVOCATION_URL\nin below command):\nCopy\nAsk AI\ncurl\n-X\nPOST\n$INVOCATION_URL\n\\\n-H\n\"Authorization: Api-Key\n$BASETEN_API_KEY\n\"\n\\\n-d\n'{\"words\": [\"bird\", \"plane\", \"superman\"]}'\n#[[\n#\"<s> [INST] Generate a poem about: bird [/INST] In the quiet hush of...</s>\",\n#\"<s> [INST] Generate a poem about: plane [/INST] In the vast, boundless...</s>\",\n#\"<s> [INST] Generate a poem about: superman [/INST] In the realm where...</s>\"\n#]]\nWas this page helpful?\nYes\nNo\nPrevious\nArchitecture & Design\nHow to structure your Chainlets\nNext\nOn this page\nPrerequisites\nExample: Hello World\nThe Chainlet class-contract\nDeploy your Chain to Baseten\nExample: Poetry with LLMs\nBuilding the LLM Chainlet\nBuilding the entrypoint\nDeploy your Chain to Baseten\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 69430, "end_char_idx": 71751, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2de4ea91-6cbd-45cf-841b-73895c2b025c": {"__data__": {"id_": "2de4ea91-6cbd-45cf-841b-73895c2b025c", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "74c803eb-152f-477b-82ea-a36750e40a11", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "0103970e0226d5a91e102b705b4a6add0a83f71d269a69e1b4ee8212aecbafc5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3267fb5c-e0ca-451e-be15-e12501b36b41", "node_type": "1", "metadata": {}, "hash": "c89aa7b7fa00ae4a9cf702170de0c0dea733392865468e7f8ac0504af158a68f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Yes\nNo\nPrevious\nArchitecture & Design\nHow to structure your Chainlets\nNext\nOn this page\nPrerequisites\nExample: Hello World\nThe Chainlet class-contract\nDeploy your Chain to Baseten\nExample: Poetry with LLMs\nBuilding the LLM Chainlet\nBuilding the entrypoint\nDeploy your Chain to Baseten\nAssistant\nResponses are generated using AI and may contain mistakes.\n\n\nContent from https://docs.baseten.co/development/chain/invocation:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nDeveloping a Chain\nOverview\nConcepts\nYour first Chain\nArchitecture & Design\nLocal Development\nDeploy\nInvocation\nWatch\nSubclassing\nStreaming\nBinary IO\nError Handling\nTruss Integration\nEngine Builder Models\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nExporting metrics\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nDeveloping a Chain\nInvocation\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nOnce your Chain is deployed, you can call it via its API endpoint. Chains use\nthe same inference API as models:\nEnvironment endpoint\nDevelopment endpoint\nEndpoint by ID\nHere\u2019s an example which calls the development deployment:\ncall_chain.py\nCopy\nAsk AI\nimport\nrequests\nimport\nos\n# From the Chain overview page on Baseten\n# E.g. \"https://chain-<CHAIN_ID>.api.baseten.co/development/run_remote\"\nCHAIN_URL\n=\n\"\"\nbaseten_api_key\n=\nos.environ[\n\"BASETEN_API_KEY\"\n]\n# JSON keys and types match the `run_remote` method signature.\ndata\n=\n{\n...\n}\nresp\n=\nrequests.post(\nCHAIN_URL\n,\nheaders\n=\n{\n\"Authorization\"\n:\nf\n\"Api-Key\n{\nbaseten_api_key\n}\n\"\n},\njson\n=\ndata,\n)\nprint\n(resp.json())\n\u200b\nHow to pass chain input\nThe data schema of the inference request corresponds to the function\nsignature of\nrun_remote()\nin your entrypoint Chainlet.\nFor example, for the Hello Chain,\nHelloAll.run_remote()\n:\nCopy\nAsk AI\nasync\ndef\nrun_remote\n(\nself\n,\nnames\n: list[\nstr\n]) ->\nstr\n:\nYou\u2019d pass the following JSON payload:\nCopy\nAsk AI\n{\n\"names\"\n: [\n\"Marius\"\n,\n\"Sid\"\n,\n\"Bola\"\n] }\nI.e. the keys in the JSON record, match the argument names and values\nmatch the types of\nrun_remote.\n\u200b\nAsync chain inference\nLike Truss models, Chains support async invocation. The\nguide for\nmodels\napplies largely - in particular for how to wrap the\ninput and set up the webhook to process results.\nThe following additional points are chains specific:\nUse chain-based URLS:\nhttps://chain-{chain}.api.baseten.co/production/async_run_remote\nhttps://chain-{chain}.api.baseten.co/development/async_run_remote\nhttps://chain-{chain}.api.baseten.co/deployment/{deployment}/async_run_remote\n.\nhttps://chain-{chain}.api.baseten.co/environments/{env_name}/async_run_remote\n.\nOnly the entrypoint is invoked asynchronously. Internal Chainlet-Chainlet\ncalls run synchronously.\nWas this page helpful?\nYes\nNo\nPrevious\nWatch\nLive-patch deployed code\nNext\nOn this page\nHow to pass chain input\nAsync chain inference\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 71398, "end_char_idx": 74775, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3267fb5c-e0ca-451e-be15-e12501b36b41": {"__data__": {"id_": "3267fb5c-e0ca-451e-be15-e12501b36b41", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2de4ea91-6cbd-45cf-841b-73895c2b025c", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ebbf3dcdacb3fd446323ff597ad082f2ba48b1951bdcbdb305ab461050c50524", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "25bf7ade-a83b-408c-9b6a-9219de556a66", "node_type": "1", "metadata": {}, "hash": "887e75a9dfaf08ec7ca4c0033c4f8f33590a2101cd156312968c411b23e22b55", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Content from https://docs.baseten.co/development/chain/localdev:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nDeveloping a Chain\nOverview\nConcepts\nYour first Chain\nArchitecture & Design\nLocal Development\nDeploy\nInvocation\nWatch\nSubclassing\nStreaming\nBinary IO\nError Handling\nTruss Integration\nEngine Builder Models\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nExporting metrics\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nDeveloping a Chain\nLocal Development\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nChains are designed for production in replicated remote deployments. But\nalongside that production-ready power, we offer great local development and\ndeployment experiences.\nThe 6 principles behind Chains\nChains exists to help you build multi-step, multi-model pipelines. The\nabstractions that Chains introduces are based on six opinionated principles:\nthree for architecture and three for developer experience.\nArchitecture principles\n1\nAtomic components\nEach step in the pipeline can set its own hardware requirements and\nsoftware dependencies, separating GPU and CPU workloads.\n2\nModular scaling\nEach component has independent autoscaling parameters for targeted\nresource allocation, removing bottlenecks from your pipelines.\n3\nMaximum composability\nComponents specify a single public interface for flexible-but-safe\ncomposition and are reusable between projects\nDeveloper experience principles\n4\nType safety and validation\nEliminate entire taxonomies of bugs by writing typed Python code and\nvalidating inputs, outputs, module initializations, function signatures,\nand even remote server configurations.\n5\nLocal debugging\nSeamless local testing and cloud deployments: test Chains locally with\nsupport for mocking the output of any step and simplify your cloud\ndeployment loops by separating large model deployments from quick\nupdates to glue code.\n6\nIncremental adoption\nUse Chains to orchestrate existing model deployments, like pre-packaged\nmodels from Baseten\u2019s model library, alongside new model pipelines built\nentirely within Chains.\nLocally, a Chain is just Python files in a source tree. While that gives you a\nlot of flexibility in how you structure your code, there are some constraints\nand rules to follow to ensure successful distributed, remote execution in\nproduction.\nThe best thing you can do while developing locally with Chains is to run your\ncode frequently, even if you do not have a\n__main__\nsection: the Chains\nframework runs various validations at\nmodule initialization\nto help\nyou catch issues early.\nAdditionally, running\nmypy\nand fixing reported type errors can help you\nfind problems early in a rapid feedback loop, before attempting a (much\nslower) deployment.\nComplementary to the purely local development Chains also has a \u201cwatch\u201d mode,\nlike Truss, see the\nwatch guide\n.\n\u200b\nTest a Chain locally\nLet\u2019s revisit our \u201cHello World\u201d Chain:\nhello_chain/hello.py\nCopy\nAsk AI\nimport\nasyncio\nimport\ntruss_chains\nas\nchains\n# This Chainlet does the work\nclass\nSayHello\n(\nchains\n.\nChainletBase\n):\nasync\ndef\nrun_remote\n(\nself\n,\nname\n:\nstr\n) ->\nstr\n:\nreturn\nf\n\"Hello,\n{\nname\n}\n\"\n# This Chainlet orchestrates the work\n@chains.mark_entrypoint\nclass\nHelloAll\n(\nchains\n.", "mimetype": "text/plain", "start_char_idx": 74778, "end_char_idx": 78475, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "25bf7ade-a83b-408c-9b6a-9219de556a66": {"__data__": {"id_": "25bf7ade-a83b-408c-9b6a-9219de556a66", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3267fb5c-e0ca-451e-be15-e12501b36b41", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "396d077c9788bbc95c20948b46aed257e0bf473024159c610290969ba390f04a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ea27b600-b6aa-4287-b010-f870bea23948", "node_type": "1", "metadata": {}, "hash": "b389980da34204f485718484834d96841cafbdf3b69bf1ff7901b2ae6736c95c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Additionally, running\nmypy\nand fixing reported type errors can help you\nfind problems early in a rapid feedback loop, before attempting a (much\nslower) deployment.\nComplementary to the purely local development Chains also has a \u201cwatch\u201d mode,\nlike Truss, see the\nwatch guide\n.\n\u200b\nTest a Chain locally\nLet\u2019s revisit our \u201cHello World\u201d Chain:\nhello_chain/hello.py\nCopy\nAsk AI\nimport\nasyncio\nimport\ntruss_chains\nas\nchains\n# This Chainlet does the work\nclass\nSayHello\n(\nchains\n.\nChainletBase\n):\nasync\ndef\nrun_remote\n(\nself\n,\nname\n:\nstr\n) ->\nstr\n:\nreturn\nf\n\"Hello,\n{\nname\n}\n\"\n# This Chainlet orchestrates the work\n@chains.mark_entrypoint\nclass\nHelloAll\n(\nchains\n.\nChainletBase\n):\ndef\n__init__\n(\nself\n,\nsay_hello_chainlet\n=\nchains.depends(SayHello)) ->\nNone\n:\nself\n._say_hello\n=\nsay_hello_chainlet\nasync\ndef\nrun_remote\n(\nself\n,\nnames\n: list[\nstr\n]) ->\nstr\n:\ntasks\n=\n[]\nfor\nname\nin\nnames:\ntasks.append(asyncio.ensure_future(\nself\n._say_hello.run_remote(name)))\nreturn\n\"\n\\n\n\"\n.join(\nawait\nasyncio.gather(\n*\ntasks))\n# Test the Chain locally\nif\n__name__\n==\n\"__main__\"\n:\nwith\nchains.run_local():\nhello_chain\n=\nHelloAll()\nresult\n=\nasyncio.get_event_loop().run_until_complete(\nhello_chain.run_remote([\n\"Marius\"\n,\n\"Sid\"\n,\n\"Bola\"\n]))\nprint\n(result)\nWhen the\n__main__()\nmodule is run, local instances of the Chainlets are\ncreated, allowing you to test functionality of your chain just by executing the\nPython file:\nCopy\nAsk AI\ncd\nhello_chain\npython\nhello.py\n# Hello, Marius\n# Hello, Sid\n# Hello, Bola\n\u200b\nMock execution of GPU Chainlets\nUsing\nrun_local()\nto run your code locally requires that your development\nenvironment have the compute resources and dependencies that each Chainlet\nneeds. But that often isn\u2019t possible when building with AI models.\nChains offers a workaround, mocking, to let you test the coordination and\nbusiness logic of your multi-step inference pipeline without worrying about\nrunning the model locally.\nThe second example in the\ngetting started guide\nimplements a Truss Chain for generating poems with Phi-3.\nThis Chain has two Chainlets:\nThe\nPhiLLM\nChainlet, which requires an NVIDIA A10G GPU.\nThe\nPoemGenerator\nChainlet, which easily runs on a CPU.\nIf you have an NVIDIA T4 under your desk, good for you. For the rest of us, we\ncan mock the\nPhiLLM\nChainlet that is infeasible to run locally so that we can\nquickly test the\nPoemGenerator\nChainlet.\nTo do this, we define a mock Phi-3 model in our\n__main__\nmodule and give it\na\nrun_remote()\nmethod that\nproduces a test output that matches the output type we expect from the real\nChainlet. Then, we inject an instance of this mock Chainlet into our Chain:\npoems.py\nCopy\nAsk AI\nif\n__name__\n==\n\"__main__\"\n:\nclass\nFakePhiLLM\n:\nasync\ndef\nrun_remote\n(\nself\n,\nprompt\n:\nstr\n) ->\nstr\n:\nreturn\nf\n\"Here's a poem about\n{\nprompt.split(\n\" \"\n)[\n-\n1\n]\n}\n\"\nwith\nchains.run_local():\npoem_generator\n=\nPoemGenerator(\nphi_llm\n=\nFakePhiLLM())\nresult\n=\nasyncio.get_event_loop().run_until_complete(\npoem_generator.run_remote(\nwords\n=\n[\n\"bird\"\n,\n\"plane\"\n,\n\"superman\"\n]))\nprint\n(result)\nAnd run your Python file:\nCopy\nAsk AI\npython\npoems.py\n# ['Here's a poem about bird', 'Here's a poem about plane', 'Here's a poem about superman']\n\u200b\nTyping of mocks\nYou may notice that the argument\nphi_llm\nexpects a type\nPhiLLM\n, while we\npass an instance of\nFakePhiLLM\n. These aren\u2019t the same, which is formally a\ntype error.\nHowever, this works at runtime because we constructed\nFakePhiLLM\nto\nimplement the same\nprotocol\nas the real thing.", "mimetype": "text/plain", "start_char_idx": 77820, "end_char_idx": 81276, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ea27b600-b6aa-4287-b010-f870bea23948": {"__data__": {"id_": "ea27b600-b6aa-4287-b010-f870bea23948", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "25bf7ade-a83b-408c-9b6a-9219de556a66", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "8df69b0cbccf212ed9c99eba3f5b028a1b7192ed5bc125f2c9c3dbeec7f548e8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "48041027-0da9-4fef-9cfb-9abad4b9fa31", "node_type": "1", "metadata": {}, "hash": "5ba3209a3e910c100ae952063452d42faed6de226f72f12039a1ac4b654885e0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "These aren\u2019t the same, which is formally a\ntype error.\nHowever, this works at runtime because we constructed\nFakePhiLLM\nto\nimplement the same\nprotocol\nas the real thing. We can make this explicit by\ndefining a\nProtocol\nas a type annotation:\nCopy\nAsk AI\nfrom\ntyping\nimport\nProtocol\nclass\nPhiProtocol\n(\nProtocol\n):\ndef\nrun_remote\n(\nself\n,\ndata\n:\nstr\n) ->\nstr\n:\n...\nand changing the argument type in\nPoemGenerator\n:\nCopy\nAsk AI\n@chains.mark_entrypoint\nclass\nPoemGenerator\n(\nchains\n.\nChainletBase\n):\ndef\n__init__\n(\nself\n,\nphi_llm\n: PhiProtocol\n=\nchains.depends(PhiLLM)) ->\nNone\n:\nself\n._phi_llm\n=\nphi_llm\nThis is a bit more work and not needed to execute the code, but it shows how\ntyping consistency can be achieved - if desired.\nWas this page helpful?\nYes\nNo\nPrevious\nDeploy\nDeploy your Chain on Baseten\nNext\nOn this page\nTest a Chain locally\nMock execution of GPU Chainlets\nTyping of mocks\nAssistant\nResponses are generated using AI and may contain mistakes.\n\n\nContent from https://docs.baseten.co/development/chain/overview:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nDeveloping a Chain\nOverview\nConcepts\nYour first Chain\nArchitecture & Design\nLocal Development\nDeploy\nInvocation\nWatch\nSubclassing\nStreaming\nBinary IO\nError Handling\nTruss Integration\nEngine Builder Models\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nExporting metrics\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nDeveloping a Chain\nOverview\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nChains is a framework for building robust, performant multi-step and multi-model\ninference pipelines and deploying them to production. It addresses the common\nchallenges of managing latency, cost and dependencies for complex workflows,\nwhile leveraging Truss\u2019 existing battle-tested performance, reliability and\ndeveloper toolkit.\n\u200b\nUser Guides\nGuides focus on specific features and use cases. Also refer to\ngetting started\nand\ngeneral concepts\n.\nDesign\nHow to structure your Chainlets, concurrency, file structure\nLocal Dev\nIterating, Debugging, Testing, Mocking\nDeploy\nDeploy your Chain on Baseten\nInvocation\nCall your deployed Chain\nWatch\nLive-patch deployed code\nSubclassing\nModularize and re-use Chainlet implementations\nStreaming\nStreaming outputs, reducing latency, SSEs\nBinary IO\nPerformant serialization of numeric data\nError Propagation\nUnderstanding and handling Chains errors\nTruss Integration\nIntegrate deployed Truss models with stubs\n\u200b\nFrom model to system\nSome models are actually pipelines (e.g. invoking a LLM involves sequentially\ntokenizing the input, predicting the next token, and then decoding the predicted\ntokens). These pipelines generally make sense to bundle together in a monolithic\ndeployment because they have the same dependencies, require the same compute\nresources, and have a robust ecosystem of tooling to improve efficiency and\nperformance in a single deployment.\nMany other pipelines and systems do not share these properties. Some examples\ninclude:\nRunning multiple different models in sequence.\nChunking/partitioning a set of files and concatenating/organizing results.\nPulling inputs from or saving outputs to a database or vector store.\nEach step in these workflows has different hardware requirements, software\ndependencies, and scaling needs so it doesn\u2019t make sense to bundle them in a\nmonolithic deployment. That\u2019s where Chains comes in!\n\u200b\nSix principles behind Chains\nChains exists to help you build multi-step, multi-model pipelines. The\nabstractions that Chains introduces are based on six opinionated principles:\nthree for architecture and three for developer experience.\nArchitecture principles\n1\nAtomic components\nEach step in the pipeline can set its own hardware requirements and\nsoftware dependencies, separating GPU and CPU workloads.", "mimetype": "text/plain", "start_char_idx": 81107, "end_char_idx": 85362, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "48041027-0da9-4fef-9cfb-9abad4b9fa31": {"__data__": {"id_": "48041027-0da9-4fef-9cfb-9abad4b9fa31", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ea27b600-b6aa-4287-b010-f870bea23948", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "938b01f79ed7044a9cada66718ce2a6a2b1bacf09926c4e245ec94db5628331a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a7a9fe00-0761-420c-aa86-6f09e532e997", "node_type": "1", "metadata": {}, "hash": "4e0c58a9b123d436a271efae6cc5315389434ec6b952eea4d2c9ec87f281945d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Many other pipelines and systems do not share these properties. Some examples\ninclude:\nRunning multiple different models in sequence.\nChunking/partitioning a set of files and concatenating/organizing results.\nPulling inputs from or saving outputs to a database or vector store.\nEach step in these workflows has different hardware requirements, software\ndependencies, and scaling needs so it doesn\u2019t make sense to bundle them in a\nmonolithic deployment. That\u2019s where Chains comes in!\n\u200b\nSix principles behind Chains\nChains exists to help you build multi-step, multi-model pipelines. The\nabstractions that Chains introduces are based on six opinionated principles:\nthree for architecture and three for developer experience.\nArchitecture principles\n1\nAtomic components\nEach step in the pipeline can set its own hardware requirements and\nsoftware dependencies, separating GPU and CPU workloads.\n2\nModular scaling\nEach component has independent autoscaling parameters for targeted\nresource allocation, removing bottlenecks from your pipelines.\n3\nMaximum composability\nComponents specify a single public interface for flexible-but-safe\ncomposition and are reusable between projects\nDeveloper experience principles\n4\nType safety and validation\nEliminate entire taxonomies of bugs by writing typed Python code and\nvalidating inputs, outputs, module initializations, function signatures,\nand even remote server configurations.\n5\nLocal debugging\nSeamless local testing and cloud deployments: test Chains locally with\nsupport for mocking the output of any step and simplify your cloud\ndeployment loops by separating large model deployments from quick\nupdates to glue code.\n6\nIncremental adoption\nUse Chains to orchestrate existing model deployments, like pre-packaged\nmodels from Baseten\u2019s model library, alongside new model pipelines built\nentirely within Chains.\n\u200b\nHello World with Chains\nHere\u2019s a simple Chain that says \u201chello\u201d to each person in a list of provided\nnames:\nhello_chain/hello.py\nCopy\nAsk AI\nimport\nasyncio\nimport\ntruss_chains\nas\nchains\n# This Chainlet does the work.\nclass\nSayHello\n(\nchains\n.\nChainletBase\n):\nasync\ndef\nrun_remote\n(\nself\n,\nname\n:\nstr\n) ->\nstr\n:\nreturn\nf\n\"Hello,\n{\nname\n}\n\"\n# This Chainlet orchestrates the work.\n@chains.mark_entrypoint\nclass\nHelloAll\n(\nchains\n.\nChainletBase\n):\ndef\n__init__\n(\nself\n,\nsay_hello_chainlet\n=\nchains.depends(SayHello)) ->\nNone\n:\nself\n._say_hello\n=\nsay_hello_chainlet\nasync\ndef\nrun_remote\n(\nself\n,\nnames\n: list[\nstr\n]) ->\nstr\n:\ntasks\n=\n[]\nfor\nname\nin\nnames:\ntasks.append(asyncio.ensure_future(\nself\n._say_hello.run_remote(name)))\nreturn\n\"\n\\n\n\"\n.join(\nawait\nasyncio.gather(\n*\ntasks))\nThis is a toy example, but it shows how Chains can be used to separate\npreprocessing steps like chunking from workload execution steps. If SayHello\nwere an LLM instead of a simple string template, we could do a much more complex\naction for each person on the list.\n\u200b\nWhat to build with Chains\nRAG: retrieval-augmented generation\nConnect to a vector databases and augment LLM results with additional\ncontext information without introducing overhead to the model inference\nstep.\nTry it yourself:\nRAG Chain\n.\nChunked Audio Transcription and high-throughput pipelines\nTranscribe large audio files by splitting them into smaller chunks and\nprocessing them in parallel \u2014 we\u2019ve used this approach to process 10-hour\nfiles in minutes.\nTry it yourself:\nAudio Transcription Chain\n.\nEfficient multi-model pipelines\nBuild powerful experiences wit optimal scaling in each step like:\nAI phone calling (transcription + LLM + speech synthesis)\nMulti-step image generation (SDXL + LoRAs + ControlNets)\nMultimodal chat (LLM + vision + document parsing + audio)\nSince each stage runs on its hardware with independent auto-scaling,\nyou chan achieve better hardware utilization and save costs.\nGet started by\nbuilding and deploying your first chain\n.\nWas this page helpful?\nYes\nNo\nPrevious\nConcepts\nGlossary of Chains concepts and terminology\nNext\nOn this page\nUser Guides\nFrom model to system\nSix principles behind Chains\nHello World with Chains\nWhat to build with Chains\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 84473, "end_char_idx": 88624, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a7a9fe00-0761-420c-aa86-6f09e532e997": {"__data__": {"id_": "a7a9fe00-0761-420c-aa86-6f09e532e997", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "48041027-0da9-4fef-9cfb-9abad4b9fa31", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "626ad5a044d87e4ddf3d0d946f0cd7f367cf3278accf5498383a7f2c10c0bc40", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f136077d-cfa8-4e46-9428-70f8a35a3cd9", "node_type": "1", "metadata": {}, "hash": "bf79661e24438f5614a6cf7ca3024e14cd931d405f36227aabbb2723c323d281", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Try it yourself:\nAudio Transcription Chain\n.\nEfficient multi-model pipelines\nBuild powerful experiences wit optimal scaling in each step like:\nAI phone calling (transcription + LLM + speech synthesis)\nMulti-step image generation (SDXL + LoRAs + ControlNets)\nMultimodal chat (LLM + vision + document parsing + audio)\nSince each stage runs on its hardware with independent auto-scaling,\nyou chan achieve better hardware utilization and save costs.\nGet started by\nbuilding and deploying your first chain\n.\nWas this page helpful?\nYes\nNo\nPrevious\nConcepts\nGlossary of Chains concepts and terminology\nNext\nOn this page\nUser Guides\nFrom model to system\nSix principles behind Chains\nHello World with Chains\nWhat to build with Chains\nAssistant\nResponses are generated using AI and may contain mistakes.\n\n\nContent from https://docs.baseten.co/development/chain/streaming:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nDeveloping a Chain\nOverview\nConcepts\nYour first Chain\nArchitecture & Design\nLocal Development\nDeploy\nInvocation\nWatch\nSubclassing\nStreaming\nBinary IO\nError Handling\nTruss Integration\nEngine Builder Models\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nExporting metrics\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nDeveloping a Chain\nStreaming\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nStreaming outputs is useful for returning partial results to the client, before\nall data has been processed.\nFor example LLM text generation happens in incremental text chunks, so the\nbeginning of the reply can already be sent to the client before the whole\nprediction is complete.\nSimilarly, transcribing audio to text happens in ~30 second chunks and the\nfirst ones can be returned before all completed.\nIn general, this does not reduce the overall processing time (still the same\namount of work must be done), but the initial latency to get some response\ncan be reduced significantly.\nIn some cases it might even reduce overall time, when streaming results\ninternally in a Chain, allows to start subsequent processing steps sooner -\ni.e. pipelining the operations in a more efficient way.\n\u200b\nLow-level streaming\nLow-level, streaming works by sending byte chunks (unicode strings will be\nimplicitly encoded) via HTTP. The most primitive way of doing this in Chains\nis by implementing\nrun_remote\nas a bytes- or string-iterator, e.g.:\nCopy\nAsk AI\nfrom\ntyping\nimport\nAsyncIterator\nimport\ntruss_chains\nas\nchains\nclass\nStreamlet\n(\nchains\n.\nChainletBase\n):\nasync\ndef\nrun_remote\n(\nself\n,\ninputs\n:\n...\n) -> AsyncIterator[\nstr\n]:\nasync\nfor\ntext_chunk\nin\nmake_incremental_outputs(inputs):\nyield\ntext_chunk\nYou are free to chose what data to represent in the byte/string chunks, it\ncould be raw text generated by an LLM, it could be JSON string, bytes or\nanything else.\n\u200b\nServer-sent events (SSEs)\nA possible choice is to generate chunks that comply with the\nspecification\nof server-sent events.\nConcretely, sending JSON strings with\ndata\n,\nevent\nand potentially\nother fields and content-type\ntext/event-stream\n.\nHowever, the SSE specification is not opinionated regarding what exactly is\nencoded in\ndata\nand what\nevent\n-types exist - you have to make up your schema\nthat is useful for the client that consumes the data.\n\u200b\nPydantic and Chainlet-Chainlet-streams\nWhile above low-level streaming is stable, the following helper APIs for typed\nstreaming are only stable for intra-Chain streaming.\nIf you want to use them for end clients, please reach out to Baseten support,\nso we can discuss the stable solutions.\nUnlike above \u201craw\u201d stream example, Chains takes the general opinion that\ninput and output types should be definite, so that divergence and type\nerrors can be avoided.", "mimetype": "text/plain", "start_char_idx": 87831, "end_char_idx": 92002, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f136077d-cfa8-4e46-9428-70f8a35a3cd9": {"__data__": {"id_": "f136077d-cfa8-4e46-9428-70f8a35a3cd9", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a7a9fe00-0761-420c-aa86-6f09e532e997", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ea7c5ccc331fa7ab6013017454427c97c3f6e90777625e45768fa633bf0c3f03", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c9dbb20c-6eec-4fe9-a790-4f71ffb6f519", "node_type": "1", "metadata": {}, "hash": "4754345cfb90f2067d8b92513ac7aeb7674d3b5cbaf9de8d9dc449953a00f622", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Concretely, sending JSON strings with\ndata\n,\nevent\nand potentially\nother fields and content-type\ntext/event-stream\n.\nHowever, the SSE specification is not opinionated regarding what exactly is\nencoded in\ndata\nand what\nevent\n-types exist - you have to make up your schema\nthat is useful for the client that consumes the data.\n\u200b\nPydantic and Chainlet-Chainlet-streams\nWhile above low-level streaming is stable, the following helper APIs for typed\nstreaming are only stable for intra-Chain streaming.\nIf you want to use them for end clients, please reach out to Baseten support,\nso we can discuss the stable solutions.\nUnlike above \u201craw\u201d stream example, Chains takes the general opinion that\ninput and output types should be definite, so that divergence and type\nerrors can be avoided.\nJust like you type-annotate Chainlet inputs and outputs in the non-streaming\ncase, and use pydantic to manage more complex data structures, we built\ntooling to bring the same benefits to streaming.\n\u200b\nHeaders and footers\nThis also helps to solve another challenge of streaming: you might want to\nsent data of different kinds at the beginning or end of a stream than in\nthe \u201cmain\u201d part.\nFor example if you transcribe an audio file, you might want\nto send many transcription segments in a stream and at the end send some\naggregate information such as duration, detected languages etc.\nWe model typed streaming like this:\n[optionally] send a chunk that conforms to the schema of a\nHeader\npydantic\nmodel.\nSend 0 to N chunks each conforming to the schema of an\nItem\npydantic\nmodel.\n[optionally] send a chunk that conforms to the schema of a\nFooter\npydantic\nmodel.\n\u200b\nAPIs\n\u200b\nStreamTypes\nTo have a single source of truth for the types that can be shared between\nthe producing Chainlet and the consuming client (either a Chainlet in the\nChain or an external client), the chains framework uses a\nStreamType\n-object:\nCopy\nAsk AI\nimport\npydantic\nfrom\ntruss_chains\nimport\nstreaming\nclass\nMyDataChunk\n(\npydantic\n.\nBaseModel\n):\nwords: list[\nstr\n]\nSTREAM_TYPES\n=\nstreaming.stream_types(\nMyDataChunk,\nheader_type\n=\n...\n,\nfooter_type\n=\n...\n)\nNote that header and footer types are optional and can be left out:\nCopy\nAsk AI\nSTREAM_TYPES\n=\nstreaming.stream_types(MyDataChunk)\n\u200b\nStreamWriter\nUse the\nSTREAM_TYPES\nto create a matching stream writer:\nCopy\nAsk AI\nfrom\ntyping\nimport\nAsyncIterator\nimport\npydantic\nimport\ntruss_chains\nas\nchains\nfrom\ntruss_chains\nimport\nstreaming\nclass\nMyDataChunk\n(\npydantic\n.\nBaseModel\n):\nwords: list[\nstr\n]\nSTREAM_TYPES\n=\nstreaming.stream_types(MyDataChunk)\nclass\nStreamlet\n(\nchains\n.\nChainletBase\n):\nasync\ndef\nrun_remote\n(\nself\n,\ninputs\n:\n...\n) -> AsyncIterator[\nbytes\n]:\nstream_writer\n=\nstreaming.stream_writer(\nSTREAM_TYPES\n)\nasync\nfor\nitem\nin\nmake_pydantic_items(inputs):\nyield\nstream_writer.yield_item(item)\nIf your stream types have header or footer types, corresponding\nyield_header\nand\nyield_footer\nmethods are available on the writer.\nThe writer serializes the pydantic data to\nbytes\n, so you can also\nefficiently represent numeric data (see the\nbinary IO guide\n).\n\u200b\nStreamReader\nTo consume the stream on either another Chainlet or in the external client, an\nmatching\nStreamReader\nis created form your\nStreamTypes\n. Besides the\ntypes, you connect the reader to the bytes generator that you obtain from the\nremote invocation of the streaming Chainlet:\nCopy\nAsk AI\nimport\ntruss_chains\nas\nchains\nfrom\ntruss_chains\nimport\nstreaming\nclass\nConsumer\n(\nchains\n.\nChainletBase\n):\ndef\n__init__\n(\nself\n,\nstreamlet\n=\nchains.depends(Streamlet)):\nself\n._streamlet\n=\nstreamlet\nasync\ndef\nrun_remote\n(\nself\n,\ndata\n:\n...\n):\nbyte_stream\n=\nself\n._streamlet.run_remote(data)\nreader\n=\nstreaming.stream_reader(\nSTREAM_TYPES\n, byte_stream)\nchunks\n=\n[]\nasync\nfor\ndata\nin\nreader.read_items():\nchunks.append(data)\nIf you use headers or footers, the reader has async\nread_header\nand\nread_footer\nmethods.", "mimetype": "text/plain", "start_char_idx": 91220, "end_char_idx": 95094, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c9dbb20c-6eec-4fe9-a790-4f71ffb6f519": {"__data__": {"id_": "c9dbb20c-6eec-4fe9-a790-4f71ffb6f519", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f136077d-cfa8-4e46-9428-70f8a35a3cd9", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "26ae0ae4a2f611cf18fdd39f23592d0b5558e5ac705d280cc8c9159b2d10333c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a6d19960-2a44-4207-b169-cfb9751e2e36", "node_type": "1", "metadata": {}, "hash": "035f98c4813e3fc8e0342e142a46b7a14b7ef02c4bd6aa7d91a8518230e100f6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Besides the\ntypes, you connect the reader to the bytes generator that you obtain from the\nremote invocation of the streaming Chainlet:\nCopy\nAsk AI\nimport\ntruss_chains\nas\nchains\nfrom\ntruss_chains\nimport\nstreaming\nclass\nConsumer\n(\nchains\n.\nChainletBase\n):\ndef\n__init__\n(\nself\n,\nstreamlet\n=\nchains.depends(Streamlet)):\nself\n._streamlet\n=\nstreamlet\nasync\ndef\nrun_remote\n(\nself\n,\ndata\n:\n...\n):\nbyte_stream\n=\nself\n._streamlet.run_remote(data)\nreader\n=\nstreaming.stream_reader(\nSTREAM_TYPES\n, byte_stream)\nchunks\n=\n[]\nasync\nfor\ndata\nin\nreader.read_items():\nchunks.append(data)\nIf you use headers or footers, the reader has async\nread_header\nand\nread_footer\nmethods.\nNote that the stream can only be consumed once and you have to consume\nheader, items and footer in order.\nThe implementation of\nStreamReader\nonly needs\npydantic\n, no other Chains\ndependencies. So you can take that implementation code in isolation and\nintegrated it in your client code.\nWas this page helpful?\nYes\nNo\nPrevious\nBinary IO\nPerformant serialization of numeric data\nNext\nOn this page\nLow-level streaming\nServer-sent events (SSEs)\nPydantic and Chainlet-Chainlet-streams\nHeaders and footers\nAPIs\nStreamTypes\nStreamWriter\nStreamReader\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 94436, "end_char_idx": 95705, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a6d19960-2a44-4207-b169-cfb9751e2e36": {"__data__": {"id_": "a6d19960-2a44-4207-b169-cfb9751e2e36", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c9dbb20c-6eec-4fe9-a790-4f71ffb6f519", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "89ebaa0c52963b66e9fca42e33ed7969310e053e7b47c70d98b8e4369d4fcca7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b9d5ed90-13d6-4d84-bf61-245566012bec", "node_type": "1", "metadata": {}, "hash": "55b6e02694549b7d9397cbe9832bce02594cfc025a7c0a6ce94cde1ac43d26c4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Note that the stream can only be consumed once and you have to consume\nheader, items and footer in order.\nThe implementation of\nStreamReader\nonly needs\npydantic\n, no other Chains\ndependencies. So you can take that implementation code in isolation and\nintegrated it in your client code.\nWas this page helpful?\nYes\nNo\nPrevious\nBinary IO\nPerformant serialization of numeric data\nNext\nOn this page\nLow-level streaming\nServer-sent events (SSEs)\nPydantic and Chainlet-Chainlet-streams\nHeaders and footers\nAPIs\nStreamTypes\nStreamWriter\nStreamReader\nAssistant\nResponses are generated using AI and may contain mistakes.\n\n\nContent from https://docs.baseten.co/development/chain/stub:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nDeveloping a Chain\nOverview\nConcepts\nYour first Chain\nArchitecture & Design\nLocal Development\nDeploy\nInvocation\nWatch\nSubclassing\nStreaming\nBinary IO\nError Handling\nTruss Integration\nEngine Builder Models\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nExporting metrics\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nDeveloping a Chain\nTruss Integration\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nChains can be combined with existing Truss models using Stubs.\nA Stub acts as a substitute (client-side proxy) for a remotely deployed\ndependency, either a Chainlet or a Truss model. The Stub performs the remote\ninvocations as if it were local by taking care of the transport layer,\nauthentication, data serialization and retries.\nStubs can be integrated into Chainlets by passing in a URL of the deployed\nmodel. They also require\ncontext\nto be initialized\n(for authentication).\nCopy\nAsk AI\nimport\ntruss_chains\nas\nchains\nclass\nLLMClient\n(\nchains\n.\nStubBase\n):\nasync\ndef\nrun_remote\n(\nself\n,\nprompt\n:\nstr\n) ->\nstr\n:\n# Call the deployed model\nresp\n=\nawait\nself\n.predict_async(\ninputs\n=\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: prompt}],\n\"stream\"\n:\nFalse\n})\n# Return a string with the model output\nreturn\nresp[\n\"output\"\n]\nLLM_URL\n=\n...\nclass\nMyChainlet\n(\nchains\n.\nChainletBase\n):\ndef\n__init__\n(\nself\n,\ncontext\n: chains.DeploymentContext\n=\nchains.depends_context(),\n):\nself\n._llm\n=\nLLMClient.from_url(\nLLM_URL\n, context)\nThere are various ways how you can make a call to the other deployment:\nInput as JSON dict (like above) or pydantic model.\nAutomatic parsing of the response into an pydantic model using the\noutput_model\nargument.\npredict_async\n(recommended) or\npredict_async\n.\nStreaming responses using\npredict_async_stream\nwhich returns an async\nbytes iterator.\nCustomized with\nRPCOptions\n.\nSee the\nStubBase reference\nfor all APIS.\nWas this page helpful?\nYes\nNo\nPrevious\nEngine Builder Models\nEngine Builder models are pre-trained models that are optimized for specific inference tasks.\nNext\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 95095, "end_char_idx": 98385, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b9d5ed90-13d6-4d84-bf61-245566012bec": {"__data__": {"id_": "b9d5ed90-13d6-4d84-bf61-245566012bec", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a6d19960-2a44-4207-b169-cfb9751e2e36", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "1c0145e99b2778ed9251d97c6984b58e26a93e65cbe8677859abafcaed8811ad", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fbcb1b65-21f3-4a94-b7cb-0ffd09e7a4b2", "node_type": "1", "metadata": {}, "hash": "456ab95c6cf76054cc24b474a5861e02a9aa948f9d5f4f454d371d715c2eb340", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Content from https://docs.baseten.co/development/chain/subclassing:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nDeveloping a Chain\nOverview\nConcepts\nYour first Chain\nArchitecture & Design\nLocal Development\nDeploy\nInvocation\nWatch\nSubclassing\nStreaming\nBinary IO\nError Handling\nTruss Integration\nEngine Builder Models\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nExporting metrics\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nDeveloping a Chain\nSubclassing\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nSometimes you want to write one \u201cmain\u201d implementation of a complicated inference\ntask, but then re-use it for similar variations. For example:\nDeploy it on different hardware and with different concurrency.\nReplace a dependency (e.g. silence detection in audio files) with a\ndifferent implementation of that step - while keeping all other processing\nthe same.\nDeploy the same inference flow, but exchange the model weights used. E.g. for\na large and small version of an LLM or different model weights fine-tuned to\ndomains.\nAdd an adapter to convert between a different input/output schema.\nIn all of those cases, you can create lightweight subclasses of your main\nchainlet.\nBelow are some example code snippets - they can all be combined with each other!\n\u200b\nExample base class\nCopy\nAsk AI\nimport\nasyncio\nimport\ntruss_chains\nas\nchains\nclass\nPreprocess2x\n(\nchains\n.\nChainletBase\n):\nasync\ndef\nrun_remote\n(\nself\n,\nnumber\n:\nint\n) ->\nint\n:\nreturn\n2\n*\nnumber\nclass\nMyBaseChainlet\n(\nchains\n.\nChainletBase\n):\nremote_config\n=\nchains.RemoteConfig(\ncompute\n=\nchains.Compute(\ncpu_count\n=\n1\n,\nmemory\n=\n\"100Mi\"\n),\noptions\n=\nchains.ChainletOptions(\nenable_b10_tracing\n=\nTrue\n),\n)\ndef\n__init__\n(\nself\n,\npreprocess\n=\nchains.depends(Preprocess2x)):\nself\n._preprocess\n=\npreprocess\nasync\ndef\nrun_remote\n(\nself\n,\nnumber\n:\nint\n) ->\nfloat\n:\nreturn\n1.0\n/\nawait\nself\n._preprocess.run_remote(number)\n# Assert base behavior.\nwith\nchains.run_local():\nchainlet\n=\nMyBaseChainlet()\nresult\n=\nasyncio.get_event_loop().run_until_complete(chainlet.run_remote(\n4\n))\nassert\nresult\n==\n1\n/\n(\n4\n*\n2\n)\n\u200b\nAdapter for different I/O\nThe base class\nMyBaseChainlet\nworks with integer inputs and returns floats. If\nyou want to reuse the computation, but provide an alternative interface (e.g.\nfor a different client with different request/response schema), you can create\na subclass which does the I/O conversion. The actual computation is delegated to\nthe base classes above.\nCopy\nAsk AI\nclass\nChainletStringIO\n(\nMyBaseChainlet\n):\nasync\ndef\nrun_remote\n(\nself\n,\nnumber\n:\nstr\n) ->\nstr\n:\nreturn\nstr\n(\nawait\nsuper\n().run_remote(\nint\n(number)))\n# Assert new behavior.\nwith\nchains.run_local():\nchainlet_string_io\n=\nChainletStringIO()\nresult\n=\nasyncio.get_event_loop().run_until_complete(\nchainlet_string_io.run_remote(\n\"4\"\n))\nassert\nresult\n==\n\"0.125\"\n\u200b\nChain with substituted dependency\nThe base class\nMyBaseChainlet\nuses preprocessing that doubles the input. If\nyou want to use a different variant of preprocessing - while keeping\nMyBaseChainlet.run_remote\nand everything else as is - you can define a shallow\nsubclass of\nMyBaseChainlet\nwhere you use a different dependency\nPreprocess8x\n, which multiplies by 8 instead of 2.\nCopy\nAsk AI\nclass\nPreprocess8x\n(\nchains\n.", "mimetype": "text/plain", "start_char_idx": 98388, "end_char_idx": 102113, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fbcb1b65-21f3-4a94-b7cb-0ffd09e7a4b2": {"__data__": {"id_": "fbcb1b65-21f3-4a94-b7cb-0ffd09e7a4b2", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b9d5ed90-13d6-4d84-bf61-245566012bec", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "69ddf75e79580613dc37b16c75ad20a37dae5b04cd448d2363757b6713ce0f68", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2966a825-d7a4-44e9-adca-dd45b773d7b5", "node_type": "1", "metadata": {}, "hash": "2a54e6323f66cd13904bcf9f04fee93546bfff4f16207ac0e0f1c4af739154a2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "with\nchains.run_local():\nchainlet_string_io\n=\nChainletStringIO()\nresult\n=\nasyncio.get_event_loop().run_until_complete(\nchainlet_string_io.run_remote(\n\"4\"\n))\nassert\nresult\n==\n\"0.125\"\n\u200b\nChain with substituted dependency\nThe base class\nMyBaseChainlet\nuses preprocessing that doubles the input. If\nyou want to use a different variant of preprocessing - while keeping\nMyBaseChainlet.run_remote\nand everything else as is - you can define a shallow\nsubclass of\nMyBaseChainlet\nwhere you use a different dependency\nPreprocess8x\n, which multiplies by 8 instead of 2.\nCopy\nAsk AI\nclass\nPreprocess8x\n(\nchains\n.\nChainletBase\n):\nasync\ndef\nrun_remote\n(\nself\n,\nnumber\n:\nint\n) ->\nint\n:\nreturn\n8\n*\nnumber\nclass\nChainlet8xPreprocess\n(\nMyBaseChainlet\n):\ndef\n__init__\n(\nself\n,\npreprocess\n=\nchains.depends(Preprocess8x)):\nsuper\n().\n__init__\n(\npreprocess\n=\npreprocess)\n# Assert new behavior.\nwith\nchains.run_local():\nchainlet_8x_preprocess\n=\nChainlet8xPreprocess()\nresult\n=\nasyncio.get_event_loop().run_until_complete(\nchainlet_8x_preprocess.run_remote(\n4\n))\nassert\nresult\n==\n1\n/\n(\n4\n*\n8\n)\n\u200b\nOverride remote config.\nIf you want to re-deploy a chain, but change some deployment options, e.g. run\non different hardware, you can create a subclass and override\nremote_config\n.\nCopy\nAsk AI\nclass\nChainlet16Core\n(\nMyBaseChainlet\n):\nremote_config\n=\nchains.RemoteConfig(\ncompute\n=\nchains.Compute(\ncpu_count\n=\n16\n,\nmemory\n=\n\"100Mi\"\n),\noptions\n=\nchains.ChainletOptions(\nenable_b10_tracing\n=\nTrue\n),\n)\nBe aware that\nremote_config\nis a class variable. In the example above we\ncreated a completely new\nRemoteConfig\nvalue, because changing fields\ninplace\nwould also affect the base class.\nIf you want to share config between the base class and subclasses, you can\ndefine them in additional variables e.g. for the image:\nCopy\nAsk AI\nDOCKER_IMAGE\n=\nchains.DockerImage(\npip_requirements\n=\n[\n...\n],\n...\n)\nclass\nMyBaseChainlet\n(\nchains\n.\nChainletBase\n):\nremote_config\n=\nchains.RemoteConfig(\ndocker_image\n=\nDOCKER_IMAGE\n,\n...\n)\nclass\nChainlet16Core\n(\nMyBaseChainlet\n):\nremote_config\n=\nchains.RemoteConfig(\ndocker_image\n=\nDOCKER_IMAGE\n,\n...\n)\nWas this page helpful?\nYes\nNo\nPrevious\nStreaming\nStreaming outputs, reducing latency, SSEs\nNext\nOn this page\nExample base class\nAdapter for different I/O\nChain with substituted dependency\nOverride remote config.\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 101515, "end_char_idx": 103894, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2966a825-d7a4-44e9-adca-dd45b773d7b5": {"__data__": {"id_": "2966a825-d7a4-44e9-adca-dd45b773d7b5", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fbcb1b65-21f3-4a94-b7cb-0ffd09e7a4b2", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "8bb20204fc28a952ad6e71bd73bab371050d82f1edb558554400222295c32855", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aedff500-b446-415a-bd6d-0e47d0c8ede5", "node_type": "1", "metadata": {}, "hash": "6afeced75ddb6dd1131f7369f8ea933fb44a131f55ff1a4954b0d17963b11947", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "If you want to share config between the base class and subclasses, you can\ndefine them in additional variables e.g. for the image:\nCopy\nAsk AI\nDOCKER_IMAGE\n=\nchains.DockerImage(\npip_requirements\n=\n[\n...\n],\n...\n)\nclass\nMyBaseChainlet\n(\nchains\n.\nChainletBase\n):\nremote_config\n=\nchains.RemoteConfig(\ndocker_image\n=\nDOCKER_IMAGE\n,\n...\n)\nclass\nChainlet16Core\n(\nMyBaseChainlet\n):\nremote_config\n=\nchains.RemoteConfig(\ndocker_image\n=\nDOCKER_IMAGE\n,\n...\n)\nWas this page helpful?\nYes\nNo\nPrevious\nStreaming\nStreaming outputs, reducing latency, SSEs\nNext\nOn this page\nExample base class\nAdapter for different I/O\nChain with substituted dependency\nOverride remote config.\nAssistant\nResponses are generated using AI and may contain mistakes.\n\n\nContent from https://docs.baseten.co/development/chain/watch:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nDeveloping a Chain\nOverview\nConcepts\nYour first Chain\nArchitecture & Design\nLocal Development\nDeploy\nInvocation\nWatch\nSubclassing\nStreaming\nBinary IO\nError Handling\nTruss Integration\nEngine Builder Models\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nExporting metrics\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nDeveloping a Chain\nWatch\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nThe\nwatch command\n(\ntruss chains watch\n) combines\nthe best of local development and full deployment.\nwatch\nlets you run on an\nexact copy of the production hardware and interface but gives you live code\npatching that lets you test changes in seconds without creating a new\ndeployment.\nTo use\ntruss chains watch\n:\nPush a chain in development mode (i.e.\npublish\nand\npromote\nflags are\nfalse).\nRun the watch command\ntruss chains watch SOURCE\n. You can also add the\nwatch\noption to the\npush\ncommand and combine both to a single step.\nEach time you edit a file and save the changes, the watcher patches the\nremote deployments. Updating the deployments might take a moment, but is\ngenerally\nmuch\nfaster than creating a new deployment.\nYou can call the chain with test data via\ncURL\nor the playground dialogue\nin the UI and observe the result and logs.\nIterate steps 3. and 4. until your chain behaves in the desired way.\n\u200b\nSelective Watch\nSome large ML models might have a slow cycle time to reload (e.g. if the\nweights are huge). For this case, we provide a \u201cselective\u201d watch option. For\nexample if you chain has such a heavy model Chainlet and other Chainlets\nthat contain only business logic, you can iterate on those, while not patching\nand reloading the heavy model Chainlet.\nThis feature is really useful for advanced use case, but must be used with\ncaution.\nIf you change the code of a Chainlet not watched, in particular I/O types,\nyou get an inconsistent deployment.\nAdd the Chainlet names you want to watch as a comma separated list:\nCopy\nAsk AI\ntruss\nchains\nwatch\n...\n--experimental-chainlet-names=ChainletA,ChainletB\nWas this page helpful?\nYes\nNo\nPrevious\nSubclassing\nModularize and re-use Chainlet implementations\nNext\nOn this page\nSelective Watch\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 103167, "end_char_idx": 106717, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "aedff500-b446-415a-bd6d-0e47d0c8ede5": {"__data__": {"id_": "aedff500-b446-415a-bd6d-0e47d0c8ede5", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2966a825-d7a4-44e9-adca-dd45b773d7b5", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "3c6fd699db9f6300ac872373c1b8c4f3bd87b5665c66768a92eb7b5fb21e721b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "49c78894-9ad2-43d4-aaaa-801a8f0ea6b4", "node_type": "1", "metadata": {}, "hash": "65d47fe00fdf6ddcb0e3b00aeb690a6b49cecd453b5b87b46e6edf6286276f3a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Content from https://docs.baseten.co/development/concepts:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nDeveloping a Chain\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nExporting metrics\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nDevelopment\nConcepts\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nBaseten provides two core development workflows:\ndeveloping a model with Truss\nand orchestrating models with\nChains\n. Both are building blocks for production-grade ML systems, but they solve different problems.\nDeveloping a model with Truss\nPackage and deploy any AI/ML model as an API with Truss or a Custom Server.\nDeveloping a Chain\nOrchestrate multiple models and logic, enabling complex inference workflows.\n\u200b\nTruss vs. Chains: When to use each\n\u200b\nDeveloping a model with Truss\nTruss\nis the open-source package you use to turn any ML model into a production-ready API on Baseten - without needing to learn Docker or build custom infrastructure.\nUse Truss when:\nYou\u2019re deploying a single model.\nWhether it\u2019s a fine-tuned transformer, diffusion model, or traditional classifier, Truss helps you package it with code, configuration, and system requirements to deploy at scale.\nYou want flexibility across tools and frameworks.\nBuild with your preferred model frameworks (e.g.\nPyTorch\n,\ntransformers\n,\ndiffusers\n), inference engines (e.g.\nTensorRT-LLM\n,\nSGLang\n,\nvLLM\n), and serving technologies (like\nTriton\n) as well as\nany package\ninstallable via\npip\nor\napt\n.\nYou need control over how your model runs.\nDefine pre- and post-processing, batching, logging, and custom inference logic. Truss gives you full access to environment settings and dependencies, versioned and deployable.\nYou want to keep development local and reproducible.\nDevelop locally in a containerized environment that mirrors production, test confidently, and ship your model without surprises.\n\u200b\nOrchestrating with Chains\nChains\nare for building inference workflows that span multiple steps, models, or tools. You define a sequence of steps \u2014 like routing, transformation, or chaining outputs \u2014 and run them as a single unit.\nUse Chains when:\nYou\u2019re combining multiple models or tools.\nFor example, running a vector search + LLM pipeline, or combining OCR, classification, and validation steps.\nYou want visibility into intermediate steps.\nChains let you debug and monitor each part of the workflow, retry failed steps, and trace outputs with ease \u2014 something that\u2019s much harder with a single model endpoint.\nYou\u2019re using orchestration libraries like LangChain or LlamaIndex.\nChains integrate natively with these frameworks, while still allowing you to insert your own logic or wrap Truss models as steps.\nWas this page helpful?\nYes\nNo\nPrevious\nOverview\nUse Baseten's OpenAI-compatible Model APIs for LLMs, including structured outputs and tool calling.\nNext\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 106720, "end_char_idx": 110123, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "49c78894-9ad2-43d4-aaaa-801a8f0ea6b4": {"__data__": {"id_": "49c78894-9ad2-43d4-aaaa-801a8f0ea6b4", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "aedff500-b446-415a-bd6d-0e47d0c8ede5", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ab2469181eaf671724fc0b31b49d62bb2644258527ef9d0d3736812d4498d565", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d06066b2-9f34-4b6d-8b6a-bb5da37bdc09", "node_type": "1", "metadata": {}, "hash": "6be03b9861ce786ba39cf8d875a392899011f9be4a129c8984643f338b971b80", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Content from https://docs.baseten.co/development/model-apis/overview:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nOverview\nRate Limits & Budgets\nDeveloping a model\nDeveloping a Chain\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nExporting metrics\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nModel APIs\nUsing Model APIs on Baseten\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nBaseten provides\nOpenAI-compatible API endpoints\nfor all available Model APIs. This means you can use standard OpenAI client libraries\u2014no wrappers, no rewrites, no surprises. If your code already works with OpenAI, it\u2019ll work with Baseten.\nThis guide walks you through getting started, making your first call, and using advanced features like structured outputs and tool calling.\n\u200b\nPrerequisites\nBefore you begin, make sure you have:\nA\nBaseten account\nAn\nAPI key\nThe\nOpenAI client library\nfor your language of choice\n\u200b\nSupported models\nBaseten currently offers several high-performing open-source LLMs as\nModels APIs\n:\nDeepseek R1 0528\n(slug:\ndeepseek-ai/DeepSeek-R1-0528\n)\nDeepseek V3 0324\n(slug:\ndeepseek-ai/DeepSeek-V3-0324\n)\nLlama 4 Maverick\n(slug:\nmeta-llama/Llama-4-Maverick-17B-128E-Instruct\n)\nLlama 4 Scout\n(slug:\nmeta-llama/Llama-4-Scout-17B-16E-Instruct\n)\nQwen 3\n\ud83d\udd1c\nPlease update the\nmodel\nin the examples below to the slug of the model you\u2019d like to test.\n\u200b\nMake your first API call\nPython\nJavaScript\ncURL\nCopy\nAsk AI\nclient\n=\nOpenAI(\nbase_url\n=\n\"https://inference.baseten.co/v1\"\n,\napi_key\n=\nos.environ.get(\n\"BASETEN_API_KEY\"\n)\n)\nresponse\n=\nclient.chat.completions.create(\nmodel\n=\n\"deepseek-ai/DeepSeek-V3-0324\"\n,\nmessages\n=\n[\n{\n\"role\"\n:\n\"system\"\n,\n\"content\"\n:\n\"You are a helpful assistant.\"\n},\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"Your question here\"\n}\n]\n)\nprint\n(response.choices[\n0\n].message.content)\nCopy\nAsk AI\nclient\n=\nOpenAI(\nbase_url\n=\n\"https://inference.baseten.co/v1\"\n,\napi_key\n=\nos.environ.get(\n\"BASETEN_API_KEY\"\n)\n)\nresponse\n=\nclient.chat.completions.create(\nmodel\n=\n\"deepseek-ai/DeepSeek-V3-0324\"\n,\nmessages\n=\n[\n{\n\"role\"\n:\n\"system\"\n,\n\"content\"\n:\n\"You are a helpful assistant.\"\n},\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"Your question here\"\n}\n]\n)\nprint\n(response.choices[\n0\n].message.content)\nCopy\nAsk AI\nconst\nclient\n=\nnew\nOpenAI\n({\nbaseURL:\n\"https://inference.baseten.co/v1\"\n,\napiKey:\nprocess\n.\nenv\n.\nBASETEN_API_KEY\n,\n});\n// Use the client\ntry\n{\nconst\nresponse\n=\nawait\nclient\n.\nchat\n.\ncompletions\n.\ncreate\n({\nmodel:\n\"deepseek-ai/DeepSeek-V3-0324\"\n,\nmessages:\n[\n{\nrole:\n\"system\"\n,\ncontent:\n\"You are a helpful assistant.\"\n},\n{\nrole:\n\"user\"\n,\ncontent:\n\"Hello, how are you?\"", "mimetype": "text/plain", "start_char_idx": 110126, "end_char_idx": 113176, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d06066b2-9f34-4b6d-8b6a-bb5da37bdc09": {"__data__": {"id_": "d06066b2-9f34-4b6d-8b6a-bb5da37bdc09", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "49c78894-9ad2-43d4-aaaa-801a8f0ea6b4", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "b08afea2d95b8b74ae4e5562c2ef062b00899e1cd31d177e84e4a135f09660f3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9ed8b590-d78d-4563-8eaf-c0b9943087a2", "node_type": "1", "metadata": {}, "hash": "d28d7b92162e9133eedc06056d6e2d5fc248f67809030e8c914fae989b12efef", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "},\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"Your question here\"\n}\n]\n)\nprint\n(response.choices[\n0\n].message.content)\nCopy\nAsk AI\nconst\nclient\n=\nnew\nOpenAI\n({\nbaseURL:\n\"https://inference.baseten.co/v1\"\n,\napiKey:\nprocess\n.\nenv\n.\nBASETEN_API_KEY\n,\n});\n// Use the client\ntry\n{\nconst\nresponse\n=\nawait\nclient\n.\nchat\n.\ncompletions\n.\ncreate\n({\nmodel:\n\"deepseek-ai/DeepSeek-V3-0324\"\n,\nmessages:\n[\n{\nrole:\n\"system\"\n,\ncontent:\n\"You are a helpful assistant.\"\n},\n{\nrole:\n\"user\"\n,\ncontent:\n\"Hello, how are you?\"\n},\n],\n})\nCopy\nAsk AI\ncurl\nhttps://inference.baseten.co/v1/chat/completions\n\\\n-H\n\"Content-Type: application/json\"\n\\\n-H\n\"Authorization: Api-Key\n$BASETEN_API_KEY\n\"\n\\\n-d\n'{\n\"model\": \"deepseek-ai/DeepSeek-V3-0324\",\n\"messages\": [{ \"role\": \"user\", \"content\": \"Your content here\" }],\n\"stream\": true,\n\"max_tokens\": 10\n}'\necho\n# Add a newline for cleaner output\n\u200b\nRequest parameters\nModel APIs support all commonly used\nOpenAI ChatCompletions\nparameters, including:\nmodel\n: Slug of the model you want to call (see below)\nmessages\n: Array of message objects (\nrole\n+\ncontent\n)\ntemperature\n: Controls randomness (0-2, default 1)\nmax_tokens\n: Maximum number of tokens to generate\nstream\n: Boolean to enable streaming responses\n\u200b\nStructured outputs\nTo get structured JSON output from the model, you can use the\nresponse_format\nparameter. Set\nresponse_format={\"type\": \"json_object\"}\nto enable JSON mode. For more complex schemas, you can define a JSON schema.\nLet\u2019s say you want to extract specific information from a user\u2019s query, like a name and an email address.\nPython\nJavaScript\ncURL\nCopy\nAsk AI\nclient\n=\nOpenAI(\nbase_url\n=\n\"https://inference.baseten.co/v1\"\n,\napi_key\n=\nos.environ.get(\n\"BASETEN_API_KEY\"\n)\n)\nresponse\n=\nclient.chat.completions.create(\nmodel\n=\n\"deepseek-ai/DeepSeek-V3-0324\"\n,\n# Or any other supported model\nmessages\n=\n[\n{\n\"role\"\n:\n\"system\"\n,\n\"content\"\n:\n\"You are an expert at extracting information.\"\n},\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"My name is Jane Doe and my email is jane.doe@example.com. I\n\\'\nd like to know more about your services.\"\n}\n],\nresponse_format\n=\n{\n\"type\"\n:\n\"json_object\"\n,\n\"json_schema\"\n: {\n\"name\"\n:\n\"user_details\"\n,\n\"description\"\n:\n\"User contact information\"\n,\n\"schema\"\n: {\n\"type\"\n:\n\"object\"\n,\n\"properties\"\n: {\n\"name\"\n: {\n\"type\"\n:\n\"string\"\n,\n\"description\"\n:\n\"The user\n\\'\ns full name\"\n},\n\"email\"\n: {\n\"type\"\n:\n\"string\"\n,\n\"description\"\n:\n\"The user\n\\'\ns email address\"\n}\n},\n\"required\"\n: [\n\"name\"\n,\n\"email\"\n]\n},\n\"strict\"\n:\nTrue\n# Enforce schema adherence\n}\n}\n)\noutput\n=\njson.loads(response.choices[\n0\n].message.content)\nprint\n(output)\n# Expected output:\n# {\n#   \"name\": \"Jane Doe\",\n#   \"email\": \"jane.doe@example.com\"\n# }\nCopy\nAsk AI\nclient\n=\nOpenAI(\nbase_url\n=\n\"https://inference.baseten.co/v1\"\n,\napi_key\n=\nos.environ.get(\n\"BASETEN_API_KEY\"\n)\n)\nresponse\n=\nclient.chat.completions.create(\nmodel\n=\n\"deepseek-ai/DeepSeek-V3-0324\"\n,\n# Or any other supported model\nmessages\n=\n[\n{\n\"role\"\n:\n\"system\"\n,\n\"content\"\n:\n\"You are an expert at extracting information.\"\n},\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"My name is Jane Doe and my email is jane.doe@example.com. I\n\\'\nd like to know more about your services.\"", "mimetype": "text/plain", "start_char_idx": 112688, "end_char_idx": 115793, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9ed8b590-d78d-4563-8eaf-c0b9943087a2": {"__data__": {"id_": "9ed8b590-d78d-4563-8eaf-c0b9943087a2", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d06066b2-9f34-4b6d-8b6a-bb5da37bdc09", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "5330769509ed708ac351c7801ac0fdce7943937b000478ae888d6dfd59d93650", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5eea0768-1899-498f-975b-3539cf6918b6", "node_type": "1", "metadata": {}, "hash": "7886d3be1b313ddf8e162f466d8af8db9d802d94586170e1de72339b548a8603", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "},\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"My name is Jane Doe and my email is jane.doe@example.com. I\n\\'\nd like to know more about your services.\"\n}\n],\nresponse_format\n=\n{\n\"type\"\n:\n\"json_object\"\n,\n\"json_schema\"\n: {\n\"name\"\n:\n\"user_details\"\n,\n\"description\"\n:\n\"User contact information\"\n,\n\"schema\"\n: {\n\"type\"\n:\n\"object\"\n,\n\"properties\"\n: {\n\"name\"\n: {\n\"type\"\n:\n\"string\"\n,\n\"description\"\n:\n\"The user\n\\'\ns full name\"\n},\n\"email\"\n: {\n\"type\"\n:\n\"string\"\n,\n\"description\"\n:\n\"The user\n\\'\ns email address\"\n}\n},\n\"required\"\n: [\n\"name\"\n,\n\"email\"\n]\n},\n\"strict\"\n:\nTrue\n# Enforce schema adherence\n}\n}\n)\noutput\n=\njson.loads(response.choices[\n0\n].message.content)\nprint\n(output)\n# Expected output:\n# {\n#   \"name\": \"Jane Doe\",\n#   \"email\": \"jane.doe@example.com\"\n# }\nCopy\nAsk AI\nconst\nclient\n=\nnew\nOpenAI\n({\nbaseURL:\n\"https://inference.baseten.co/v1\"\n,\napiKey:\nprocess\n.\nenv\n.\nBASETEN_API_KEY\n,\n});\n// Use the client for structured output\ntry\n{\nconst\nresponse\n=\nawait\nclient\n.\nchat\n.\ncompletions\n.\ncreate\n({\nmodel:\n\"deepseek-ai/DeepSeek-V3-0324\"\n,\nmessages:\n[\n{\nrole:\n\"system\"\n,\ncontent:\n\"You are an expert at extracting information.\"\n},\n{\nrole:\n\"user\"\n,\ncontent:\n\"My name is Jane Doe and my email is jane.doe@example.com. I'd like to know more about your services.\"\n},\n],\nresponse_format:\n{\ntype:\n\"json_object\"\n,\njson_schema:\n{\nname:\n\"user_details\"\n,\ndescription:\n\"User contact information\"\n,\nschema:\n{\ntype:\n\"object\"\n,\nproperties:\n{\nname:\n{\ntype:\n\"string\"\n,\ndescription:\n\"The user's full name\"\n},\nemail:\n{\ntype:\n\"string\"\n,\ndescription:\n\"The user's email address\"\n}\n},\nrequired:\n[\n\"name\"\n,\n\"email\"\n]\n},\nstrict:\ntrue\n}\n}\n})\nCopy\nAsk AI\ncurl\n-s\n\"https://inference.baseten.co/v1/chat/completions\"\n\\\n-H\n\"Content-Type: application/json\"\n\\\n-H\n\"Authorization: Api-Key\n$BASETEN_API_KEY\n\"\n\\\n-d\n@-\n<<\nEOF\n{\n\"model\": \"deepseek-ai/DeepSeek-V3-0324\",\n\"messages\": [\n{\"role\": \"system\", \"content\": \"You are an expert at extracting information.\"},\n{\"role\": \"user\", \"content\": \"My name is Jane Doe and my email is jane.doe@example.com. I'd like to know more about your services.\"}\n],\n\"response_format\": {\n\"type\": \"json_object\",\n\"json_schema\": {\n\"name\": \"user_details\",\n\"description\": \"User contact information\",\n\"schema\": {\n\"type\": \"object\",\n\"properties\": {\n\"name\": { \"type\": \"string\", \"description\": \"The user's full name\" },\n\"email\": { \"type\": \"string\", \"description\": \"The user's email address\" }\n},\n\"required\": [\"name\", \"email\"]\n},\n\"strict\": true\n}\n}\n}\nEOF\necho\n# Add a newline for cleaner output\nWhen\nstrict: true\nis specified within the\njson_schema\n, the model is constrained to produce output that strictly adheres to the provided schema. If the model cannot or will not produce output that matches the schema, it may return an error or a refusal.\n\u200b\nTool calling\nModel compatibility note:\nWe recommend using\nDeepseek V3\nfor tool calling functionality. We do not recommend using\nDeepseek R1\nfor tool calling as the model was not post-trained for tool calling.\nTool calling is fully supported. Simply define a list of tools and pass them via the\ntools\nparameter:\ntype\n: The type of tool to call. Currently, the only supported value is\nfunction\n.", "mimetype": "text/plain", "start_char_idx": 115653, "end_char_idx": 118755, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5eea0768-1899-498f-975b-3539cf6918b6": {"__data__": {"id_": "5eea0768-1899-498f-975b-3539cf6918b6", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9ed8b590-d78d-4563-8eaf-c0b9943087a2", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "af44eb30b0ee9f4fa8d5b029659aea248538f82598cce5d273ec72ea43b2fb49", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7addba30-957c-4b4c-b0dc-45b354c3e054", "node_type": "1", "metadata": {}, "hash": "e8b4d8b05a050253316a95878d07fc1e6512dca12e5f419457eeb6af3ca29341", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "If the model cannot or will not produce output that matches the schema, it may return an error or a refusal.\n\u200b\nTool calling\nModel compatibility note:\nWe recommend using\nDeepseek V3\nfor tool calling functionality. We do not recommend using\nDeepseek R1\nfor tool calling as the model was not post-trained for tool calling.\nTool calling is fully supported. Simply define a list of tools and pass them via the\ntools\nparameter:\ntype\n: The type of tool to call. Currently, the only supported value is\nfunction\n.\nfunction\n: A dictionary with the following keys:\nname\n: The name of the function to be called\ndescription\n: A description of what the function does\nparameters\n: A JSON Schema object describing the function parameters\nCopy\nAsk AI\n# Example list of tools\n{\n\"tools\"\n: [\n{\n\"type\"\n:\n\"function\"\n,\n\"function\"\n: {\n\"name\"\n:\n\"get_weather\"\n,\n\"description\"\n:\n\"Get the weather for a location\"\n,\n\"parameters\"\n: {\n\"type\"\n:\n\"object\"\n,\n\"properties\"\n: {\n\"location\"\n: {\n\"type\"\n:\n\"string\"\n,\n\"description\"\n:\n\"City and state\"\n}\n},\n\"required\"\n: [\n\"location\"\n]\n}\n}\n}\n]\n}\nHere\u2019s how you might implement tool calling:\nPython\nJavaScript\ncURL\nCopy\nAsk AI\nclient\n=\nOpenAI(\nbase_url\n=\n\"https://inference.baseten.co/v1\"\n,\napi_key\n=\nos.environ.get(\n\"BASETEN_API_KEY\"\n)\n)\n# Define the message and available tools\nmessages\n=\n[{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"What's the weather like in Boston?\"\n}]\ntools\n=\n[\n{\n\"type\"\n:\n\"function\"\n,\n\"function\"\n: {\n\"name\"\n:\n\"get_weather\"\n,\n\"description\"\n:\n\"Get the current weather\"\n,\n\"parameters\"\n: {\n\"type\"\n:\n\"object\"\n,\n\"properties\"\n: {\n\"location\"\n: {\n\"type\"\n:\n\"string\"\n,\n\"description\"\n:\n\"City name\"\n},\n\"unit\"\n: {\n\"type\"\n:\n\"string\"\n,\n\"enum\"\n: [\n\"celsius\"\n,\n\"fahrenheit\"\n]}\n},\n\"required\"\n: [\n\"location\"\n]\n}\n}\n}\n]\n# Make the initial request\nresponse\n=\nclient.chat.completions.create(\nmodel\n=\n\"deepseek-ai/DeepSeek-V3-0324\"\n,\nmessages\n=\nmessages,\ntools\n=\ntools,\ntool_choice\n=\n\"auto\"\n)\n# Process tool calls if any\nif\nresponse.choices[\n0\n].message.tool_calls:\n# Get the function call details\ntool_call\n=\nresponse.choices[\n0\n].message.tool_calls[\n0\n]\nfunction_args\n=\njson.loads(tool_call.function.arguments)\n# Call the function and get the result\nfunction_response\n=\nget_weather(\nlocation\n=\nfunction_args.get(\n\"location\"\n))\n# Add function response to conversation\nmessages.append(response.choices[\n0\n].message)\nmessages.append({\n\"tool_call_id\"\n: tool_call.id,\n\"role\"\n:\n\"tool\"\n,\n\"name\"\n: tool_call.function.name,\n\"content\"\n: function_response\n})\n# Get the final response with the function result\nfinal_response\n=\nclient.chat.completions.create(\nmodel\n=\n\"deepseek-ai/DeepSeek-V3-0324\"\n,\nmessages\n=\nmessages\n)\nprint\n(final_response.choices[\n0\n].message.content)\nCopy\nAsk AI\nclient\n=\nOpenAI(\nbase_url\n=\n\"https://inference.baseten.co/v1\"\n,\napi_key\n=\nos.environ.get(\n\"BASETEN_API_KEY\"\n)\n)\n# Define the message and available tools\nmessages\n=\n[{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"What's the weather like in Boston?\"", "mimetype": "text/plain", "start_char_idx": 118251, "end_char_idx": 121153, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7addba30-957c-4b4c-b0dc-45b354c3e054": {"__data__": {"id_": "7addba30-957c-4b4c-b0dc-45b354c3e054", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5eea0768-1899-498f-975b-3539cf6918b6", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "5a0499c33bbaf70a12dec7bcbc07b0242ff49ffac54d3998abc85e851ab392a6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "18d24414-c932-44fa-a638-367219367557", "node_type": "1", "metadata": {}, "hash": "aa3be809978b6448206ee675934137e3ec691ff6796904d4e8659468d3c22301", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "}]\ntools\n=\n[\n{\n\"type\"\n:\n\"function\"\n,\n\"function\"\n: {\n\"name\"\n:\n\"get_weather\"\n,\n\"description\"\n:\n\"Get the current weather\"\n,\n\"parameters\"\n: {\n\"type\"\n:\n\"object\"\n,\n\"properties\"\n: {\n\"location\"\n: {\n\"type\"\n:\n\"string\"\n,\n\"description\"\n:\n\"City name\"\n},\n\"unit\"\n: {\n\"type\"\n:\n\"string\"\n,\n\"enum\"\n: [\n\"celsius\"\n,\n\"fahrenheit\"\n]}\n},\n\"required\"\n: [\n\"location\"\n]\n}\n}\n}\n]\n# Make the initial request\nresponse\n=\nclient.chat.completions.create(\nmodel\n=\n\"deepseek-ai/DeepSeek-V3-0324\"\n,\nmessages\n=\nmessages,\ntools\n=\ntools,\ntool_choice\n=\n\"auto\"\n)\n# Process tool calls if any\nif\nresponse.choices[\n0\n].message.tool_calls:\n# Get the function call details\ntool_call\n=\nresponse.choices[\n0\n].message.tool_calls[\n0\n]\nfunction_args\n=\njson.loads(tool_call.function.arguments)\n# Call the function and get the result\nfunction_response\n=\nget_weather(\nlocation\n=\nfunction_args.get(\n\"location\"\n))\n# Add function response to conversation\nmessages.append(response.choices[\n0\n].message)\nmessages.append({\n\"tool_call_id\"\n: tool_call.id,\n\"role\"\n:\n\"tool\"\n,\n\"name\"\n: tool_call.function.name,\n\"content\"\n: function_response\n})\n# Get the final response with the function result\nfinal_response\n=\nclient.chat.completions.create(\nmodel\n=\n\"deepseek-ai/DeepSeek-V3-0324\"\n,\nmessages\n=\nmessages\n)\nprint\n(final_response.choices[\n0\n].message.content)\nCopy\nAsk AI\nconst\nclient\n=\nnew\nOpenAI\n({\nbaseURL:\n\"https://inference.baseten.co/v1\"\n,\napiKey:\nprocess\n.\nenv\n.\nBASETEN_API_KEY\n,\n});\n// Make initial request with tools\nconst\nresponse\n=\nawait\nclient\n.\nchat\n.\ncompletions\n.\ncreate\n({\nmodel:\n\"deepseek-ai/DeepSeek-V3-0324\"\n,\nmessages:\n[{\nrole:\n\"user\"\n,\ncontent:\n\"What's the weather like in Boston?\"\n}],\ntools:\n[{\ntype:\n\"function\"\n,\nfunction:\n{\nname:\n\"get_weather\"\n,\ndescription:\n\"Get the current weather\"\n,\nparameters:\n{\ntype:\n\"object\"\n,\nproperties:\n{\nlocation:\n{\ntype:\n\"string\"\n,\ndescription:\n\"City name\"\n}\n},\nrequired:\n[\n\"location\"\n]\n}\n}\n}]\n});\n// Process tool calls if any\nif\n(\nresponse\n.\nchoices\n[\n0\n].\nmessage\n.\ntool_calls\n) {\nconst\ntoolCall\n=\nresponse\n.\nchoices\n[\n0\n].\nmessage\n.\ntool_calls\n[\n0\n];\nconst\nargs\n=\nJSON\n.\nparse\n(\ntoolCall\n.\nfunction\n.\narguments\n);\n// Call function and get result\nconst\nfunctionResponse\n=\ngetWeather\n(\nargs\n.\nlocation\n);\n// Submit function result back to model\nconst\nmessages\n=\n[\n{\nrole:\n\"user\"\n,\ncontent:\n\"What's the weather like in Boston?\"\n},\nresponse\n.\nchoices\n[\n0\n].\nmessage\n,\n{\ntool_call_id:\ntoolCall\n.\nid\n,\nrole:\n\"tool\"\n,\nname:\n\"get_weather\"\n,\ncontent:\nfunctionResponse\n}\n];\nconst\nfinalResponse\n=\nawait\nclient\n.\nchat\n.\ncompletions\n.\ncreate\n({\nmodel:\n\"deepseek-ai/DeepSeek-V3-0324\"\n,\nmessages:\nmessages\n})\nCopy\nAsk AI\ncurl\n-s\n\"https://inference.baseten.co/v1/chat/completions\"\n\\\n-H\n\"Content-Type: application/json\"\n\\\n-H\n\"Authorization: Api-Key\n$BASETEN_API_KEY\n\"\n\\\n-d\n'{\n\"model\": \"deepseek-ai/DeepSeek-V3-0324\",\n\"messages\": [{\"role\": \"user\", \"content\": \"What'\n\\'\n's the weather like in Boston?", "mimetype": "text/plain", "start_char_idx": 121154, "end_char_idx": 124023, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "18d24414-c932-44fa-a638-367219367557": {"__data__": {"id_": "18d24414-c932-44fa-a638-367219367557", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7addba30-957c-4b4c-b0dc-45b354c3e054", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "23b8319e6c32c83a0881a01471e2c98528597965426bd8cff1ae24fc2c94e085", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "60a99625-ae64-4a40-993a-3ea42f7f301a", "node_type": "1", "metadata": {}, "hash": "41e67a09a81c000b433e3610c8b30f52ff1e2db13b199a0f7f43fe76a4a011b3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "},\nresponse\n.\nchoices\n[\n0\n].\nmessage\n,\n{\ntool_call_id:\ntoolCall\n.\nid\n,\nrole:\n\"tool\"\n,\nname:\n\"get_weather\"\n,\ncontent:\nfunctionResponse\n}\n];\nconst\nfinalResponse\n=\nawait\nclient\n.\nchat\n.\ncompletions\n.\ncreate\n({\nmodel:\n\"deepseek-ai/DeepSeek-V3-0324\"\n,\nmessages:\nmessages\n})\nCopy\nAsk AI\ncurl\n-s\n\"https://inference.baseten.co/v1/chat/completions\"\n\\\n-H\n\"Content-Type: application/json\"\n\\\n-H\n\"Authorization: Api-Key\n$BASETEN_API_KEY\n\"\n\\\n-d\n'{\n\"model\": \"deepseek-ai/DeepSeek-V3-0324\",\n\"messages\": [{\"role\": \"user\", \"content\": \"What'\n\\'\n's the weather like in Boston?\"}],\n\"tools\": [{\n\"type\": \"function\",\n\"function\": {\n\"name\": \"get_weather\",\n\"description\": \"Get the current weather\",\n\"parameters\": {\n\"type\": \"object\",\n\"properties\": {\n\"location\": {\"type\": \"string\", \"description\": \"City name\"}\n},\n\"required\": [\"location\"]\n}\n}\n}]\n}'\n)\n# Extract tool call details\nTOOL_CALL_ID\n=\n$(\n# If we have a tool call, prepare the weather data and send it back\nif\n[\n-n\n\"\n$TOOL_CALL_ID\n\"\n];\nthen\nWEATHER_DATA\n=\n'{\"location\":\"Boston\",\"temperature\":\"72\",\"forecast\":\"sunny\"}'\n# Send the function result back to the API\nFINAL_RESPONSE\n=\n$(\ncurl\n-s\n\"https://inference.baseten.co/v1/chat/completions\"\n\\\n-H\n\"Content-Type: application/json\"\n\\\n-H\n\"Authorization: Api-Key\n$BASETEN_API_KEY\n\"\n\\\n-d\n'{\n\"model\": \"deepseek-ai/DeepSeek-V3-0324\",\n\"messages\": [\n{\"role\": \"user\", \"content\": \"What'\n\\'\n's the weather like in Boston?\"},\n{\"role\": \"assistant\", \"content\": null, \"tool_calls\": [{\"id\": \"'\n$TOOL_CALL_ID\n'\", \"type\": \"function\", \"function\": {\"name\": \"get_weather\", \"arguments\": \"{\\\"location\\\":\\\"Boston\\\"}\"}}]},\n{\"role\": \"tool\", \"tool_call_id\": \"'\n$TOOL_CALL_ID\n'\", \"name\": \"get_weather\", \"content\": \"'\n$WEATHER_DATA\n'\"}\n]\n}'\n)\n# Print the final response\nfi\n\u200b\nError Handling\nThe API returns standard HTTP error codes:\n400\n: Bad request (malformed input)\n401\n: Unauthorized (invalid or missing API key)\n402\n: Payment required\n404\n: Model not found\n429\n: Rate limit exceeded\n500\n: Internal server error\nCheck the response body for specific error details and suggested resolutions.\n\u200b\nMigrating from OpenAI\nTo migrate from OpenAI to Baseten\u2019s OpenAI-compatible API, you need to make these changes to your existing code:\nReplace your OpenAI API key with your Baseten API key\nChange the base URL to\nhttps://inference.baseten.co/v1.\nUpdate model names to match Baseten-supported slugs.\nWas this page helpful?\nYes\nNo\nPrevious\nRate Limits & Budgets\nLearn about rate limits for Baseten's Model APIs and how to set usage budgets to control costs.\nNext\nOn this page\nPrerequisites\nSupported models\nMake your first API call\nRequest parameters\nStructured outputs\nTool calling\nError Handling\nMigrating from OpenAI\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 123467, "end_char_idx": 126194, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "60a99625-ae64-4a40-993a-3ea42f7f301a": {"__data__": {"id_": "60a99625-ae64-4a40-993a-3ea42f7f301a", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "18d24414-c932-44fa-a638-367219367557", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "0a88c86035c9d888c7f6ee2fd7a354b5eaa825b42f3d50a698809f228f78ef68", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3572b7c9-76ec-4f9c-b38c-7f273ffaa74d", "node_type": "1", "metadata": {}, "hash": "c3887a4d7b7287f010101f5bd9e96275f15c0c56e64bea4a6d7efced62e97874", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u200b\nMigrating from OpenAI\nTo migrate from OpenAI to Baseten\u2019s OpenAI-compatible API, you need to make these changes to your existing code:\nReplace your OpenAI API key with your Baseten API key\nChange the base URL to\nhttps://inference.baseten.co/v1.\nUpdate model names to match Baseten-supported slugs.\nWas this page helpful?\nYes\nNo\nPrevious\nRate Limits & Budgets\nLearn about rate limits for Baseten's Model APIs and how to set usage budgets to control costs.\nNext\nOn this page\nPrerequisites\nSupported models\nMake your first API call\nRequest parameters\nStructured outputs\nTool calling\nError Handling\nMigrating from OpenAI\nAssistant\nResponses are generated using AI and may contain mistakes.\n\n\nContent from https://docs.baseten.co/development/model-apis/rate-limits-and-budgets:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nOverview\nRate Limits & Budgets\nDeveloping a model\nDeveloping a Chain\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nExporting metrics\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nModel APIs\nRate Limits & Budgets\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\n\u200b\nRate Limits\nTo ensure fair use and system stability, Baseten enforces two rate limits:\nRequest rate limits\n\u2014 maximum number of API requests per minute\nToken rate limits\n\u2014 maximum number of tokens processed per minute (input + output combined)\nDefault limits vary based on your account status.\nAccount\nRPM\nTPM\nStarter\n(unverified)\n15\n100,000\nStarter\n(verified)\n120\n500,000\nBusiness\n120\n1,000,000\nEnterprise\nCustom\nCustom\nIf you exceed these limits, the API will return a\n429 Too Many Requests\nerror. Request a higher rate limit by\ncontacting us\n.\n\u200b\nRequesting higher limits\nIf you have high volume, are a verified customer, and need more headroom, you can\ncontact us\nto request a rate limit increase.\n\u200b\nSetting budgets\nSetting a budget allows you to control your Model API usage and avoid unexpected costs. Usage budgets apply only to Model APIs (not dedicated deployments). Your team will be notified by email at 75%, 90%, and 100% of budget.\n\u200b\nEnforcing budgets\nWhen setting a budget, you can choose to enforce it or not.\nIf you choose to enforce it\n, requests will be rejected once the budget is reached.\nIf you choose not to enforce it\n, you will be notified at 75%, 90%, and 100% of budget and you\u2019ll be responsible for any costs incurred over the budget.\nWas this page helpful?\nYes\nNo\nPrevious\nOverview\nThis page introduces the key concepts and workflow you'll use to package, configure, and iterate on models using Baseten\u2019s developer tooling.\nNext\nOn this page\nRate Limits\nRequesting higher limits\nSetting budgets\nEnforcing budgets\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 125507, "end_char_idx": 128681, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3572b7c9-76ec-4f9c-b38c-7f273ffaa74d": {"__data__": {"id_": "3572b7c9-76ec-4f9c-b38c-7f273ffaa74d", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "60a99625-ae64-4a40-993a-3ea42f7f301a", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "b39495cc23c7e4e7c5e18648097b50c3e12a58310a517abdc0bce86105a70e71", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "68b97399-8de0-49ed-b969-77332f492ad7", "node_type": "1", "metadata": {}, "hash": "fe6ee16b9653e6ee85853478e670bea5a558f2ed09364770756d12b2577dbfe6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Content from https://docs.baseten.co/development/model/base-images:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nOverview\nYour first model\nSetup and dependencies\nConfiguration\nCustom build commands\nBase Docker images\nPrivate Docker Registries\nImplementation\nImplementation (Advanced)\nDeploy and iterate\nPerformance optimization\nSecurity and secrets\nData and storage\nPython driven configuration for models \ud83c\udd95\nDeveloping a Chain\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nExporting metrics\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nSetup and dependencies\nBase Docker images\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nTruss uses containerized environments to ensure consistent model execution across deployments. While the default Truss image works for most cases, you may need a custom base image to meet specific package or system requirements.\n\u200b\nSetting a base image in\nconfig.yaml\nSpecify a custom base image in\nconfig.yaml\n:\nconfig.yaml\nCopy\nAsk AI\nbase_image\n:\nimage\n:\n<image_name:tag>\npython_executable_path\n:\n<path-to-python>\nimage\n: The Docker image to use.\npython_executable_path\n: The path to the Python binary inside the container.\n\u200b\nExample: NVIDIA NeMo Model\nUsing a custom image to deploy\nNVIDIA NeMo TitaNet\nmodel:\nconfig.yaml\nCopy\nAsk AI\nbase_image\n:\nimage\n:\nnvcr.io/nvidia/nemo:23.03\npython_executable_path\n:\n/usr/bin/python\napply_library_patches\n:\ntrue\nrequirements\n:\n-\nPySoundFile\nresources\n:\naccelerator\n:\nT4\ncpu\n:\n2500m\nmemory\n:\n4512Mi\nuse_gpu\n:\ntrue\nsecrets\n: {}\nsystem_packages\n:\n-\npython3.8-venv\n\u200b\nUsing Private Base Images\nIf your base image is private, ensure that you have configured your model to use a\nprivate registry\n\u200b\nCreating a custom base image\nYou can build a new base image using Truss\u2019s base images as a foundation. Available images are listed on\nDocker Hub\n.\n\u200b\nExample: Customizing a Truss Base Image\nDockerfile\nCopy\nAsk AI\nFROM\nbaseten/truss-server-base:3.11-gpu-v0.7.16\nRUN\npip uninstall cython -y\nRUN\npip install cython==0.29.30\n\u200b\nBuilding & Pushing Your Custom Image\nEnsure Docker is installed and running. Then, build, tag, and push your image:\nCopy\nAsk AI\ndocker\nbuild\n-t\nmy-custom-base-image:0.1\n.\ndocker\ntag\nmy-custom-base-image:0.1\nyour-docker-username/my-custom-base-image:0.1\ndocker\npush\nyour-docker-username/my-custom-base-image:0.1\nWas this page helpful?\nYes\nNo\nPrevious\nPrivate Docker Registries\nA guide to configuring a private container registry for your truss\nNext\nOn this page\nSetting a base image inconfig.yaml\nExample: NVIDIA NeMo Model\nUsing Private Base Images\nCreating a custom base image\nExample: Customizing a Truss Base Image\nBuilding & Pushing Your Custom Image\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 128684, "end_char_idx": 131892, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "68b97399-8de0-49ed-b969-77332f492ad7": {"__data__": {"id_": "68b97399-8de0-49ed-b969-77332f492ad7", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3572b7c9-76ec-4f9c-b38c-7f273ffaa74d", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "336b9c40a2039d3d96fe7e6e673fd50c01eaf9f4caf4c5cb95d97484cfadfc5a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5ed7643e-deec-4591-a6fe-87c4570b93b0", "node_type": "1", "metadata": {}, "hash": "84cc1a1d28090f333bc48da5daa1e90eba286b0cdd4b574ab3c52f2c32704122", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Content from https://docs.baseten.co/development/model/build-commands:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nOverview\nYour first model\nSetup and dependencies\nConfiguration\nCustom build commands\nBase Docker images\nPrivate Docker Registries\nImplementation\nImplementation (Advanced)\nDeploy and iterate\nPerformance optimization\nSecurity and secrets\nData and storage\nPython driven configuration for models \ud83c\udd95\nDeveloping a Chain\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nExporting metrics\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nSetup and dependencies\nCustom build commands\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nThe\nbuild_commands\nfeature allows you to\nrun custom Docker commands\nduring the\nbuild stage\n, enabling\nadvanced caching\n,\ndependency management\n,\nand environment setup\n.\nUse Cases:\nClone GitHub repositories\nInstall dependencies\nCreate directories\nPre-download model weights\n\u200b\n1. Using Build Commands in\nconfig.yaml\nAdd\nbuild_commands\nto your\nconfig.yaml\n:\nCopy\nAsk AI\nbuild_commands\n:\n-\ngit clone https://github.com/comfyanonymous/ComfyUI.git\n-\ncd ComfyUI && git checkout b1fd26fe9e55163f780bf9e5f56bf9bf5f035c93 && pip install -r requirements.txt\nmodel_name\n:\nBuild Commands Demo\npython_version\n:\npy310\nresources\n:\naccelerator\n:\nA100\nuse_gpu\n:\ntrue\nWhat happens?\nThe GitHub repository is cloned.\nThe specified commit is checked out.\nDependencies are installed.\nEverything is cached at build time\n, reducing deployment cold starts.\n\u200b\n2. Creating Directories in Your Truss\nUse\nbuild_commands\nto\ncreate directories\ndirectly in the container.\nCopy\nAsk AI\nbuild_commands\n:\n-\ngit clone https://github.com/comfyanonymous/ComfyUI.git\n-\ncd ComfyUI && mkdir ipadapter\n-\ncd ComfyUI && mkdir instantid\nUseful for\nlarge codebases\nrequiring additional structure.\n\u200b\n3. Caching Model Weights Efficiently\nFor large weights (10GB+), use\nmodel_cache\nor\nexternal_data\n.\nFor smaller weights,\nuse\nwget\nin\nbuild_commands\n:\nCopy\nAsk AI\nbuild_commands\n:\n-\ngit clone https://github.com/comfyanonymous/ComfyUI.git\n-\ncd ComfyUI && pip install -r requirements.txt\n-\ncd ComfyUI/models/controlnet && wget -O control-lora-canny-rank256.safetensors https://huggingface.co/stabilityai/control-lora/resolve/main/control-LoRAs-rank256/control-lora-canny-rank256.safetensors\n-\ncd ComfyUI/models/controlnet && wget -O control-lora-depth-rank256.safetensors https://huggingface.co/stabilityai/control-lora/resolve/main/control-LoRAs-rank256/control-lora-depth-rank256.safetensors\nmodel_name\n:\nBuild Commands Demo\npython_version\n:\npy310\nresources\n:\naccelerator\n:\nA100\nuse_gpu\n:\ntrue\nsystem_packages\n:\n-\nwget\nWhy use this?\nReduces startup time\nby\npreloading model weights\nduring the build stage.\nEnsures availability\nwithout runtime downloads.\n\u200b\n4. Running Any Shell Command\nThe\nbuild_commands\nfeature lets you execute\nany\nshell command as if running it locally, with the benefit of\ncaching the results\nat build time.\nKey Benefits:\nReduces cold starts\nby caching dependencies & data.\nEnsures reproducibility\nacross deployments.\nOptimizes environment setup\nfor fast execution.\nWas this page helpful?\nYes\nNo\nPrevious\nBase Docker images\nA guide to configuring a base image for your truss\nNext\nOn this page\n1. Using Build Commands in config.yaml\n2. Creating Directories in Your Truss\n3. Caching Model Weights Efficiently\n4. Running Any Shell Command\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 131895, "end_char_idx": 135804, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5ed7643e-deec-4591-a6fe-87c4570b93b0": {"__data__": {"id_": "5ed7643e-deec-4591-a6fe-87c4570b93b0", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "68b97399-8de0-49ed-b969-77332f492ad7", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "bf4c7ca2fb41d578e9575c68f91cc97717899e00117c983aac8974afa9477282", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "231c5d59-5343-4c3d-8f55-4ee499745ddc", "node_type": "1", "metadata": {}, "hash": "98919c72179cd23118fdb00c93fde44d240d762198d19f9ae63c21a80bb6a714", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Content from https://docs.baseten.co/development/model/build-your-first-model:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nOverview\nYour first model\nSetup and dependencies\nImplementation\nImplementation (Advanced)\nDeploy and iterate\nPerformance optimization\nSecurity and secrets\nData and storage\nPython driven configuration for models \ud83c\udd95\nDeveloping a Chain\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nExporting metrics\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nDeveloping a model\nYour first model\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nThis quickstart guide shows you how to build and deploy your first model,\nusing Baseten\u2019s Truss framework.\n\u200b\nPrerequisites\nTo use Truss, install a recent Truss version and ensure pydantic is v2:\nCopy\nAsk AI\npip\ninstall\n--upgrade\ntruss\n'pydantic>=2.0.0'\nHelp for setting up a clean development environment\nTruss requires python\n>=3.8,<3.13\n. To set up a fresh development environment,\nyou can use the following commands, creating a environment named\ntruss_env\nusing\npyenv\n:\nCopy\nAsk AI\ncurl\nhttps://pyenv.run\n|\nbash\necho\n'export PYENV_ROOT=\"$HOME/.pyenv\"'\n>>\n~/.bashrc\necho\n'[[ -d $PYENV_ROOT/bin ]] && export PATH=\"$PYENV_ROOT/bin:$PATH\"'\n>>\n~/.bashrc\necho\n'eval \"$(pyenv init -)\"'\n>>\n~/.bashrc\nsource\n~/.bashrc\npyenv\ninstall\n3.11.0\nENV_NAME\n=\n\"truss_env\"\npyenv\nvirtualenv\n3.11.0\n$ENV_NAME\npyenv\nactivate\n$ENV_NAME\npip\ninstall\n--upgrade\ntruss\n'pydantic>=2.0.0'\nTo deploy Truss remotely, you also need a\nBaseten account\n.\nIt is handy to export your API key to the current shell session or permanently in your\n.bashrc\n:\n~/.bashrc\nCopy\nAsk AI\nexport\nBASETEN_API_KEY\n=\n\"nPh8...\"\n\u200b\nInitialize your model\nTruss is a tool that helps you package your model code and configuration, and ship it to Baseten for deployment, testing, and scaling.\nTo create your first model, you can use the\ntruss init\ncommand.\nCopy\nAsk AI\n$\ntruss\ninit\nhello-world\n?\n\ud83d\udce6 Name this model: HelloWorld\nTruss\nHelloWorld\nwas\ncreated\nin\n~/hello-world\nThis will create a new directory called\nhello-world\nwith the following files:\nconfig.yaml\n- A configuration file for your model.\nmodel/model.py\n- A Python file that contains your model code\npackages/\n- A folder to hold any dependencies your model needs\ndata/\n- A folder to hold any data your model needs\nFor this example, we\u2019ll focus on the\nconfig.yaml\nfile and the\nmodel.py\nfile.\n\u200b\nconfig.yaml\nThe\nconfig.yaml\nfile is used to configure dependencies, resources, and\nother settings for your model.\nLet\u2019s take a look at the contents:\nconfig.yaml\nCopy\nAsk AI\nbuild_commands\n: []\nenvironment_variables\n: {}\nexternal_package_dirs\n: []\nmodel_metadata\n: {}\nmodel_name\n:\nHelloWorld\npython_version\n:\npy311\nrequirements\n: []\nresources\n:\naccelerator\n:\nnull\ncpu\n:\n'1'\nmemory\n:\n2Gi\nuse_gpu\n:\nfalse\nsecrets\n: {}\nsystem_packages\n: []\nSome key fields to note:\nrequirements\n: This is a list of\npip\npackages that will be installed when\nyour model is deployed.\nresources\n: This is where you can specify the resources your model will use.\nsecrets\n: This is where you can specify any secrets your model will need, such as\nHuggingFace API keys.\nSee the\nConfiguration\npage for more information on the\nconfig.yaml\nfile.", "mimetype": "text/plain", "start_char_idx": 135807, "end_char_idx": 139469, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "231c5d59-5343-4c3d-8f55-4ee499745ddc": {"__data__": {"id_": "231c5d59-5343-4c3d-8f55-4ee499745ddc", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5ed7643e-deec-4591-a6fe-87c4570b93b0", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "873e4d2d8db15bf854d38837d92ead5a69ea881486365d805ae69539b3f6b378", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ef26e5f2-a389-4894-a859-71c908852cef", "node_type": "1", "metadata": {}, "hash": "4159a005817547f37b5ce7525089892a3e8917013431e0b744511bf402503a55", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Let\u2019s take a look at the contents:\nconfig.yaml\nCopy\nAsk AI\nbuild_commands\n: []\nenvironment_variables\n: {}\nexternal_package_dirs\n: []\nmodel_metadata\n: {}\nmodel_name\n:\nHelloWorld\npython_version\n:\npy311\nrequirements\n: []\nresources\n:\naccelerator\n:\nnull\ncpu\n:\n'1'\nmemory\n:\n2Gi\nuse_gpu\n:\nfalse\nsecrets\n: {}\nsystem_packages\n: []\nSome key fields to note:\nrequirements\n: This is a list of\npip\npackages that will be installed when\nyour model is deployed.\nresources\n: This is where you can specify the resources your model will use.\nsecrets\n: This is where you can specify any secrets your model will need, such as\nHuggingFace API keys.\nSee the\nConfiguration\npage for more information on the\nconfig.yaml\nfile.\n\u200b\nmodel.py\nNext, let\u2019s take a look at the\nmodel.py\nfile.\nCopy\nAsk AI\nclass\nModel\n:\ndef\n__init__\n(\nself\n,\n**\nkwargs\n):\npass\ndef\nload\n(\nself\n):\npass\ndef\npredict\n(\nself\n,\nmodel_input\n):\nreturn\nmodel_input\nIn Truss models, we expect users to provide a Python class with the following methods:\n__init__\n: This is the constructor.\nload\n: This is called at model startup, and should include any setup logic, such as weight downloading or initialization\npredict\n: This is the method that is called during inference.\n\u200b\nDeploy your model\nTo deploy your model, you can use the\ntruss push\ncommand.\nCopy\nAsk AI\n$\ntruss\npush\nThis will deploy your model to Baseten.\n\u200b\nInvoke your model\nAfter deploying your model, you can invoke it with the invocation URL provided:\nCopy\nAsk AI\n$\ncurl\n-X\nPOST\nhttps://model-{model-id}.api.baseten.co/development/predict\n\\\n-H\n\"Authorization: Api-Key\n$BASETEN_API_KEY\n\"\n\\\n-d\n'\"some text\"'\n\"some text\"\n\u200b\nA Real Example\nTo show a slightly more complex example, let\u2019s deploy a text classification model\nfrom HuggingFace!\nIn this example, we\u2019ll use the\ntransformers\nlibrary to load a pre-trained model,\nfrom HuggingFace, and use it to classify the given text.\n\u200b\nconfig.yaml\nTo deploy this model, we need to add a few more dependencies to our\nconfig.yaml\nfile.\nconfig.yaml\nCopy\nAsk AI\nrequirements\n:\n-\ntransformers\n-\ntorch\n\u200b\nmodel.py\nNext, let\u2019s change our\nmodel.py\nfile to use the\ntransformers\nlibrary to load the model,\nand then use it to predict the sentiment of a given text.\nmodel.py\nCopy\nAsk AI\nfrom\ntransformers\nimport\npipeline\nclass\nModel\n:\ndef\n__init__\n(\nself\n,\n**\nkwargs\n):\npass\ndef\nload\n(\nself\n):\nself\n._model\n=\npipeline(\n\"text-classification\"\n)\ndef\npredict\n(\nself\n,\nmodel_input\n):\nreturn\nself\n._model(model_input)\n\u200b\nRunning inference\nSimilarly to our previous example, we can deploy this model using\ntruss push\nCopy\nAsk AI\n$\ntruss\npush\nAnd then invoke it using the invocation URL on Baseten.\nCopy\nAsk AI\n$\ncurl\n-X\nPOST\nhttps://model-{model-id}.api.baseten.co/development/predict\n\\\n-H\n\"Authorization: Api-Key\n$BASETEN_API_KEY\n\"\n\\\n-d\n'{\"text\": \"some text\"}'\n\u200b\nNext steps\nNow that you\u2019ve deployed your first model, you can learn more about more\noptions for\nconfiguring your model\n,\nand\nimplementing your model\n.\nWas this page helpful?\nYes\nNo\nPrevious\nConfiguration\nHow to configure your model.\nNext\nOn this page\nPrerequisites\nInitialize your model\nconfig.yaml\nmodel.py\nDeploy your model\nInvoke your model\nA Real Example\nconfig.yaml\nmodel.py\nRunning inference\nNext steps\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 138771, "end_char_idx": 142028, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ef26e5f2-a389-4894-a859-71c908852cef": {"__data__": {"id_": "ef26e5f2-a389-4894-a859-71c908852cef", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "231c5d59-5343-4c3d-8f55-4ee499745ddc", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "eb8c5c6b28138a57d6e7ddfcfb6914d3648c1bf77e943faa790069f654630d66", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "62acc989-c403-467c-965f-2b12e4713814", "node_type": "1", "metadata": {}, "hash": "531c4637fd9cc1b29f078751c2cca5337ea1202803eef36af05a87e465979c38", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Copy\nAsk AI\n$\ncurl\n-X\nPOST\nhttps://model-{model-id}.api.baseten.co/development/predict\n\\\n-H\n\"Authorization: Api-Key\n$BASETEN_API_KEY\n\"\n\\\n-d\n'{\"text\": \"some text\"}'\n\u200b\nNext steps\nNow that you\u2019ve deployed your first model, you can learn more about more\noptions for\nconfiguring your model\n,\nand\nimplementing your model\n.\nWas this page helpful?\nYes\nNo\nPrevious\nConfiguration\nHow to configure your model.\nNext\nOn this page\nPrerequisites\nInitialize your model\nconfig.yaml\nmodel.py\nDeploy your model\nInvoke your model\nA Real Example\nconfig.yaml\nmodel.py\nRunning inference\nNext steps\nAssistant\nResponses are generated using AI and may contain mistakes.\n\n\nContent from https://docs.baseten.co/development/model/code-first-development:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nOverview\nYour first model\nSetup and dependencies\nImplementation\nImplementation (Advanced)\nDeploy and iterate\nPerformance optimization\nSecurity and secrets\nData and storage\nPython driven configuration for models \ud83c\udd95\nDeveloping a Chain\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nExporting metrics\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nDeveloping a model\nPython driven configuration for models \ud83c\udd95\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nThis feature is still in beta.\nIn addition to our normal YAML configuration, we support configuring your model using pure Python. This offers the following benefits:\nTyped configuration via Python code\nwith IDE autocomplete, instead of a separate\nyaml\nconfiguration file\nSimpler directory structure\nthat IDEs support for module resolution\nIn this guide, we go through deploying a simple Model using this new framework.\n\u200b\nStep 1: Initializing your project\nWe leverage traditional\ntruss init\nfunctionality with a new flag to create the directory structure:\nCopy\nAsk AI\ntruss\ninit\nmy-new-model\n--python-config\n\u200b\nStep 2: Write your model\nTo build a model with this new framework, we require two things:\nA class that inherits from\nbaseten.ModelBase\n, which will serve as the entrypoint when invoking\n/predict\nA\npredict\nmethod with type hints\nThat\u2019s it! The following is a contrived example of a complete model that will keep a running total of user provided input:\nmy_model.py\nCopy\nAsk AI\nimport\ntruss_chains\nas\nbaseten\nclass\nRunningTotalCalculator\n(\nbaseten\n.\nModelBase\n):\ndef\n__init__\n(\nself\n):\nself\n._running_total\n=\n0\nasync\ndef\npredict\n(\nself\n,\nincrement\n:\nint\n) ->\nint\n:\nself\n._running_total\n+=\nincrement\nreturn\nself\n._running_total\n\u200b\nStep 3: Deploy, patch, and public your model\nIn order to deploy the first version of your new model, you can run:\nCopy\nAsk AI\ntruss\npush\nmy_model.py\nPlease note that\npush\n(as well as all other commands below) will require that you pass the path to the file containing the model as the final argument.\nThis new workflow also supports patching, so you can quickly iterate during development without building new images every time.\nCopy\nAsk AI\ntruss\nwatch\nmy_model.py\n\u200b\nModel Configuration\nModels can configure requirements for compute hardware (CPU count, GPU type and count, etc) and software dependencies (Python libraries or system packages) via the\nremote_config\nclass variable within the model:\nmy_model.py\nCopy\nAsk AI\nclass\nRunningTotalCalculator\n(\nbaseten\n.\nModelBase\n):\nremote_config: baseten.RemoteConfig\n=\nbaseten.RemoteConfig(\ncompute\n=\nbaseten.Compute(\ncpu_count\n=\n4\n,\nmemory\n=\n\"1Gi\"\n,\ngpu\n=\n\"T4\"\n,\ngpu_count\n=\n2\n)\n)\n...\nSee the\nremote configuration reference\nfor a complete list of options.", "mimetype": "text/plain", "start_char_idx": 141385, "end_char_idx": 145345, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "62acc989-c403-467c-965f-2b12e4713814": {"__data__": {"id_": "62acc989-c403-467c-965f-2b12e4713814", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ef26e5f2-a389-4894-a859-71c908852cef", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "b6c82ecc05e137415f1684c839e4264e399a1c57d6731d69c67cfbf8bf9ac087", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6a83038e-cbba-4fb3-b8b8-b6b689f6a3ed", "node_type": "1", "metadata": {}, "hash": "264f7ac4987b801892f1d769f29bea7d42e0a5d021eaf21ff48fb3dd51f1aa55", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This new workflow also supports patching, so you can quickly iterate during development without building new images every time.\nCopy\nAsk AI\ntruss\nwatch\nmy_model.py\n\u200b\nModel Configuration\nModels can configure requirements for compute hardware (CPU count, GPU type and count, etc) and software dependencies (Python libraries or system packages) via the\nremote_config\nclass variable within the model:\nmy_model.py\nCopy\nAsk AI\nclass\nRunningTotalCalculator\n(\nbaseten\n.\nModelBase\n):\nremote_config: baseten.RemoteConfig\n=\nbaseten.RemoteConfig(\ncompute\n=\nbaseten.Compute(\ncpu_count\n=\n4\n,\nmemory\n=\n\"1Gi\"\n,\ngpu\n=\n\"T4\"\n,\ngpu_count\n=\n2\n)\n)\n...\nSee the\nremote configuration reference\nfor a complete list of options.\n\u200b\nContext (access information)\nYou can add\nDeploymentContext\nobject as an optional final argument to the\n__init__\n-method of a Model. This allows you to use secrets within your Model, but note that they\u2019ll also need to be added to the\nassets\n.\nWe only expose secrets to the model that were explicitly requested in\nassets\nto comply with best security practices.\nmy_model.py\nCopy\nAsk AI\nclass\nRunningTotalCalculator\n(\nbaseten\n.\nModelBase\n):\nremote_config: baseten.RemoteConfig\n=\nbaseten.RemoteConfig(\n...\nassets\n=\nbaseten.Assets(\nsecret_keys\n=\n[\n\"token\"\n])\n)\ndef\n__init__\n(\nself\n,\ncontext\n: baseten.DeploymentContext\n=\nbaseten.depends_context()):\n...\nself\n._token\n=\ncontext.secrets[\n\"token\"\n]\n\u200b\nPackages\nIf you want to include modules in your model, you can easily create them from the root of the project:\nCopy\nAsk AI\nmy-new-model/\nmodule_1/\nsubmodule/\nscript.py\nmodule_2/\nanother_script.py\nmy_model.py\nWith this file structure, you would import in\nmy_model.py\nas follows:\nmy_model.py\nCopy\nAsk AI\nimport\ntruss_chains\nas\nbaseten\nfrom\nmodule_1.submodule\nimport\nscript\nfrom\nmodule_2\nimport\nanother_script\nclass\nRunningTotalCalculator\n(\nbaseten\n.\nModelBase\n):\n...\n.\n\u200b\nKnown Limitations\nRemoteConfig does\nnot\nsupport all the options exposed by the traditional\nconfig.yaml\n. If you\u2019re excited about this new development experience but need a specific feature ported over, please reach out to us!\nThis new framework does not support\npreprocess\nor\npostprocess\nhooks. We typically recommend inlining functionality from those functions if easy, or utilizing\nchains\nif the needs are more complex.\nWas this page helpful?\nYes\nNo\nPrevious\nOverview\nNext\nOn this page\nStep 1: Initializing your project\nStep 2: Write your model\nStep 3: Deploy, patch, and public your model\nModel Configuration\nContext (access information)\nPackages\nKnown Limitations\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 144645, "end_char_idx": 147245, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6a83038e-cbba-4fb3-b8b8-b6b689f6a3ed": {"__data__": {"id_": "6a83038e-cbba-4fb3-b8b8-b6b689f6a3ed", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "62acc989-c403-467c-965f-2b12e4713814", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "842d1f73c6a32fda76b8797777e570f732c1dbe3f574dcfd1f872316897d97bd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0439b1ad-5724-4381-afe3-964117079cdc", "node_type": "1", "metadata": {}, "hash": "2b8919a0d1cba1f5aecd2662f026ce197435ce94593ef20fba54aaa125a87bf2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "ModelBase\n):\n...\n.\n\u200b\nKnown Limitations\nRemoteConfig does\nnot\nsupport all the options exposed by the traditional\nconfig.yaml\n. If you\u2019re excited about this new development experience but need a specific feature ported over, please reach out to us!\nThis new framework does not support\npreprocess\nor\npostprocess\nhooks. We typically recommend inlining functionality from those functions if easy, or utilizing\nchains\nif the needs are more complex.\nWas this page helpful?\nYes\nNo\nPrevious\nOverview\nNext\nOn this page\nStep 1: Initializing your project\nStep 2: Write your model\nStep 3: Deploy, patch, and public your model\nModel Configuration\nContext (access information)\nPackages\nKnown Limitations\nAssistant\nResponses are generated using AI and may contain mistakes.\n\n\nContent from https://docs.baseten.co/development/model/configuration:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nOverview\nYour first model\nSetup and dependencies\nConfiguration\nCustom build commands\nBase Docker images\nPrivate Docker Registries\nImplementation\nImplementation (Advanced)\nDeploy and iterate\nPerformance optimization\nSecurity and secrets\nData and storage\nPython driven configuration for models \ud83c\udd95\nDeveloping a Chain\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nExporting metrics\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nSetup and dependencies\nConfiguration\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nML models often have dependencies on external libraries, data,\nand other resources. These models also typically have particular\nhardware configurations.\nIn this guide, we\u2019ll cover the basics of how to configure your model\nto specify this information.\nConfiguration for models is specified in the\nconfig.yaml\nfile. Here are some\nof the common configuration options:\n\u200b\nEnvironment variables\nYou can specify environment variables to be set in the model serving environment\nusing the\nenvironment_variables\nkey.\nconfig.yaml\nCopy\nAsk AI\nenvironment_variables\n:\nMY_ENV_VAR\n:\nmy_value\n\u200b\nPython Packages\nPython packages can be specified in two ways in the\nconfig.yaml\nfile:\nrequirements\n: A list of Python packages to install.\nrequirements_file\n: A requirements.txt file to install pip packages from.\nFor example, if you have a simple list of packages, you can specify them as follows:\nconfig.yaml\nCopy\nAsk AI\nrequirements\n:\n-\npackage_name\n-\npackage_name2\nNote that you can pin versions using the\n==\noperator.\nconfig.yaml\nCopy\nAsk AI\nrequirements\n:\n-\npackage_name==1.0.0\n-\npackage_name2==2.0.0\nIf you need more control over the installation process and want to use\ndifferent pip options or repositories, you can specify a\nrequirements_file\ninstead.\nconfig.yaml\nCopy\nAsk AI\nrequirements_file\n:\n./requirements.txt\n\u200b\nSystem Packages\nTruss also has support for installing apt-installable Debian packages. to\nadd system packages to your model serving environment, add the following to\nyour\nconfig.yaml\nfile:\nconfig.yaml\nCopy\nAsk AI\nsystem_packages\n:\n-\npackage_name\n-\npackage_name2\nFor a more concrete examples,\nconfig.yaml\nCopy\nAsk AI\nsystem_packages\n:\n-\ntesseract-ocr\n\u200b\nResources\nAnother key part of configuring your model is specifying hardware resources needed.\nYou can use the\nresources\nkey to specify these. For a CPU model, your resources\nconfiguration might look something like:\nconfig.yaml\nCopy\nAsk AI\nresources\n:\naccelerator\n:\nnull\ncpu\n:\n\"1\"\nmemory\n:\n2Gi\nuse_gpu\n:\nfalse\nFor a GPU model, your resources configuration might look like:\nconfig.yaml\nCopy\nAsk AI\nresources\n:\naccelerator\n:\n\"A10G\"\nWhen you push your model, it will be assigned an instance type matching the\nspecifications required.\nSee the\nResources\npage for more information on\noptions available.\n\u200b\nAdvanced configuration\nThere are numerous other options for configuring your model. See some\nof the other guides:\nSecrets\nData\nCustom Build Commands\nBase Docker Images\nCustom Servers\nCustom Health Checks\nWas this page helpful?\nYes\nNo\nPrevious\nCustom build commands\nHow to run your own docker commands during the build stage\nNext\nOn this page\nEnvironment variables\nPython Packages\nSystem Packages\nResources\nAdvanced configuration\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 146488, "end_char_idx": 151127, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0439b1ad-5724-4381-afe3-964117079cdc": {"__data__": {"id_": "0439b1ad-5724-4381-afe3-964117079cdc", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6a83038e-cbba-4fb3-b8b8-b6b689f6a3ed", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "928a25b93bcd8a23474837147406690fad6857f1a633364f9dba44917bb9970d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0807d0df-90d4-4507-9156-329dab5637ab", "node_type": "1", "metadata": {}, "hash": "d7296aee9d9c7d9f8bce7a67f52c483fbecfb3aa16c13cd7a51d0cc434619340", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Content from https://docs.baseten.co/development/model/custom-health-checks:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nOverview\nYour first model\nSetup and dependencies\nImplementation\nImplementation (Advanced)\nRequest handling\nCustom Responses\nCustom servers\nCustom health checks \ud83c\udd95\nCached weights \ud83c\udd95\nAccess model environments\nRequest concurrency\nStreaming output\nDeploy and iterate\nPerformance optimization\nSecurity and secrets\nData and storage\nPython driven configuration for models \ud83c\udd95\nDeveloping a Chain\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nExporting metrics\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nImplementation (Advanced)\nCustom health checks \ud83c\udd95\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nThis feature is still in beta mode.\nWhy use custom health checks?\nControl traffic and restarts\nby configuring failure thresholds to suit your needs.\nDefine replica health with custom logic\n(e.g. fail after a certain number of 500s or a specific CUDA error).\nBy default, health checks run every 10 seconds to verify that each replica of your deployment is running successfully and can receive requests. If a health check fails for an extended period, one or both of the following actions may occur:\nTraffic is immediately stopped from reaching the failing replica.\nThe failing replica is restarted.\nThe thresholds for each of these actions are configurable.\nCustom health checks can be implemented in two ways:\nConfiguring thresholds\nfor when health check failures should stop traffic to or restart a replica.\nWriting custom health check logic\nto define how replica health is determined.\n\u200b\nConfiguring health checks\n\u200b\nParameters\nYou can customize the behavior of health checks on your deployments by setting the following parameters:\n\u200b\nstop_traffic_threshold_seconds\ninteger\ndefault:\n1800\nThe duration that health checks must continuously fail before traffic to the failing replica is stopped.\nstop_traffic_threshold_seconds\nmust be between\n30\nand\n1800\nseconds, inclusive.\n\u200b\nrestart_check_delay_seconds\ninteger\ndefault:\n0\nHow long to wait before running health checks.\nrestart_check_delay_seconds\nmust be between\n0\nand\n1800\nseconds, inclusive.\n\u200b\nrestart_threshold_seconds\ninteger\ndefault:\n1800\nThe duration that health checks must continuously fail before triggering a restart of the failing replica.\nrestart_threshold_seconds\nmust be between\n30\nand\n1800\nseconds, inclusive.\nThe combined value of\nrestart_check_delay_seconds\nand\nrestart_threshold_seconds\nmust not exceed\n1800\nseconds.\n\u200b\nModel and custom server deployments\nConfigure health checks in your\nconfig.yaml\n.\nconfig.yaml\nCopy\nAsk AI\nruntime\n:\nhealth_checks\n:\nrestart_check_delay_seconds\n:\n60\n# Waits 60 seconds after deployment before starting health checks\nrestart_threshold_seconds\n:\n300\n# Triggers a restart if health checks fail for 5 minutes\nstop_traffic_threshold_seconds\n:\n600\n# Stops traffic if health checks fail for 10 minutes\nYou can also specify custom health check endpoints for custom servers.\nSee here\nfor more details.\n\u200b\nChains\nUse\nremote_config\nto configure health checks for your chainlet classes.\nchain.py\nCopy\nAsk AI\nclass\nCustomHealthChecks\n(\nchains\n.\nChainletBase\n):\nremote_config\n=\nchains.RemoteConfig(\noptions\n=\nchains.ChainletOptions(\nhealth_checks\n=\ntruss_config.HealthChecks(\nrestart_check_delay_seconds\n=\n30\n,\n# Waits 30 seconds before starting health checks\nrestart_threshold_seconds\n=\n120\n,\n# Restart replicas after 2 minutes of failure\nstop_traffic_threshold_seconds\n=\n300\n,\n# Stop traffic after 5 minutes of failure\n)\n)\n)\n\u200b\nWriting custom health checks\nYou can write custom health checks in both\nmodel deployments\nand\nchain deployments\n.\nCustom health checks are currently not supported in development deployments.", "mimetype": "text/plain", "start_char_idx": 151130, "end_char_idx": 155352, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0807d0df-90d4-4507-9156-329dab5637ab": {"__data__": {"id_": "0807d0df-90d4-4507-9156-329dab5637ab", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0439b1ad-5724-4381-afe3-964117079cdc", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "4d0aee714ebc289db3322b9ad00171f9b249dacb09baf09bdf4056af9cbe508c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ded70955-9f7c-49f5-acdd-49cfde908eba", "node_type": "1", "metadata": {}, "hash": "20886192b9ba6a78aa7724a9f059393d8d4251211b88ad03a1561acea246aeaf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "See here\nfor more details.\n\u200b\nChains\nUse\nremote_config\nto configure health checks for your chainlet classes.\nchain.py\nCopy\nAsk AI\nclass\nCustomHealthChecks\n(\nchains\n.\nChainletBase\n):\nremote_config\n=\nchains.RemoteConfig(\noptions\n=\nchains.ChainletOptions(\nhealth_checks\n=\ntruss_config.HealthChecks(\nrestart_check_delay_seconds\n=\n30\n,\n# Waits 30 seconds before starting health checks\nrestart_threshold_seconds\n=\n120\n,\n# Restart replicas after 2 minutes of failure\nstop_traffic_threshold_seconds\n=\n300\n,\n# Stop traffic after 5 minutes of failure\n)\n)\n)\n\u200b\nWriting custom health checks\nYou can write custom health checks in both\nmodel deployments\nand\nchain deployments\n.\nCustom health checks are currently not supported in development deployments.\n\u200b\nCustom health checks in models\nmodel.py\nCopy\nAsk AI\nclass\nModel\n:\ndef\nis_healthy\n(\nself\n) ->\nbool\n:\n# Add custom health check logic for your model here\npass\n\u200b\nCustom health checks in chains\nHealth checks can be customized for each chainlet in your chain.\nchain.py\nCopy\nAsk AI\n@chains.mark_entrypoint\nclass\nCustomHealthChecks\n(\nchains\n.\nChainletBase\n):\ndef\nis_healthy\n(\nself\n) ->\nbool\n:\n# Add custom health check logic for your chainlet here\npass\n\u200b\nHealth checks in action\n\u200b\nIdentifying 5xx errors\nYou might create a custom health check to identify 5xx errors like the following:\nmodel.py\nCopy\nAsk AI\nclass\nModel\n:\ndef\n__init__\n(\nself\n):\n...\nself\n._is_healthy\n=\nTrue\ndef\nload\n(\nself\n):\n...\n# Set the model to healthy once loading is complete\nself\n._is_healthy\n=\nTrue\ndef\nis_healthy\n(\nself\n):\nreturn\nself\n._is_healthy\ndef\npredict\n(\nself\n,\ninput\n):\ntry\n:\n# Perform inference\n...\nexcept\nSome5xxError:\nself\n._is_healthy\n=\nFalse\nraise\nCustom health check failures are indicated by the following log:\nExample health check failure log line\nCopy\nAsk AI\nJan 27 10:36:03pm md2pg Health check failed.\nDeployment restarts due to health check failures are indicated by the following log:\nExample restart log line\nCopy\nAsk AI\nJan 27 12:02:47pm zgbmb Model terminated unexpectedly. Exit code: 0, reason: Completed, restart count: 1\n\u200b\nFAQs\n\u200b\nIs there a rule of thumb for configuring thresholds for stopping traffic and restarting?\nIt depends on your health check implementation. If your health check relies on conditions that only change during inference (e.g.,\n_is_healthy\nis set in\npredict\n), restarting before stopping traffic is generally better, as it allows recovery without disrupting traffic.\nStopping traffic first may be preferable if a failing replica is actively degrading performance or causing inference errors, as it prevents the failing replica from affecting the overall deployment while allowing time for debugging or recovery.\n\u200b\nWhen should I configure\nrestart_check_delay_seconds\n?\nConfigure\nrestart_check_delay_seconds\nto allow replicas sufficient time to initialize after deployment or a restart. This delay helps reduce unnecessary restarts, particularly for services with longer startup times.\n\u200b\nWhy am I seeing two health check failure logs in my logs?\nThese refer to two separate health checks we run every 10 seconds:\nOne to determine when to stop traffic to a replica.\nThe other to determine when to restart a replica.\n\u200b\nDoes stopped traffic or replica restarts affect autoscaling?\nYes, both can impact autoscaling. If traffic stops or replicas restart, the remaining replicas handle more load. If the load exceeds the concurrency target during the autoscaling window, additional replicas are spun up. Similarly, when traffic stabilizes, excess replicas are scaled down after the scale down delay.\nSee here\nfor more details on autoscaling.\n\u200b\nHow does billing get affected?\nYou are billed for the uptime of your deployment. This includes the time a replica is running, even if it is failing health checks, until it scales down.\n\u200b\nWill failing health checks cause my deployment to stay up forever?\nNo. If your deployment is configured with a scale down delay and the minimum number of replicas is set to 0, the replicas will scale down once the model is no longer receiving traffic for the duration of the scale down delay. This applies even if the replicas are failing health checks.\nSee here\nfor more details on autoscaling.\nWas this page helpful?\nYes\nNo\nPrevious\nCached weights \ud83c\udd95\nAccelerate cold starts and availability by prefetching and caching your weights.", "mimetype": "text/plain", "start_char_idx": 154614, "end_char_idx": 158927, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ded70955-9f7c-49f5-acdd-49cfde908eba": {"__data__": {"id_": "ded70955-9f7c-49f5-acdd-49cfde908eba", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0807d0df-90d4-4507-9156-329dab5637ab", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "899c2e3b044632d61626c707aff4afe782c956570ce7999b6c184007aaab3f30", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ad2a9ea5-0f2f-441f-8592-1c9e0226767b", "node_type": "1", "metadata": {}, "hash": "4bac5c003f11fda8e8c2716d6eaad6daa3824a7885750c33d773b75e1012d742", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Similarly, when traffic stabilizes, excess replicas are scaled down after the scale down delay.\nSee here\nfor more details on autoscaling.\n\u200b\nHow does billing get affected?\nYou are billed for the uptime of your deployment. This includes the time a replica is running, even if it is failing health checks, until it scales down.\n\u200b\nWill failing health checks cause my deployment to stay up forever?\nNo. If your deployment is configured with a scale down delay and the minimum number of replicas is set to 0, the replicas will scale down once the model is no longer receiving traffic for the duration of the scale down delay. This applies even if the replicas are failing health checks.\nSee here\nfor more details on autoscaling.\nWas this page helpful?\nYes\nNo\nPrevious\nCached weights \ud83c\udd95\nAccelerate cold starts and availability by prefetching and caching your weights.\nNext\nOn this page\nConfiguring health checks\nParameters\nModel and custom server deployments\nChains\nWriting custom health checks\nCustom health checks in models\nCustom health checks in chains\nHealth checks in action\nIdentifying 5xx errors\nFAQs\nIs there a rule of thumb for configuring thresholds for stopping traffic and restarting?\nWhen should I configure restart_check_delay_seconds?\nWhy am I seeing two health check failure logs in my logs?\nDoes stopped traffic or replica restarts affect autoscaling?\nHow does billing get affected?\nWill failing health checks cause my deployment to stay up forever?\nAssistant\nResponses are generated using AI and may contain mistakes.\n\n\nContent from https://docs.baseten.co/development/model/custom-server:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nOverview\nYour first model\nSetup and dependencies\nImplementation\nImplementation (Advanced)\nRequest handling\nCustom Responses\nCustom servers\nCustom health checks \ud83c\udd95\nCached weights \ud83c\udd95\nAccess model environments\nRequest concurrency\nStreaming output\nDeploy and iterate\nPerformance optimization\nSecurity and secrets\nData and storage\nPython driven configuration for models \ud83c\udd95\nDeveloping a Chain\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nExporting metrics\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nImplementation (Advanced)\nDeploy Custom servers from Docker Images\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nIf you have an existing API server packaged in a\nDocker image\n\u2014whether an open-source server like\nvLLM\nor a custom-built image\u2014you can deploy it on Baseten\nwith just a\nconfig.yaml\nfile\n.\n\u200b\n1. Configuring a Custom Server in\nconfig.yaml\nDefine a\nDocker-based server\nby adding\ndocker_server\n:\nconfig.yaml\nCopy\nAsk AI\nbase_image\n:\nimage\n:\nvllm/vllm-openai:latest\ndocker_server\n:\nstart_command\n:\nvllm serve meta-llama/Meta-Llama-3.1-8B-Instruct --port 8000 --max-model-len 1024\nreadiness_endpoint\n:\n/health\nliveness_endpoint\n:\n/health\npredict_endpoint\n:\n/v1/chat/completions\nserver_port\n:\n8000\n\u200b\nKey Configurations\nstart_command\n(\nrequired\n) \u2013 Command to start the server.\npredict_endpoint\n(\nrequired\n) \u2013 Endpoint for serving requests (only one per model).\nserver_port\n(\nrequired\n) \u2013 Port where the server runs.\nreadiness_endpoint\n(\nrequired\n) \u2013 Used for\nKubernetes readiness probes\nto determine when the container is ready to accept traffic.\nliveness_endpoint\n(\nrequired\n) \u2013 Used for\nKubernetes liveness probes\nto determine if the container\nneeds to be restarted\n.\n\u200b\n2. Example: Running a vLLM Server\nThis example deploys\nMeta-Llama-3.1-8B-Instruct\nusing\nvLLM\non an\nA10G GPU\n, with\n/health\nas the readiness and liveness probe.", "mimetype": "text/plain", "start_char_idx": 158068, "end_char_idx": 162049, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ad2a9ea5-0f2f-441f-8592-1c9e0226767b": {"__data__": {"id_": "ad2a9ea5-0f2f-441f-8592-1c9e0226767b", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ded70955-9f7c-49f5-acdd-49cfde908eba", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "836f020c0d2e9f7b569ff7d6189f289770ef09f67b275f5ab3f3b3f2b029a2a8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e4185784-9e89-4d35-9edc-24c10fe9ca06", "node_type": "1", "metadata": {}, "hash": "39db35473ab20c7678e49f3a36835449fea4dd3541633bd31326f5ac71db00e5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "predict_endpoint\n(\nrequired\n) \u2013 Endpoint for serving requests (only one per model).\nserver_port\n(\nrequired\n) \u2013 Port where the server runs.\nreadiness_endpoint\n(\nrequired\n) \u2013 Used for\nKubernetes readiness probes\nto determine when the container is ready to accept traffic.\nliveness_endpoint\n(\nrequired\n) \u2013 Used for\nKubernetes liveness probes\nto determine if the container\nneeds to be restarted\n.\n\u200b\n2. Example: Running a vLLM Server\nThis example deploys\nMeta-Llama-3.1-8B-Instruct\nusing\nvLLM\non an\nA10G GPU\n, with\n/health\nas the readiness and liveness probe.\nconfig.yaml\nCopy\nAsk AI\nbase_image\n:\nimage\n:\nvllm/vllm-openai:latest\ndocker_server\n:\nstart_command\n:\nsh -c \"HF_TOKEN=$(cat /secrets/hf_access_token) vllm serve meta-llama/Meta-Llama-3.1-8B-Instruct --port 8000 --max-model-len 1024\"\nreadiness_endpoint\n:\n/health\nliveness_endpoint\n:\n/health\npredict_endpoint\n:\n/v1/chat/completions\nserver_port\n:\n8000\nresources\n:\naccelerator\n:\nA10G\nmodel_name\n:\nvllm-model-server\nsecrets\n:\nhf_access_token\n:\nnull\nruntime\n:\npredict_concurrency\n:\n128\nvLLM\u2019s /health endpoint is used to determine when the server is ready or needs\nrestarting.\nMore examples available in Truss examples repo.\n\u200b\n3. Installing custom Python packages\nTo install additional Python dependencies, add a\nrequirements.txt\nfile to your Truss.\n\u200b\nExample: Infinity embedding model server\nconfig.yaml\nCopy\nAsk AI\nbase_image\n:\nimage\n:\npython:3.11-slim\ndocker_server\n:\nstart_command\n:\nsh -c \"infinity_emb v2 --model-id BAAI/bge-small-en-v1.5\"\nreadiness_endpoint\n:\n/health\nliveness_endpoint\n:\n/health\npredict_endpoint\n:\n/embeddings\nserver_port\n:\n7997\nresources\n:\naccelerator\n:\nL4\nuse_gpu\n:\ntrue\nmodel_name\n:\ninfinity-embedding-server\nrequirements\n:\n-\ninfinity-emb[all]\nenvironment_variables\n:\nhf_access_token\n:\nnull\n\u200b\n4. Accessing secrets in custom servers\nTo use\nAPI keys or other secrets\n, store them in Baseten and\naccess them from\n/secrets\nin the container.\n\u200b\nExample: Accessing a Hugging Face token\nconfig.yaml\nCopy\nAsk AI\nsecrets\n:\nhf_access_token\n:\nnull\nInside your server, access it like this:\nCopy\nAsk AI\nHF_TOKEN\n=\n$(\ncat\n/secrets/hf_access_token\n)\nMore on secrets management\nhere\n.\nWas this page helpful?\nYes\nNo\nPrevious\nCustom health checks \ud83c\udd95\nCustomize the health of your deployments.\nNext\nOn this page\n1. Configuring a Custom Server in config.yaml\nKey Configurations\n2. Example: Running a vLLM Server\n3. Installing custom Python packages\nExample: Infinity embedding model server\n4. Accessing secrets in custom servers\nExample: Accessing a Hugging Face token\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 161495, "end_char_idx": 164083, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e4185784-9e89-4d35-9edc-24c10fe9ca06": {"__data__": {"id_": "e4185784-9e89-4d35-9edc-24c10fe9ca06", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ad2a9ea5-0f2f-441f-8592-1c9e0226767b", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "a03708e2e6e97a91aa26b970ce97069e6dd09e4c34776913aa7bc3c8179ffe38", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6f449a64-d70d-4317-b295-9717ad8b2642", "node_type": "1", "metadata": {}, "hash": "49761da4ed3a7374ee8f84e2e0e4fb8a65903afd1cf2d8f9e6a9735a713a468a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u200b\nExample: Accessing a Hugging Face token\nconfig.yaml\nCopy\nAsk AI\nsecrets\n:\nhf_access_token\n:\nnull\nInside your server, access it like this:\nCopy\nAsk AI\nHF_TOKEN\n=\n$(\ncat\n/secrets/hf_access_token\n)\nMore on secrets management\nhere\n.\nWas this page helpful?\nYes\nNo\nPrevious\nCustom health checks \ud83c\udd95\nCustomize the health of your deployments.\nNext\nOn this page\n1. Configuring a Custom Server in config.yaml\nKey Configurations\n2. Example: Running a vLLM Server\n3. Installing custom Python packages\nExample: Infinity embedding model server\n4. Accessing secrets in custom servers\nExample: Accessing a Hugging Face token\nAssistant\nResponses are generated using AI and may contain mistakes.\n\n\nContent from https://docs.baseten.co/development/model/data-directory:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nOverview\nYour first model\nSetup and dependencies\nImplementation\nImplementation (Advanced)\nDeploy and iterate\nPerformance optimization\nSecurity and secrets\nData and storage\nPython driven configuration for models \ud83c\udd95\nDeveloping a Chain\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nExporting metrics\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nDeveloping a model\nData and storage\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nModel files, such as weights, can be\nlarge\n(often\nmultiple GBs\n). Truss supports\nmultiple ways\nto load them efficiently:\nPublic Hugging Face models\n(default)\nBundled directly in Truss\n\u200b\n1. Bundling model weights in Truss\nStore model files\ninside Truss\nusing the\ndata/\ndirectory.\nExample: Stable Diffusion 2.1 Truss structure\nCopy\nAsk AI\ndata/\nscheduler/\nscheduler_config.json\ntext_encoder/\nconfig.json\ndiffusion_pytorch_model.bin\ntokenizer/\nmerges.txt\ntokenizer_config.json\nvocab.json\nunet/\nconfig.json\ndiffusion_pytorch_model.bin\nvae/\nconfig.json\ndiffusion_pytorch_model.bin\nmodel_index.json\nAccess bundled files in\nmodel.py\n:\nCopy\nAsk AI\nclass\nModel\n:\ndef\n__init__\n(\nself\n,\n**\nkwargs\n):\nself\n._data_dir\n=\nkwargs[\n\"data_dir\"\n]\ndef\nload\n(\nself\n):\nself\n.model\n=\nStableDiffusionPipeline.from_pretrained(\nstr\n(\nself\n._data_dir),\nrevision\n=\n\"fp16\"\n,\ntorch_dtype\n=\ntorch.float16,\n).to(\n\"cuda\"\n)\nLimitation: Large weights increase deployment size, making it slower. Consider\ncloud storage instead.\n\u200b\n2. Loading private model weights from S3\nIf using\nprivate S3 storage\n, first\nconfigure secure authentication\n.\n\u200b\nStep 1: Define AWS secrets in\nconfig.yaml\nCopy\nAsk AI\nsecrets\n:\naws_access_key_id\n:\nnull\naws_secret_access_key\n:\nnull\naws_region\n:\nnull\n# e.g., us-east-1\naws_bucket\n:\nnull\nDo not store actual credentials here. Add them securely to\nBaseten secrets\nmanager\n.\n\u200b\nStep 2: Authenticate with AWS in\nmodel.py\nCopy\nAsk AI\nimport\nboto3\ndef\n__init__\n(\nself\n,\n**\nkwargs\n):\nself\n._config\n=\nkwargs.get(\n\"config\"\n)\nsecrets\n=\nkwargs.get(\n\"secrets\"\n)\nself\n.s3_client\n=\nboto3.client(\n\"s3\"\n,\naws_access_key_id\n=\nsecrets[\n\"aws_access_key_id\"\n],\naws_secret_access_key\n=\nsecrets[\n\"aws_secret_access_key\"\n],\nregion_name\n=\nsecrets[\n\"aws_region\"\n],\n)\nself\n.s3_bucket\n=\nsecrets[\n\"aws_bucket\"\n]\n\u200b\nStep 3: Deploy\nCopy\nAsk AI\ntruss\npush\nWas this page helpful?\nYes\nNo\nPrevious\nPython driven configuration for models \ud83c\udd95\nUse code-first development tools to streamline model production.\nNext\nOn this page\n1. Bundling model weights in Truss\n2. Loading private model weights from S3\nStep 1: Define AWS secrets in config.yaml\nStep 2: Authenticate with AWS in model.py\nStep 3: Deploy\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 163406, "end_char_idx": 167353, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6f449a64-d70d-4317-b295-9717ad8b2642": {"__data__": {"id_": "6f449a64-d70d-4317-b295-9717ad8b2642", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e4185784-9e89-4d35-9edc-24c10fe9ca06", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "1218e0ea79f87701d3944b2345cba1e4a0a2627dcb6cb6f7e02fcdade58c9e3b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "69b0019f-26cf-4f6b-8aa2-222ce52928e5", "node_type": "1", "metadata": {}, "hash": "c035b82e02996ae7d7c71655965683405315621319de24d51514bd944a583bfa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Content from https://docs.baseten.co/development/model/deploy-and-iterate:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nOverview\nYour first model\nSetup and dependencies\nImplementation\nImplementation (Advanced)\nDeploy and iterate\nPerformance optimization\nSecurity and secrets\nData and storage\nPython driven configuration for models \ud83c\udd95\nDeveloping a Chain\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nExporting metrics\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nDeveloping a model\nDeploy and iterate\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nIn\nYour First Model\n, we walked through\nhow to deploy a basic model to Baseten. If you are trying to rapidly make changes\nand iterate on your model, you\u2019ll notice that there is quite a bit of time between\nrunning\ntruss push\nand when the changes are reflected on Baseten.\nAlso, a lot of models require special hardware that you may not immediately have\naccess to.\nTo solve this problem, we have a feature called\nTruss Watch\n, that allows you to\nlive reload your model as you work.\n\u200b\nTruss Watch\nTo make use of\ntruss watch\n, start by deploying your model:\nCopy\nAsk AI\n$\ntruss\npush\nBy default, this will deploy a \u201cdevelopment\u201d version of your model. This means that the model\nhas a live reload server attached to it and supports hot reloading. To get the hot reload\nloop working, simply run\ntruss watch\nafterwards:\nCopy\nAsk AI\n$\ntruss\nwatch\nNow, if you make changes to your model, you\u2019ll see them reflected in the model logs!\nYou can now happily iterate on your model without having to go through the entire\nbuild & deploy loop between each change.\n\u200b\nReady for Production?\nOnce you\u2019ve iterated on your model, and you\u2019re ready to deploy it to production,\nyou can use the\ntruss push --publish\ncommand. This will deploy a \u201cpublished\u201d\nversion of your model\nCopy\nAsk AI\ntruss\npush\n--publish\nNote that development models have slightly worse performance, and have more\nlimited scaling properites, so it\u2019s highly recommend to not use these for\nany production use-case.\nWas this page helpful?\nYes\nNo\nPrevious\nConcepts\nImprove your latency and throughput\nNext\nOn this page\nTruss Watch\nReady for Production?\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 167356, "end_char_idx": 170070, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "69b0019f-26cf-4f6b-8aa2-222ce52928e5": {"__data__": {"id_": "69b0019f-26cf-4f6b-8aa2-222ce52928e5", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6f449a64-d70d-4317-b295-9717ad8b2642", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "d9331df89f91831bdfdf00a1a3ef83025f2f5537afc186cdaee41a4f51b34cc2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b2d0893b-25ed-4ae7-933b-1971a1b409c6", "node_type": "1", "metadata": {}, "hash": "761428f9cab251530448b0893e34c8ecab3aee4c986af7c25501651fbeec8646", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Content from https://docs.baseten.co/development/model/environments:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nOverview\nYour first model\nSetup and dependencies\nImplementation\nImplementation (Advanced)\nRequest handling\nCustom Responses\nCustom servers\nCustom health checks \ud83c\udd95\nCached weights \ud83c\udd95\nAccess model environments\nRequest concurrency\nStreaming output\nDeploy and iterate\nPerformance optimization\nSecurity and secrets\nData and storage\nPython driven configuration for models \ud83c\udd95\nDeveloping a Chain\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nExporting metrics\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nImplementation (Advanced)\nAccess model environments\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nModel environments help configure behavior based on\ndeployment stage\n(e.g., production vs. staging). You can access the environment details via\nkwargs\nin the\nModel\nclass.\n\u200b\n1. Retrieve Environment Variables\nAccess the environment in\n__init__\n:\nCopy\nAsk AI\ndef\n__init__\n(\nself\n,\n**\nkwargs\n):\nself\n._environment\n=\nkwargs[\n\"environment\"\n]\n\u200b\n2. Configure Behavior Based on Environment\nUse environment variables in the\nload\nfunction:\nCopy\nAsk AI\ndef\nload\n(\nself\n):\nif\nself\n._environment.get(\n\"name\"\n)\n==\n\"production\"\n:\n# Production setup\nself\n.setup_sentry()\nself\n.setup_logging(\nlevel\n=\n\"INFO\"\n)\nself\n.load_production_weights()\nelse\n:\n# Default setup for staging or development deployments\nself\n.setup_logging(\nlevel\n=\n\"DEBUG\"\n)\nself\n.load_default_weights()\nWhy use this?\nCustomize logging levels\nLoad environment-specific model weights\nEnable monitoring tools (e.g., Sentry)\nWas this page helpful?\nYes\nNo\nPrevious\nRequest concurrency\nA guide to setting concurrency for your model\nNext\nOn this page\n1. Retrieve Environment Variables\n2. Configure Behavior Based on Environment\nAssistant\nResponses are generated using AI and may contain mistakes.\n\n\nContent from https://docs.baseten.co/development/model/implementation:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nOverview\nYour first model\nSetup and dependencies\nImplementation\nImplementation (Advanced)\nDeploy and iterate\nPerformance optimization\nSecurity and secrets\nData and storage\nPython driven configuration for models \ud83c\udd95\nDeveloping a Chain\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nExporting metrics\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nDeveloping a model\nImplementation\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nIn this section, we\u2019ll cover how to implement the actual logic for your model.\nAs was mentioned in\nYour First Model\n, the\nlogic for the model itself is specified in a\nmodel/model.py\nfile. To recap, the simplest\ndirectory structure for a model is:\nCopy\nAsk AI\nmodel/\nmodel.py\nconfig.yaml\nIt\u2019s expected that the\nmodel.py\nfile contains a class with particular methods:\nmodel.py\nCopy\nAsk AI\nclass\nModel\n:\ndef\n__init__\n(\nself\n):\npass\ndef\nload\n(\nself\n):\npass\ndef\npredict\n(\nself\n,\ninput_data\n):\npass\nThe\n__init__\nmethod is used to initialize the\nModel\nclass, and allows you to read\nin configuration parameters and other information.\nThe\nload\nmethod is where you define the logic for initializing the model.", "mimetype": "text/plain", "start_char_idx": 170073, "end_char_idx": 174171, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b2d0893b-25ed-4ae7-933b-1971a1b409c6": {"__data__": {"id_": "b2d0893b-25ed-4ae7-933b-1971a1b409c6", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "69b0019f-26cf-4f6b-8aa2-222ce52928e5", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "974ad73bfa566345ab0c470a1fb2a8e491dc38cd752e7bbd08f1f1200e75ac07", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b82c1f65-168c-4812-8c00-dbcd4a41f388", "node_type": "1", "metadata": {}, "hash": "aa9548a2a234226d0e61428591c2e60fd4a46737af42ffe882a48873f4061aba", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As was mentioned in\nYour First Model\n, the\nlogic for the model itself is specified in a\nmodel/model.py\nfile. To recap, the simplest\ndirectory structure for a model is:\nCopy\nAsk AI\nmodel/\nmodel.py\nconfig.yaml\nIt\u2019s expected that the\nmodel.py\nfile contains a class with particular methods:\nmodel.py\nCopy\nAsk AI\nclass\nModel\n:\ndef\n__init__\n(\nself\n):\npass\ndef\nload\n(\nself\n):\npass\ndef\npredict\n(\nself\n,\ninput_data\n):\npass\nThe\n__init__\nmethod is used to initialize the\nModel\nclass, and allows you to read\nin configuration parameters and other information.\nThe\nload\nmethod is where you define the logic for initializing the model. This might\ninclude downloading model weights, or loading them onto a GPU.\nThe\npredict\nmethod is where you define the logic for inference.\nIn the next sections, we\u2019ll cover each of these methods in more detail.\n\u200b\ninit\nAs mentioned above, the\n__init__\nmethod is used to initialize the\nModel\nclass, and allows you to\nread in configuration parameters and runtime information.\nThe simplest signature for\n__init__\nis:\nmodel.py\nCopy\nAsk AI\ndef\n__init__\n(\nself\n):\npass\nIf you need more information, however, you have the option to define your\ninit\nmethod\nsuch that it accepts the following parameters:\nmodel.py\nCopy\nAsk AI\ndef\n__init__\n(\nself\n,\nconfig\n:\ndict\n,\ndata_dir\n:\nstr\n,\nsecrets\n:\ndict\n,\nenvironment\n:\nstr\n):\npass\nconfig\n: A dictionary containing the config.yaml for the model.\ndata_dir\n: A string containing the path to the data directory for the model.\nsecrets\n: A dictionary containing the secrets for the model. Note that at runtime,\nthese will be populated with the actual values as stored on Baseten.\nenvironment\n: A string containing the environment for the model, if the model has been\ndeployed to an environment.\nYou can then make use of these parameters in the rest of your model but saving these as\nattributes:\nmodel.py\nCopy\nAsk AI\ndef\n__init__\n(\nself\n,\nconfig\n:\ndict\n,\ndata_dir\n:\nstr\n,\nsecrets\n:\ndict\n,\nenvironment\n:\nstr\n):\nself\n._config\n=\nconfig\nself\n._data_dir\n=\ndata_dir\nself\n._secrets\n=\nsecrets\nself\n._environment\n=\nenvironment\n\u200b\nload\nThe\nload\nmethod is where you define the logic for initializing the model. As\nmentioned before, this might include downloading model weights or loading them\nonto the GPU.\nload\n, unlike the other method mentioned, does not accept any parameters:\nmodel.py\nCopy\nAsk AI\ndef\nload\n(\nself\n):\npass\nAfter deploying your model, the deployment will not be considered \u201cReady\u201d until\nload\nhas\ncompleted successfully. Note that there is a\ntimeout of 30 minutes\nfor this, after which,\nif\nload\nhas not completed, the deployment will be marked as failed.\n\u200b\npredict\nThe\npredict\nmethod is where you define the logic for performing inference.\nThe simplest signature for\npredict\nis:\nmodel.py\nCopy\nAsk AI\ndef\npredict\n(\nself\n,\ninput_data\n) ->\nstr\n:\nreturn\n\"Hello\"\nThe return type of\npredict\nmust be JSON-serializable, so it can be:\ndict\nlist\nstr\nIf you would like to return a more strictly typed object, you can return a\nPydantic\nobject.\nmodel.py\nCopy\nAsk AI\nfrom\npydantic\nimport\nBaseModel\nclass\nResult\n(\nBaseModel\n):\nvalue:\nstr\nYou can then return an instance of this model from\npredict\n:\nmodel.py\nCopy\nAsk AI\ndef\npredict\n(\nself\n,\ninput_data\n) -> Prediction:\nreturn\nResult(\nvalue\n=\n\"Hello\"\n)\n\u200b\nStreaming\nIn addition to supporting a single request/response cycle, Truss also supports streaming.\nSee the\nStreaming\nguide for more information.\n\u200b\nAsync vs. Sync\nNote that the\npredict\nmethod is synchronous by default. However, if your model inference\ndepends on APIs require\nasyncio\n,\npredict\ncan also be written as a coroutine.", "mimetype": "text/plain", "start_char_idx": 173551, "end_char_idx": 177121, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b82c1f65-168c-4812-8c00-dbcd4a41f388": {"__data__": {"id_": "b82c1f65-168c-4812-8c00-dbcd4a41f388", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b2d0893b-25ed-4ae7-933b-1971a1b409c6", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "0ea70fc30c628e9884c58ca3ee15762c2cd1ff967ce713c6f7aa19eb9196745f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4c24cadc-7500-433d-ad59-c229e4bfda4c", "node_type": "1", "metadata": {}, "hash": "90035a99eb40c1875e42d3b34c6554d2ba3c8b1934bab98d17f1750b9c943d2b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "model.py\nCopy\nAsk AI\nfrom\npydantic\nimport\nBaseModel\nclass\nResult\n(\nBaseModel\n):\nvalue:\nstr\nYou can then return an instance of this model from\npredict\n:\nmodel.py\nCopy\nAsk AI\ndef\npredict\n(\nself\n,\ninput_data\n) -> Prediction:\nreturn\nResult(\nvalue\n=\n\"Hello\"\n)\n\u200b\nStreaming\nIn addition to supporting a single request/response cycle, Truss also supports streaming.\nSee the\nStreaming\nguide for more information.\n\u200b\nAsync vs. Sync\nNote that the\npredict\nmethod is synchronous by default. However, if your model inference\ndepends on APIs require\nasyncio\n,\npredict\ncan also be written as a coroutine.\nmodel.py\nCopy\nAsk AI\nimport\nasyncio\nasync\ndef\npredict\n(\nself\n,\ninput_data\n) ->\ndict\n:\n# Async logic here\nawait\nasyncio.sleep(\n1\n)\nreturn\n{\n\"value\"\n:\n\"Hello\"\n}\nIf you are using\nasyncio\nin your\npredict\nmethod, be sure not to perform any blocking\noperations, such as a synchronous file download. This can result in degraded performance.\nWas this page helpful?\nYes\nNo\nPrevious\nRequest handling\nGet more control by directly using the request object.\nNext\nOn this page\ninit\nload\npredict\nStreaming\nAsync vs. Sync\nAssistant\nResponses are generated using AI and may contain mistakes.\n\n\nContent from https://docs.baseten.co/development/model/model-cache:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nOverview\nYour first model\nSetup and dependencies\nImplementation\nImplementation (Advanced)\nRequest handling\nCustom Responses\nCustom servers\nCustom health checks \ud83c\udd95\nCached weights \ud83c\udd95\nAccess model environments\nRequest concurrency\nStreaming output\nDeploy and iterate\nPerformance optimization\nSecurity and secrets\nData and storage\nPython driven configuration for models \ud83c\udd95\nDeveloping a Chain\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nExporting metrics\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nImplementation (Advanced)\nCached weights \ud83c\udd95\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\n\u200b\nWhat is a \u201ccold start\u201d?\nCold start\u201d is a term used to describe the time taken when a request is received when the model is scaled to 0 until it is ready to handle the first request. This process is a critical factor in allowing your deployments to be responsive to traffic while maintaining your SLAs and lowering your costs.\nTo optimize cold starts, we will go over the following stategies: Downloading them in a background thread in Rust that runs during the module import, caching weights in a distributed filesystem, and moving weights into the docker image.\nIn practice, this reduces the cold start for large models to just a few seconds. For example, Stable Diffusion XL can take a few minutes to boot up without caching. With caching, it takes just under 10 seconds.\n\u200b\nEnabling Caching + Prefetching for a Model\nTo enable caching, simply add\nmodel_cache\nto your\nconfig.yaml\nwith a valid\nrepo_id\n. The\nmodel_cache\nhas a few key configurations:\nrepo_id\n(required): The repo name from Hugging Face.\nrevision\n(required): The revision of the huggingface repo, such as the sha or branch name such as\nrefs/pr/1\nor\nmain\n.\nuse_volume\n: Boolean flag to determine if the weights are downloaded to the Baseten Filesystem at runtime (recommended) or bundled into the container image (not recommended).\nvolume_folder\n: string, folder name under which the model weights appear. Setting it to\nmy-llama-model\nwill mount the repo to\n/app/model_cache/my-llama-model\nat runtime.\nallow_patterns\n: Only cache files that match specified patterns. Utilize Unix shell-style wildcards to denote these patterns.\nignore_patterns\n: Conversely, you can also denote file patterns to ignore, hence streamlining the caching process.", "mimetype": "text/plain", "start_char_idx": 176535, "end_char_idx": 180609, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4c24cadc-7500-433d-ad59-c229e4bfda4c": {"__data__": {"id_": "4c24cadc-7500-433d-ad59-c229e4bfda4c", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b82c1f65-168c-4812-8c00-dbcd4a41f388", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "7acfc8e66dd87e8a3b1c91877d11d60036b0182161fb390634d4b4e08d007873", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ec065687-46bd-4cb7-94e5-8f5f89fcdce6", "node_type": "1", "metadata": {}, "hash": "f0deed4aaccc58d272d191f14c06a850aa3dacc6833a234ef8cb797b2c3f5d46", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The\nmodel_cache\nhas a few key configurations:\nrepo_id\n(required): The repo name from Hugging Face.\nrevision\n(required): The revision of the huggingface repo, such as the sha or branch name such as\nrefs/pr/1\nor\nmain\n.\nuse_volume\n: Boolean flag to determine if the weights are downloaded to the Baseten Filesystem at runtime (recommended) or bundled into the container image (not recommended).\nvolume_folder\n: string, folder name under which the model weights appear. Setting it to\nmy-llama-model\nwill mount the repo to\n/app/model_cache/my-llama-model\nat runtime.\nallow_patterns\n: Only cache files that match specified patterns. Utilize Unix shell-style wildcards to denote these patterns.\nignore_patterns\n: Conversely, you can also denote file patterns to ignore, hence streamlining the caching process.\nHere is an example of a well written\nmodel_cache\nfor Stable Diffusion XL. Note how it only pulls the model weights that it needs using\nallow_patterns\n.\nconfig.yaml\nCopy\nAsk AI\nmodel_cache\n:\n-\nrepo_id\n:\nmadebyollin/sdxl-vae-fp16-fix\nrevision\n:\n207b116dae70ace3637169f1ddd2434b91b3a8cd\nuse_volume\n:\ntrue\nvolume_folder\n:\nsdxl-vae-fp16\nallow_patterns\n:\n-\nconfig.json\n-\ndiffusion_pytorch_model.safetensors\n-\nrepo_id\n:\nstabilityai/stable-diffusion-xl-base-1.0\nrevision\n:\n462165984030d82259a11f4367a4eed129e94a7b\nuse_volume\n:\ntrue\nvolume_folder\n:\nstable-diffusion-xl-base\nallow_patterns\n:\n-\n\"*.json\"\n-\n\"*.fp16.safetensors\"\n-\nsd_xl_base_1.0.safetensors\n-\nrepo_id\n:\nstabilityai/stable-diffusion-xl-refiner-1.0\nrevision\n:\n5d4cfe854c9a9a87939ff3653551c2b3c99a4356\nuse_volume\n:\ntrue\nvolume_folder\n:\nstable-diffusion-xl-refiner\nallow_patterns\n:\n-\n\"*.json\"\n-\n\"*.fp16.safetensors\"\n-\nsd_xl_refiner_1.0.safetensors\nMany Hugging Face repos have model weights in different formats (\n.bin\n,\n.safetensors\n,\n.h5\n,\n.msgpack\n, etc.). You only need one of these most of the time. To minimize cold starts, ensure that you only cache the weights you need.\n\u200b\nWhat is weight \u201cpre-fetching\u201d?\nWith\nmodel_cache\n, weights are pre-fetched by downloading your weights ahead of time in a dedicated Rust thread.\nThis means, you can perform all kinds of preparation work (importing libraries, jit compilation of torch/triton modules), until you need access to the files.\nIn practice, executing statements like\nimport tensorrt_llm\ntypically take 10\u201315 seconds. By that point, the first 5\u201310GB of the weights will have already been downloaded.\nTo use the\nmodel_cache\nconfig with truss,  we require you to actively interact with the\nlazy_data_resolver\n.\nBefore using any of the downloaded files, you must call the\nlazy_data_resolver.block_until_download_complete()\n. This will block until all files in the\n/app/model_cache\ndirectory are downloaded & ready to use.\nThis call must be either part of your\n__init__\nor\nload\nimplementation.\nmodel.py\nCopy\nAsk AI\n# <- download is invoked before here.\nimport\ntorch\n# this line usually takes 2-5 seconds.", "mimetype": "text/plain", "start_char_idx": 179807, "end_char_idx": 182714, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ec065687-46bd-4cb7-94e5-8f5f89fcdce6": {"__data__": {"id_": "ec065687-46bd-4cb7-94e5-8f5f89fcdce6", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4c24cadc-7500-433d-ad59-c229e4bfda4c", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "0f73f4d67cf582e9aa66b14b3f616cfe129c994c5f8af651cc43ff78f85b268e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3d141dfb-9965-464a-b56b-801c6659061d", "node_type": "1", "metadata": {}, "hash": "be34f6e175824c1979360055b24aecfb5bb0968f6dee24085c063e3f1f6de4bc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This means, you can perform all kinds of preparation work (importing libraries, jit compilation of torch/triton modules), until you need access to the files.\nIn practice, executing statements like\nimport tensorrt_llm\ntypically take 10\u201315 seconds. By that point, the first 5\u201310GB of the weights will have already been downloaded.\nTo use the\nmodel_cache\nconfig with truss,  we require you to actively interact with the\nlazy_data_resolver\n.\nBefore using any of the downloaded files, you must call the\nlazy_data_resolver.block_until_download_complete()\n. This will block until all files in the\n/app/model_cache\ndirectory are downloaded & ready to use.\nThis call must be either part of your\n__init__\nor\nload\nimplementation.\nmodel.py\nCopy\nAsk AI\n# <- download is invoked before here.\nimport\ntorch\n# this line usually takes 2-5 seconds.\nimport\ntensorrt_llm\n# this line usually takes 10-15 seconds\nimport\nonnxruntime\n# this line usually takes 5-10 seconds\nclass\nModel\n:\n\"\"\"example usage of `model_cache` in truss\"\"\"\ndef\n__init__\n(\nself\n,\n*\nargs\n,\n**\nkwargs\n):\n# `lazy_data_resolver` is passed as keyword-argument in init\nself\n._lazy_data_resolver\n=\nkwargs[\n\"lazy_data_resolver\"\n]\ndef\nload\n():\n# work that does not require the download may be done beforehand\nrandom_vector\n=\ntorch.randn(\n1000\n)\n# important to collect the download before using any incomplete data\nself\n._lazy_data_resolver.block_until_download_complete()\n# after the call, you may use the /app/model_cache directory\ntorch.load(\n\"/app/model_cache/your_model.pt\"\n)\n*\nrandom_vector\n\u200b\nPrivate Hugging Face repositories \ud83e\udd17\nFor any public Hugging Face repo, you don\u2019t need to do anything else. Adding the\nmodel_cache\nkey with an appropriate\nrepo_id\nshould be enough.\nHowever, if you want to deploy a model from a gated repo like\nLlama 2\nto Baseten, there are a few steps you need to take:\n1\nGet Hugging Face API Key\nGrab an API key\nfrom Hugging Face with\nread\naccess. Make sure you have access to the model you want to serve.\n2\nAdd it to Baseten Secrets Manager\nPaste your API key in your\nsecrets manager in Baseten\nunder the key\nhf_access_token\n. You can read more about secrets\nhere\n.\n3\nUpdate Config\nIn your Truss\u2019s\nconfig.yaml\n, add the following code:\nconfig.yaml\nCopy\nAsk AI\nsecrets\n:\nhf_access_token\n:\nnull\nMake sure that the key\nsecrets\nonly shows up once in your\nconfig.yaml\n.\nIf you run into any issues, run through all the steps above again and make sure you did not misspell the name of the repo or paste an incorrect API key.\n\u200b\nmodel_cache\nwithin Chains\nTo use\nmodel_cache\nfor\nchains\n- use the\nAssets\nspecifier. In the example below, we will download\nllama-3.2-1B\n.\nAs this model is a gated huggingface model, we are setting the mounting token as part of the assets\nchains.Assets(..., secret_keys=[\"hf_access_token\"])\n.\nThe model is quite small - in many cases, we will be able to download the model while\nfrom transformers import pipeline\nand\nimport torch\nare running.\nchain_cache.py\nCopy\nAsk AI\nimport\nrandom\nimport\ntruss_chains\nas\nchains\ntry\n:\n# imports on global level for PoemGeneratorLM, to save time during the download.\nfrom\ntransformers\nimport\npipeline\nimport\ntorch\nexcept\nImportError\n:\n# RandInt does not have these dependencies.\npass\nclass\nRandInt\n(\nchains\n.\nChainletBase\n):\nasync\ndef\nrun_remote\n(\nself\n,\nmax_value\n:\nint\n) ->\nint\n:\nreturn\nrandom.randint(\n1\n, max_value)\n@chains.mark_entrypoint\nclass\nPoemGeneratorLM\n(\nchains\n.", "mimetype": "text/plain", "start_char_idx": 181885, "end_char_idx": 185286, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3d141dfb-9965-464a-b56b-801c6659061d": {"__data__": {"id_": "3d141dfb-9965-464a-b56b-801c6659061d", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ec065687-46bd-4cb7-94e5-8f5f89fcdce6", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "51bcf1bdf32eef68324a2faf6567ae157e37a61dfabd6a080c2e3e70f3503423", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aa3a684f-0ec7-4281-9a3b-534b9cdecf75", "node_type": "1", "metadata": {}, "hash": "07b51ff108b9425b61d2d5ccd08b84706c374823564e1c70d661c9a55441c567", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The model is quite small - in many cases, we will be able to download the model while\nfrom transformers import pipeline\nand\nimport torch\nare running.\nchain_cache.py\nCopy\nAsk AI\nimport\nrandom\nimport\ntruss_chains\nas\nchains\ntry\n:\n# imports on global level for PoemGeneratorLM, to save time during the download.\nfrom\ntransformers\nimport\npipeline\nimport\ntorch\nexcept\nImportError\n:\n# RandInt does not have these dependencies.\npass\nclass\nRandInt\n(\nchains\n.\nChainletBase\n):\nasync\ndef\nrun_remote\n(\nself\n,\nmax_value\n:\nint\n) ->\nint\n:\nreturn\nrandom.randint(\n1\n, max_value)\n@chains.mark_entrypoint\nclass\nPoemGeneratorLM\n(\nchains\n.\nChainletBase\n):\nfrom\ntruss\nimport\ntruss_config\nLLAMA_CACHE\n=\ntruss_config.ModelRepo(\nrepo_id\n=\n\"meta-llama/Llama-3.2-1B-Instruct\"\n,\nrevision\n=\n\"c4219cc9e642e492fd0219283fa3c674804bb8ed\"\n,\nuse_volume\n=\nTrue\n,\nvolume_folder\n=\n\"llama_mini\"\n,\nignore_patterns\n=\n[\n\"*.pth\"\n,\n\"*.onnx\"\n]\n)\nremote_config\n=\nchains.RemoteConfig(\ndocker_image\n=\nchains.DockerImage(\n# The phi model needs some extra python packages.\npip_requirements\n=\n[\n\"transformers==4.48.0\"\n,\n\"torch==2.6.0\"\n,\n]\n),\ncompute\n=\nchains.Compute(\ngpu\n=\n\"L4\"\n),\n# The phi model needs a GPU and more CPUs.\n# compute=chains.Compute(cpu_count=2, gpu=\"T4\"),\n# Cache the model weights in the image\nassets\n=\nchains.Assets(\ncached\n=\n[\nLLAMA_CACHE\n],\nsecret_keys\n=\n[\n\"hf_access_token\"\n]),\n)\n# <- Download happens before __init__ is called.\ndef\n__init__\n(\nself\n,\nrand_int\n=\nchains.depends(RandInt,\nretries\n=\n3\n)) ->\nNone\n:\nself\n._rand_int\n=\nrand_int\nprint\n(\n\"loading cached llama_mini model\"\n)\nself\n.pipeline\n=\npipeline(\n\"text-generation\"\n,\nmodel\n=\nf\n\"/app/model_cache/llama_mini\"\n,\n)\nasync\ndef\nrun_remote\n(\nself\n,\nmax_value\n:\nint\n=\n3\n) ->\nstr\n:\nnum_repetitions\n=\nawait\nself\n._rand_int.run_remote(max_value)\nprint\n(\n\"writing poem with num_repetitions\"\n, num_repetitions)\npoem\n=\nstr\n(\nself\n.pipeline(\ntext_inputs\n=\n\"Write a beautiful and descriptive poem about the ocean. Focus on its vastness, movement, and colors.\"\n,\nmax_new_tokens\n=\n150\n,\ndo_sample\n=\nTrue\n,\nreturn_full_text\n=\nFalse\n,\ntemperature\n=\n0.7\n,\ntop_p\n=\n0.9\n,\n)[\n0\n][\n'generated_text'\n])\nreturn\npoem\n*\nnum_repetitions\n\u200b\nmodel_cache\nfor custom servers\nIf you are not using Python\u2019s\nmodel.py\nand\ncustom servers\nsuch as\nvllm\n, TEI or\nsglang\n,\nyou are required to use the\ntruss-transfer-cli\ncommand, to force population of the\n/app/model_cache\nlocation. The command will block until the weights are downloaded.\nHere is an example for how to use text-embeddings-inference on a L4 to populate a jina embeddings model from huggingface into the model_cache.", "mimetype": "text/plain", "start_char_idx": 184669, "end_char_idx": 187239, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "aa3a684f-0ec7-4281-9a3b-534b9cdecf75": {"__data__": {"id_": "aa3a684f-0ec7-4281-9a3b-534b9cdecf75", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3d141dfb-9965-464a-b56b-801c6659061d", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "e1388872fe6ed2e3ae8fe381496895514b410cf3b12d08bd3d33b3eb5acb0029", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "25dff030-b29e-4888-b567-2321192d7ead", "node_type": "1", "metadata": {}, "hash": "71f3ca32c51d65ce7f361846dccdb3e6465106fed1b58dd1341c86f74f72cd2a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Focus on its vastness, movement, and colors.\"\n,\nmax_new_tokens\n=\n150\n,\ndo_sample\n=\nTrue\n,\nreturn_full_text\n=\nFalse\n,\ntemperature\n=\n0.7\n,\ntop_p\n=\n0.9\n,\n)[\n0\n][\n'generated_text'\n])\nreturn\npoem\n*\nnum_repetitions\n\u200b\nmodel_cache\nfor custom servers\nIf you are not using Python\u2019s\nmodel.py\nand\ncustom servers\nsuch as\nvllm\n, TEI or\nsglang\n,\nyou are required to use the\ntruss-transfer-cli\ncommand, to force population of the\n/app/model_cache\nlocation. The command will block until the weights are downloaded.\nHere is an example for how to use text-embeddings-inference on a L4 to populate a jina embeddings model from huggingface into the model_cache.\nconfig.yaml\nCopy\nAsk AI\nbase_image\n:\nimage\n:\nbaseten/text-embeddings-inference-mirror:89-1.6\ndocker_server\n:\nliveness_endpoint\n:\n/health\npredict_endpoint\n:\n/v1/embeddings\nreadiness_endpoint\n:\n/health\nserver_port\n:\n7997\n# using `truss-transfer-cli` to download the weights to `cached_model`\nstart_command\n:\nbash -c \"truss-transfer-cli && text-embeddings-router --port 7997\n--model-id /app/model_cache/my_jina --max-client-batch-size 128 --max-concurrent-requests\n128 --max-batch-tokens 16384 --auto-truncate\"\nmodel_cache\n:\n-\nrepo_id\n:\njinaai/jina-embeddings-v2-base-code\nrevision\n:\n516f4baf13dec4ddddda8631e019b5737c8bc250\nuse_volume\n:\ntrue\nvolume_folder\n:\nmy_jina\nignore_patterns\n: [\n\"*.onnx\"\n]\nmodel_metadata\n:\nexample_model_input\n:\nencoding_format\n:\nfloat\ninput\n:\ntext string\nmodel\n:\nmodel\nmodel_name\n:\nTEI-jinaai-jina-embeddings-v2-base-code-truss-example\nresources\n:\naccelerator\n:\nL4\n\u200b\nOptimizing access time futher with b10cache enabled\nb10cache is currently in beta mode\nTo further reduce weights loading time, we can enable Baseten\u2019s Distributed Filesystem (b10cache) for your account.\nYou can validate that this is enabled for your account by viewing the logs of your deployment.\nCopy\nAsk AI\n[2025-09-10 01:04:35] [INFO ] b10cache is enabled.\n[2025-09-10 01:04:35] [INFO ] Symlink created successfully. Skipping download for /app/model_cache/cached_model/model.safetensors\nOnce b10cache is active, we will skip downloads that are cached in the filesystem of the region your deployment is running in.\nb10cache acts like a content delivery network: Initial cache misses are populating the filesystem, unused files are garbage collected after 14 days.\nOnce b10cache is active, it will pull from the fastest source. If another pod is active on the same physical node, artifacts may be hot-cached, and shared among your deployments.\nDownloads are fully isolated from other organizations.\nIf b10cache is not available for your account, we will provision the model_cache with a optimized download from HuggingFace.co.\nThe download is parallellized, achieving typical download speeds of greater than 1GB/s on a 10Gbit ethernet connection.\nIf you want to enable b10cache, feel free to reach out to our support.\n\u200b\nLegacy cache - weights in container\nA slower way to make sure your weights are always available, is to download them into the docker image at build time.\nWe recommend this only for small models, of up to a size of ~1GB.\nTradeoffs:\nhighest availability: model weights will never depend on S3/huggingface uptime. High availability on b10cache.\nslower cold-starts: docker images may need to be pulled from a slower source that has lower speed S3 or Huggingface.\nunsuitable for very large-models: We don\u2019t recommend placing large model artifacts into the docker image, and may lead to build failures when larger than 50GB.\n\u200b\nDownload weights into the image via\nbuild_commands\nThe most flexible way to download weights into the docker image is the usage of custom\nbuild_commands\n.\nYou can read more on build_commands\nhere.", "mimetype": "text/plain", "start_char_idx": 186599, "end_char_idx": 190269, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "25dff030-b29e-4888-b567-2321192d7ead": {"__data__": {"id_": "25dff030-b29e-4888-b567-2321192d7ead", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "aa3a684f-0ec7-4281-9a3b-534b9cdecf75", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "250f1c8a3a208c6909ed0d610e0151ac15e6e513016df186c760f9af86bcdec6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c96e6bb5-18b2-425d-b277-61bf77277545", "node_type": "1", "metadata": {}, "hash": "793a95099682b0e5fc2da22494b29a6f215468836f1c5afe7fab13c06bceffc6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u200b\nLegacy cache - weights in container\nA slower way to make sure your weights are always available, is to download them into the docker image at build time.\nWe recommend this only for small models, of up to a size of ~1GB.\nTradeoffs:\nhighest availability: model weights will never depend on S3/huggingface uptime. High availability on b10cache.\nslower cold-starts: docker images may need to be pulled from a slower source that has lower speed S3 or Huggingface.\nunsuitable for very large-models: We don\u2019t recommend placing large model artifacts into the docker image, and may lead to build failures when larger than 50GB.\n\u200b\nDownload weights into the image via\nbuild_commands\nThe most flexible way to download weights into the docker image is the usage of custom\nbuild_commands\n.\nYou can read more on build_commands\nhere.\nconfig.yaml\nCopy\nAsk AI\nbuild_commands\n:\n-\n'apt-get install git git-lfs'\n-\n'git lfs install'\n-\n'git clone https://huggingface.co/nomic-ai/nomic-embed-text-v1.5 /data/local-model'\n-\necho 'Model downloaded to /data/local-model via git clone'\n\u200b\nDownload the weights via\nmodel_cache\nand\nuse_volume: false\nIf you are setting\nuse_volume: false\n, we will not use b10cache to mount the model weights at runtime, and rather vendor them into the docker image.\n\u200b\nHuggingface\nconfig.yaml\nCopy\nAsk AI\nmodel_cache\n:\n-\nrepo_id\n:\nmadebyollin/sdxl-vae-fp16-fix\nrevision\n:\n207b116dae70ace3637169f1ddd2434b91b3a8cd\nuse_volume\n:\nfalse\nallow_patterns\n:\n-\nconfig.json\n-\ndiffusion_pytorch_model.safetensors\nWeights will be cached in the default Hugging Face cache directory,\n~/.cache/huggingface/hub/models--{your_model_name}/\n. You can change this directory by setting the\nHF_HOME\nor\nHUGGINGFACE_HUB_CACHE\nenvironment variable in your\nconfig.yaml\n.\nRead more here\n.\nHuggingface libraries will use this directly.\nmodel.py\nCopy\nAsk AI\nfrom\ntransformers\nimport\nAutoModel\nAutoModel.from_pretrained(\n\"madebyollin/sdxl-vae-fp16-fix\"\n)\n\u200b\nGoogle Cloud Storage\nGoogle Cloud Storage is a great alternative to Hugging Face when you have a custom model or fine-tune you want to gate, especially if you are already using GCP and care about security and compliance.\nYour\nmodel_cache\nshould look something like this:\nconfig.yaml\nCopy\nAsk AI\nmodel_cache\n:\n-\nrepo_id\n:\ngs://path-to-my-bucket\nuse_volume\n:\nfalse\nIf you are accessing a public GCS bucket, you can ignore the following steps, but make sure you set appropriate permissions on your bucket. Users should be able to list and view all files. Otherwise, the model build will fail.\nFor a private GCS bucket, first export your service account key. Rename it to be\nservice_account.json\nand add it to the\ndata\ndirectory of your Truss.\nYour file structure should look something like this:\nCopy\nAsk AI\nyour-truss\n|--model\n| \u2514\u2500\u2500 model.py\n|--data\n|. \u2514\u2500\u2500 service_account.json\nIf you are using version control, like git, for your Truss, make sure to add\nservice_account.json\nto your\n.gitignore\nfile. You don\u2019t want to accidentally expose your service account key.\nWeights will be cached at\n/app/model_cache/{your_bucket_name}\n.\n\u200b\nAmazon Web Services S3\nAnother popular cloud storage option for hosting model weights is AWS S3, especially if you\u2019re already using AWS services.\nYour\nmodel_cache\nshould look something like this:\nconfig.yaml\nCopy\nAsk AI\nmodel_cache\n:\n-\nrepo_id\n:\ns3://path-to-my-bucket\nuse_volume\n:\nfalse\nIf you are accessing a public S3 bucket, you can ignore the subsequent steps, but make sure you set an appropriate policy on your bucket. Users should be able to list and view all files. Otherwise, the model build will fail.\nHowever, for a private S3 bucket, you need to first find your\naws_access_key_id\n,\naws_secret_access_key\n, and\naws_region\nin your AWS dashboard. Create a file named\ns3_credentials.json\n. Inside this file, add the credentials that you identified earlier as shown below. Place this file into the\ndata\ndirectory of your Truss.", "mimetype": "text/plain", "start_char_idx": 189450, "end_char_idx": 193343, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c96e6bb5-18b2-425d-b277-61bf77277545": {"__data__": {"id_": "c96e6bb5-18b2-425d-b277-61bf77277545", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "25dff030-b29e-4888-b567-2321192d7ead", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "5cdecb58fe60dc286e865a9ae1fd95b502ca7ca34bd936038a4f9a831c023fd1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c88f18dd-26c8-46ef-afd4-b9632b02caab", "node_type": "1", "metadata": {}, "hash": "355cbcebe4e2ff60b30ea3d8feecbe7a613f85c28ce30a36c942ba1b9cfac303", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Your\nmodel_cache\nshould look something like this:\nconfig.yaml\nCopy\nAsk AI\nmodel_cache\n:\n-\nrepo_id\n:\ns3://path-to-my-bucket\nuse_volume\n:\nfalse\nIf you are accessing a public S3 bucket, you can ignore the subsequent steps, but make sure you set an appropriate policy on your bucket. Users should be able to list and view all files. Otherwise, the model build will fail.\nHowever, for a private S3 bucket, you need to first find your\naws_access_key_id\n,\naws_secret_access_key\n, and\naws_region\nin your AWS dashboard. Create a file named\ns3_credentials.json\n. Inside this file, add the credentials that you identified earlier as shown below. Place this file into the\ndata\ndirectory of your Truss.\nThe key\naws_session_token\ncan be included, but is optional.\nHere is an example of how your\ns3_credentials.json\nfile should look:\nCopy\nAsk AI\n{\n\"aws_access_key_id\"\n:\n\"YOUR-ACCESS-KEY\"\n,\n\"aws_secret_access_key\"\n:\n\"YOUR-SECRET-ACCESS-KEY\"\n,\n\"aws_region\"\n:\n\"YOUR-REGION\"\n}\nYour overall file structure should now look something like this:\nCopy\nAsk AI\nyour-truss\n|--model\n| \u2514\u2500\u2500 model.py\n|--data\n|. \u2514\u2500\u2500 s3_credentials.json\nWhen you are generating credentials, make sure that the resulting keys have at minimum the following IAM policy:\nCopy\nAsk AI\n{\n\"Version\"\n:\n\"2012-10-17\"\n,\n\"Statement\"\n: [\n{\n\"Action\"\n: [\n\"s3:GetObject\"\n,\n\"s3:ListObjects\"\n,\n],\n\"Effect\"\n:\n\"Allow\"\n,\n\"Resource\"\n: [\n\"arn:aws:s3:::S3_BUCKET/PATH_TO_MODEL/*\"\n]\n},\n{\n\"Action\"\n: [\n\"s3:ListBucket\"\n,\n],\n\"Effect\"\n:\n\"Allow\"\n,\n\"Resource\"\n: [\n\"arn:aws:s3:::S3_BUCKET\"\n]\n}\n]\n}\nIf you are using version control, like git, for your Truss, make sure to add\ns3_credentials.json\nto your\n.gitignore\nfile. You don\u2019t want to accidentally expose your service account key.\nWeights will be cached at\n/app/model_cache/{your_bucket_name}\n.\nWas this page helpful?\nYes\nNo\nPrevious\nAccess model environments\nA guide to leveraging environments in your models\nNext\nOn this page\nEnabling Caching + Prefetching for a Model\nPrivate Hugging Face repositories \ud83e\udd17\nmodel_cache within Chains\nmodel_cache for custom servers\nOptimizing access time futher with b10cache enabled\nLegacy cache - weights in container\nDownload weights into the image via build_commands\nDownload the weights via model_cache and use_volume: false\nHuggingface\nGoogle Cloud Storage\nAmazon Web Services S3\nAssistant\nResponses are generated using AI and may contain mistakes.\n\n\nContent from https://docs.baseten.co/development/model/overview:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nOverview\nYour first model\nSetup and dependencies\nImplementation\nImplementation (Advanced)\nDeploy and iterate\nPerformance optimization\nSecurity and secrets\nData and storage\nPython driven configuration for models \ud83c\udd95\nDeveloping a Chain\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nExporting metrics\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nDeveloping a model\nDeveloping a Model on Baseten\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nBaseten makes it easy to go from a trained machine learning model to a fully-deployed, production-ready API. You\u2019ll use Truss\u2014our open-source model packaging tool\u2014to containerize your model code and configuration, and ship it to Baseten for deployment, testing, and scaling.\n\u200b\nWhat does it mean to develop a model?\nIn Baseten, developing a model means:\nPackaging your model code and weights\n:\nWrap your trained model into a structured project that includes your inference logic and dependencies.", "mimetype": "text/plain", "start_char_idx": 192654, "end_char_idx": 196563, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c88f18dd-26c8-46ef-afd4-b9632b02caab": {"__data__": {"id_": "c88f18dd-26c8-46ef-afd4-b9632b02caab", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c96e6bb5-18b2-425d-b277-61bf77277545", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "f34919197ab287dd43f50b3bf97e02a544a398ae860d92aa2a9f525c1013bb30", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8555986c-bddb-40d5-9883-8ecd3bb2b1b0", "node_type": "1", "metadata": {}, "hash": "2f8d091ab05d10f8ecb77a45bc9517fb5f7f73789d1f98dd3ea0157451dd886d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "You\u2019ll use Truss\u2014our open-source model packaging tool\u2014to containerize your model code and configuration, and ship it to Baseten for deployment, testing, and scaling.\n\u200b\nWhat does it mean to develop a model?\nIn Baseten, developing a model means:\nPackaging your model code and weights\n:\nWrap your trained model into a structured project that includes your inference logic and dependencies.\nConfiguring the model environment\n:\nDefine everything needed to run your model\u2014from Python packages to system dependencies and secrets.\nDeploying and iterating quickly\n:\nPush your model to Baseten in development mode and make live edits with instant feedback.\nOnce your model works the way you want, you can promote it to\nproduction\n, ready for live traffic.\n\u200b\nDevelopment flow on Baseten\nHere\u2019s what the typical model development loop looks like:\nInitialize a new model project\nusing the Truss CLI.\nAdd your model logic\nto a Python class (model.py), specifying how to load and run inference.\nConfigure dependencies\nin a YAML or Python config.\nDeploy the model\nin development mode using truss push.\nIterate fast\nwith truss watch\u2014live-reload your dev deployment as you make changes.\nTest and tune\nthe model until it\u2019s production-ready.\nPromote the model\nto production when you\u2019re ready to scale.\nNote:\nTruss runs your model in a standardized container without needing\nDocker installed locally. It also gives you a fast developer loop and a\nconsistent way to configure and serve models.\n\u200b\nWhat is Truss?\nTruss is the tool you use to:\nScaffold a new model project\nServe models locally or in the cloud\nPackage your code, config, and model files\nPush to Baseten for deployment\nYou can think of it as the developer toolkit for building and managing model servers\u2014built specifically for machine learning workflows.\nWith Truss, you can create a containerized model server\nwithout needing to learn Docker\n, and define everything about how your model runs: Python and system packages, GPU settings, environment variables, and custom inference logic. It gives you a fast, reproducible dev loop\u2014test changes locally or in a remote environment that mirrors production.\nTruss is\nflexible enough to support a wide range of ML stacks\n, including:\nModel frameworks like\nPyTorch\n,\ntransformers\n, and\ndiffusers\nInference engines\nlike\nTensorRT-LLM\n,\nSGLang\n,\nvLLM\nServing technologies like\nTriton\nAny package installable with\npip\nor\napt\nWe\u2019ll use Truss throughout this guide, but the focus will stay on\nhow you develop models\n, not just how Truss works.\n\u200b\nFrom model to server: the key components\nWhen you develop a model on Baseten, you define:\nA\nModel\nclass\n: This is where your model is loaded, preprocessed, run, and the results returned.\nA\nconfiguration file\n(\nconfig.yaml\nor Python config): Defines the runtime environment, dependencies, and deployment settings.\nOptional\nextra assets\n, like model weights, secrets, or external packages.\nThese components together form a\nTruss\n, which is what you deploy to Baseten.\nTruss simplifies and standardizes model packaging for seamless deployment. It encapsulates model code, dependencies, and configurations into a\nportable, reproducible structure\n, enabling efficient development, scaling, and optimization.\n\u200b\nDevelopment vs. other deployments\nThe only special deployment is\ndevelopment\n.\nDevelopment deployment\nMeant for iteration and testing. It supports\nlive-reloading\nfor quick feedback loops and will only scale to\none replica\n, no autoscaling.\nAll others deployments\nStable, autoscaled, and ready for live traffic but\ndon\u2019t support live-reloading\n.\nYou\u2019ll use the dev deployment to build and test, then promote it to an environment like\nstaging\nor\nproduction\nonce you\u2019re satisfied.\nWas this page helpful?\nYes\nNo\nPrevious\nYour first model\nBuild and deploy your first model\nNext\nOn this page\nWhat does it mean to develop a model?\nDevelopment flow on Baseten\nWhat is Truss?\nFrom model to server: the key components\nDevelopment vs. other deployments\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 196177, "end_char_idx": 200215, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8555986c-bddb-40d5-9883-8ecd3bb2b1b0": {"__data__": {"id_": "8555986c-bddb-40d5-9883-8ecd3bb2b1b0", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c88f18dd-26c8-46ef-afd4-b9632b02caab", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "a155a610f4a9bc7458f2597c5c493791f2027940424ded9cd21193e4d40f9ec4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "318b6bd8-c300-4715-a2c8-a2949c2973e5", "node_type": "1", "metadata": {}, "hash": "2d970926e2026b199333771e799598e1ddb557763fe6c990aa0deac69d97b7c9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u200b\nDevelopment vs. other deployments\nThe only special deployment is\ndevelopment\n.\nDevelopment deployment\nMeant for iteration and testing. It supports\nlive-reloading\nfor quick feedback loops and will only scale to\none replica\n, no autoscaling.\nAll others deployments\nStable, autoscaled, and ready for live traffic but\ndon\u2019t support live-reloading\n.\nYou\u2019ll use the dev deployment to build and test, then promote it to an environment like\nstaging\nor\nproduction\nonce you\u2019re satisfied.\nWas this page helpful?\nYes\nNo\nPrevious\nYour first model\nBuild and deploy your first model\nNext\nOn this page\nWhat does it mean to develop a model?\nDevelopment flow on Baseten\nWhat is Truss?\nFrom model to server: the key components\nDevelopment vs. other deployments\nAssistant\nResponses are generated using AI and may contain mistakes.\n\n\nContent from https://docs.baseten.co/development/model/performance/concepts:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nOverview\nYour first model\nSetup and dependencies\nImplementation\nImplementation (Advanced)\nDeploy and iterate\nPerformance optimization\nConcepts\nEngine builder overview\nEngine control in Python\nEngine builder configuration\nSecurity and secrets\nData and storage\nPython driven configuration for models \ud83c\udd95\nDeveloping a Chain\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nExporting metrics\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nPerformance optimization\nConcepts\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nModel performance means optimizing every layer of your model serving infrastructure to balance four goals:\nLatency\n: on a per-request basis, how quickly does each user get output from the model?\nThroughput\n: how many requests or users can the deployment handle at once?\nCost\n: how much does a standardized unit of work (e.g. 1M tokens from an LLM) cost?\nQuality\n: does your model consistently deliver high-quality output after optimization?\n\u200b\nModel performance tooling\nTensorRT-LLM Engine Builder\nBaseten\u2019s TensorRT-LLM engine builder simplifies and automates the process of\nusing TensorRT-LLM for development and production.\n\u200b\nFull-stack model performance\n\u200b\nModel and GPU selection\nTwo of the highest-impact choices for model performance come before the optimization process: picking the best model size and implementation and picking the right GPU to run it on.\nModel selection\nTradeoff: Latency/Throughput/Cost vs Quality\nThe biggest factor in your latency, throughput, cost, and quality is what model you use. Before you jump into optimizing a foundation model, consider:\nCan you use a smaller size, like Llama 8B instead of 70B? Can you fine-tune the smaller model for your use case?\nCan you use a different model, like\nSDXL Lightning\ninstead of SDXL?\nCan you use a different implementation, like\nFaster Whisper\ninstead of Whisper?\nUsually, model selection is bound by quality. For example SDXL Lightning makes images incredibly quickly, but they may not be detailed enough for your use case.\nExperiment with alternative models to see if they can reset your performance expectations while meeting your quality bar.\nGPU selection\nTradeoff: Latency/Throughput vs Cost\nThe minimum requirement for a GPU instance is that it must have enough VRAM to load model weights with headroom left for inference.\nIt often makes sense to use a more powerful (but more expensive) GPU than the minimum requirement, especially if you have ambitious latency goals and/or high utilization.\nFor example, you might choose:\n(Multiple) H100 GPUs for\ndeployments optimized with TensorRT/TensorRT-LLM\nH100 MIGs for\nhigh-throughput deployments of smaller models like Llama 3 8B and SDXL\nL4 GPUs for autoscaling Whisper deployments\nThe\nGPU instance reference\nlists all available options.\n\u200b\nGPU-level optimizations\nOur first goal is to get the best possible performance out of a single GPU or GPU cluster.", "mimetype": "text/plain", "start_char_idx": 199403, "end_char_idx": 203735, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "318b6bd8-c300-4715-a2c8-a2949c2973e5": {"__data__": {"id_": "318b6bd8-c300-4715-a2c8-a2949c2973e5", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8555986c-bddb-40d5-9883-8ecd3bb2b1b0", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "22c15247a44356ab779812263e1170b57970b5839f806dee878778dabdd5dad4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8d3f051d-8f08-4cad-8121-f9a6aeda8f8b", "node_type": "1", "metadata": {}, "hash": "0129c2134197df510e9d11b5e9c8a45f824adf6d6213c57423093f4988f3653a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Experiment with alternative models to see if they can reset your performance expectations while meeting your quality bar.\nGPU selection\nTradeoff: Latency/Throughput vs Cost\nThe minimum requirement for a GPU instance is that it must have enough VRAM to load model weights with headroom left for inference.\nIt often makes sense to use a more powerful (but more expensive) GPU than the minimum requirement, especially if you have ambitious latency goals and/or high utilization.\nFor example, you might choose:\n(Multiple) H100 GPUs for\ndeployments optimized with TensorRT/TensorRT-LLM\nH100 MIGs for\nhigh-throughput deployments of smaller models like Llama 3 8B and SDXL\nL4 GPUs for autoscaling Whisper deployments\nThe\nGPU instance reference\nlists all available options.\n\u200b\nGPU-level optimizations\nOur first goal is to get the best possible performance out of a single GPU or GPU cluster.\nInference engine\nBenefit: Latency/Throughput/Cost\nYou can just use\ntransformers\nand\npytorch\nout of the box to serve your model. But best-in-class performance comes from using a dedicated inference engine, like:\nTensorRT\n/\nTensorRT-LLM\n, maintained by NVIDIA\nvLLM\n, an independent open source project\nTGI\n, maintained by Hugging Face\nWe\noften recommend TensorRT/TensorRT-LLM\nfor best performance. The easiest way to get started with TensorRT-LLM is our\nTRT-LLM engine builder\n.\nInference server\nBenefit: Latency/Throughput\nIn addition to an optimized inference engine, you need an inference server to handle requests and supply features like in-flight batching.\nBaseten runs a modified version of Triton for compatible model deployments. Other models use\nTrussServer\n, a capable general-purpose model inference server built into Truss.\nQuantization\nTradeoff: Latency/Throughput/Cost vs Quality\nBy default, model inference happens in\nfp16\n, meaning that model weights and other values are represented as 16-bit floating-point numbers.\nThrough a process called\npost-training quantization\n, you can instead run inference in a different format, like\nfp8\n,\nint8\n, or\nint4\n. This has massive benefits: more teraFLOPS at lower precision means lower latency, smaller numbers being retrieved from VRAM means higher throughput, and smaller model weights means saving on cost and potentially using fewer GPUs.\nHowever, quantization can affect output quality. Thoroughly review quantized model outputs by hand and with standard checks like perplexity to ensure that the output of the quantized model matches the original.\nWe\u2019ve had a lot of success with\nfp8 for faster inference without quality loss\nand encourage experimenting with quantization, especially when using the TRT-LLM engine builder.\nModel-level optimizations\nTradeoff: Latency/Throughput/Cost vs Quality\nThere are a number of exciting cutting-edge techniques for model inference that can massively improve latency and/or throughput for a model. For example, LLMs can use Speculative Decoding or Medusa to generate multiple tokens per forward pass, improving TPS.\nWhen using a new technique to improve model performance, always run real-world benchmarks and carefully validate output quality to ensure the performance improvements aren\u2019t undermining the model\u2019s usefulness.\nBatching (GPU concurrency)\nTradeoff: Latency vs Throughput/Cost\nBatch size is how many requests are processed concurrently on the GPU. It is a direct tradeoff between latency and throughput:\nIncrease batch size to improve throughput and cost\nReduce batch size to improve latency\n\u200b\nInfrastructure-level optimizations\nOnce we squeeze as much TPS as possible out of the GPU, we scale that out horizontally with infrastructure optimization.\nAutoscaling\nTradeoff: Latency/Throughput vs Cost\nIf traffic to a deployment is high enough, even an optimized model server won\u2019t be able to keep up. By creating replicas, you keep latency consistent for all users.\nLearn more about\nautoscaling model replicas\n.\nReplica-level concurrency\nTradeoff: Latency vs Throughput/Cost\nReplica-level concurrency sets the number of requests that can be sent to the model server at one time. This is different from the on-GPU concurrency as your model server may perform pre- and post-processing tasks on CPU.\nReplica-level concurrency should always be greater than or equal to on-device concurrency (batch size).\nNetwork latency\nTradeoff: Latency vs Cost\nIf your GPU is in us-east-1 and your customer is in Australia, it doesn\u2019t matter how much you\u2019ve optimized TTFT \u2014 your real-world latency will be terrible.\nRegion-specific deployments are available on a per-customer basis.", "mimetype": "text/plain", "start_char_idx": 202853, "end_char_idx": 207410, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8d3f051d-8f08-4cad-8121-f9a6aeda8f8b": {"__data__": {"id_": "8d3f051d-8f08-4cad-8121-f9a6aeda8f8b", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "318b6bd8-c300-4715-a2c8-a2949c2973e5", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "147ce17a43351fd076524c0adff0e12c8dbcb6bfa4912ef2baaa6ee75c735380", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "83ef7637-1af7-4a7d-a0c7-bbfb76204433", "node_type": "1", "metadata": {}, "hash": "d5ccb8f22fca1b020e1b332f07dc82563bf058fb46778d98cae8d36ca90e03a7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "By creating replicas, you keep latency consistent for all users.\nLearn more about\nautoscaling model replicas\n.\nReplica-level concurrency\nTradeoff: Latency vs Throughput/Cost\nReplica-level concurrency sets the number of requests that can be sent to the model server at one time. This is different from the on-GPU concurrency as your model server may perform pre- and post-processing tasks on CPU.\nReplica-level concurrency should always be greater than or equal to on-device concurrency (batch size).\nNetwork latency\nTradeoff: Latency vs Cost\nIf your GPU is in us-east-1 and your customer is in Australia, it doesn\u2019t matter how much you\u2019ve optimized TTFT \u2014 your real-world latency will be terrible.\nRegion-specific deployments are available on a per-customer basis. Contact us at\nsupport@baseten.co\nto discuss your needs.\n\u200b\nApplication-level optimizations\nThere are also application-level steps that you can take to make sure you\u2019re getting the most value from your optimized endpoint.\nGood prompts\nBenefits: Latency, Quality\nEvery token an LLM doesn\u2019t have to process or generate is a token that you don\u2019t have to wait for or pay for.\nPrompt engineering can be as simple as saying \u201cbe concise\u201d or as complex as making sure your RAG system returns the minimum number of highly-relevant retrievals.\nConsistent sequence shapes\nBenefits: Latency, Throughput\nWhen using TensorRT-LLM, make sure that your input and output sequences are a consistent length. The inference engine is built for a specific number of tokens, and going outside of those sequence shapes will hurt performance.\nChains for multi-step inference\nBenefits: Latency, Cost\nThe only thing running on your GPU should be the AI model. Other tasks like retrievals, secondary models, and business logic should be deployed and scaled separately to avoid bottlenecks.\nUse\nChains\nfor performant multi-step and multi-model inference.\nSession reuse during inference\nBenefit: Latency\nUse sessions rather than individual requests to avoid unnecessary network latency. See\ninference documentation\nfor details.\nWas this page helpful?\nYes\nNo\nPrevious\nEngine builder overview\nDeploy optimized model inference servers in minutes\nNext\nOn this page\nModel performance tooling\nFull-stack model performance\nModel and GPU selection\nGPU-level optimizations\nInfrastructure-level optimizations\nApplication-level optimizations\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 206646, "end_char_idx": 209077, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "83ef7637-1af7-4a7d-a0c7-bbfb76204433": {"__data__": {"id_": "83ef7637-1af7-4a7d-a0c7-bbfb76204433", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8d3f051d-8f08-4cad-8121-f9a6aeda8f8b", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "93723124892fd1773269e9b42b967837ac7abb41ce108a8a40f123897f8debb5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d5af8599-1a36-43f7-93e7-014faadf5a0f", "node_type": "1", "metadata": {}, "hash": "3b80c25e966981c8a966fac6b43082a52effc9d1da9c391ed9cd16176ec480c0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The inference engine is built for a specific number of tokens, and going outside of those sequence shapes will hurt performance.\nChains for multi-step inference\nBenefits: Latency, Cost\nThe only thing running on your GPU should be the AI model. Other tasks like retrievals, secondary models, and business logic should be deployed and scaled separately to avoid bottlenecks.\nUse\nChains\nfor performant multi-step and multi-model inference.\nSession reuse during inference\nBenefit: Latency\nUse sessions rather than individual requests to avoid unnecessary network latency. See\ninference documentation\nfor details.\nWas this page helpful?\nYes\nNo\nPrevious\nEngine builder overview\nDeploy optimized model inference servers in minutes\nNext\nOn this page\nModel performance tooling\nFull-stack model performance\nModel and GPU selection\nGPU-level optimizations\nInfrastructure-level optimizations\nApplication-level optimizations\nAssistant\nResponses are generated using AI and may contain mistakes.\n\n\nContent from https://docs.baseten.co/development/model/performance/concurrency:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nOverview\nYour first model\nSetup and dependencies\nImplementation\nImplementation (Advanced)\nRequest handling\nCustom Responses\nCustom servers\nCustom health checks \ud83c\udd95\nCached weights \ud83c\udd95\nAccess model environments\nRequest concurrency\nStreaming output\nDeploy and iterate\nPerformance optimization\nSecurity and secrets\nData and storage\nPython driven configuration for models \ud83c\udd95\nDeveloping a Chain\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nExporting metrics\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nImplementation (Advanced)\nRequest concurrency\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nConfiguring concurrency optimizes\nmodel performance\n, balancing\nthroughput\nand\nlatency\n.\nIn Baseten & Truss, concurrency is managed at\ntwo levels\n:\nConcurrency Target\n\u2013 Limits the number of requests\nsent\nto a single replica.\nPredict Concurrency\n\u2013 Limits how many requests the predict function handles\ninside the model container\n.\n\u200b\n1. Concurrency Target\nSet in the Baseten UI\n\u2013 Defines how many requests a single replica can process at once.\nTriggers autoscaling\n\u2013 If all replicas hit the concurrency target, additional replicas spin up.\nExample:\nConcurrency Target = 2, Single Replica\n5 requests arrive\n\u2192 2 are processed immediately,\n3 are queued\n.\nIf max replicas aren\u2019t reached,\nautoscaling spins up a new replica\n.\n\u200b\n2. Predict Concurrency\nSet in\nconfig.yaml\n\u2013 Controls how many requests can be\nprocessed by\npredict simultaneously.\nProtects GPU resources\n\u2013 Prevents multiple requests from overloading the GPU.\n\u200b\nConfiguring Predict Concurrency\nconfig.yaml\nCopy\nAsk AI\nmodel_name\n:\n\"My model with concurrency limits\"\nruntime\n:\npredict_concurrency\n:\n2\n# Default is 1\n\u200b\nHow It Works Inside a Model Pod\nRequests arrive\n\u2192 All begin preprocessing (e.g., downloading images from S3).\nPredict runs on GPU\n\u2192 Limited by\npredict_concurrency\n.\nPostprocessing begins\n\u2192 Can run while other requests are still in inference.\n\u200b\nWhen to Use Predict Concurrency\n\u2705\nProtect GPU resources\n\u2013 Prevent multiple requests from degrading performance.\n\u2705\nAllow parallel preprocessing/postprocessing\n\u2013 I/O tasks can continue even when inference is blocked.\nEnsure\nConcurrency Target\nis set high enough to send enough requests to the container.\nWas this page helpful?\nYes\nNo\nPrevious\nStreaming output\nStreaming Output for LLMs\nNext\nOn this page\n1. Concurrency Target\n2. Predict Concurrency\nConfiguring Predict Concurrency\nHow It Works Inside a Model Pod\nWhen to Use Predict Concurrency\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 208097, "end_char_idx": 212223, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d5af8599-1a36-43f7-93e7-014faadf5a0f": {"__data__": {"id_": "d5af8599-1a36-43f7-93e7-014faadf5a0f", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "83ef7637-1af7-4a7d-a0c7-bbfb76204433", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "a31e80777b8ea2e19d8078cc534578dede70a10348136e1a6a3a108de856d07e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f95f4e3c-7da7-455e-b679-83ca5daa71fc", "node_type": "1", "metadata": {}, "hash": "98ae2ea30f45dd9fa658ffea3a60e4031fb35f03a7b23e89e528b0fe7ad02f50", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Content from https://docs.baseten.co/development/model/performance/engine-builder-config:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nOverview\nYour first model\nSetup and dependencies\nImplementation\nImplementation (Advanced)\nDeploy and iterate\nPerformance optimization\nConcepts\nEngine builder overview\nEngine control in Python\nEngine builder configuration\nSecurity and secrets\nData and storage\nPython driven configuration for models \ud83c\udd95\nDeveloping a Chain\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nExporting metrics\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nPerformance optimization\nEngine builder configuration\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nThis reference lists every configuration option for the TensorRT-LLM Engine Builder. These options are used in\nconfig.yaml\n, such as for this Llama 3.1 8B example:\nconfig.yaml\nCopy\nAsk AI\nmodel_name\n:\nLlama 3.1 8B Engine\nresources\n:\naccelerator\n:\nH100:1\nsecrets\n:\nhf_access_token\n:\n\"set token in baseten workspace\"\ntrt_llm\n:\nbuild\n:\nbase_model\n:\ndecoder\ncheckpoint_repository\n:\nrepo\n:\nmeta-llama/Llama-3.1-8B-Instruct\nsource\n:\nHF\n\u200b\ntrt_llm.build\nTRT-LLM engine build configuration. TensorRT-LLM attempts to build a highly optimized network based on input shapes representative of your workload.\n\u200b\nbase_model\nThe base model architecture of your model checkpoint. Supported architectures include:\ndecoder\n- for CausalLM such as\nLlama/Mistral/Qwen3ForCausalLM\nencoder\n- for\nBert/Roberta/LLamaForSequenceClassification\n, sentence-transformer models, embedding models\nDeprecated:\nllama\n(decrecated, use\ndecoder\n)\nmistral\n(decrecated, use\ndecoder\n)\ndeepseek\n(decrecated, use\ndecoder\n)\nqwen\n(decrecated, use\ndecoder\n)\nwhisper\n(deprecated, part of a separate product line)\n\u200b\ncheckpoint_repository\nSpecification of the model checkpoint to be leveraged for engine building. E.g.\nCopy\nAsk AI\ncheckpoint_repository\n:\nsource\n:\nHF | GCS | REMOTE_URL\nrepo\n:\nmeta-llama/Llama-3.1-8B-Instruct | gs://bucket_name | https://your-checkpoint.com/model.tar.gz\nTo configure access to private model checkpoints,\nregister secrets in your Baseten workspace\n, namely the\nhf_access_token\nor\ntrt_llm_gcs_service_account\nsecrets with a valid service account json for HuggingFace or GCS, respectively.\n\u200b\ncheckpoint_repository.source\nSource where the checkpoint is stored. This should contain assets as if using git clone with lfs for a Hugging Face repository.\nThis includes the tokenizer files, remote code and safetensor files and any json file related to configuration.\nFor GCS/REMOTE_URL, we require the files to be organized in the same folder structured to a huggingface transformers repository.\nSupported sources include:\nHF\n(HuggingFace)\nGCS\n(Google Cloud Storage)\nREMOTE_URL\nA tarball containing your checkpoint.\nImportant\n: the archive must unpack with all required files (e.g.,\nconfig.json\n) at the root level. For example,\nconfig.json\nshould be directly in the tarball, not nested under a subdirectory like\nmodel_name/config.json\n.\n\u200b\ncheckpoint_repository.repo\nCheckpoint repository name, bucket, or url.\n\u200b\nmax_batch_size\n(default:\n256\n)\nMaximum number of input sequences to pass through the engine concurrently. Batch size and throughput share a direct relation, whereas batch size and single request latency share an indirect relation.\nTune this value according to your SLAs and latency budget.\n\u200b\nmax_seq_len\n(default: max_position_embeddings from the model repo)\nDefines the maximum sequence length (context) of single request\u200b.", "mimetype": "text/plain", "start_char_idx": 212226, "end_char_idx": 216208, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f95f4e3c-7da7-455e-b679-83ca5daa71fc": {"__data__": {"id_": "f95f4e3c-7da7-455e-b679-83ca5daa71fc", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d5af8599-1a36-43f7-93e7-014faadf5a0f", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "08bc9bea9887590265df8f8db8add1c17ba7da465c4485044a082494855ccb45", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "afaa625b-12b0-4621-b5ea-fefcdca49e17", "node_type": "1", "metadata": {}, "hash": "00ada23d72db8527c638223e891b6ab7b9f64ce4f51a7889f40efd844ca62988", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Supported sources include:\nHF\n(HuggingFace)\nGCS\n(Google Cloud Storage)\nREMOTE_URL\nA tarball containing your checkpoint.\nImportant\n: the archive must unpack with all required files (e.g.,\nconfig.json\n) at the root level. For example,\nconfig.json\nshould be directly in the tarball, not nested under a subdirectory like\nmodel_name/config.json\n.\n\u200b\ncheckpoint_repository.repo\nCheckpoint repository name, bucket, or url.\n\u200b\nmax_batch_size\n(default:\n256\n)\nMaximum number of input sequences to pass through the engine concurrently. Batch size and throughput share a direct relation, whereas batch size and single request latency share an indirect relation.\nTune this value according to your SLAs and latency budget.\n\u200b\nmax_seq_len\n(default: max_position_embeddings from the model repo)\nDefines the maximum sequence length (context) of single request\u200b.\n\u200b\nmax_num_tokens\n(default:\n8192\n)\nDefines the maximum number of batched input tokens after padding is removed in each batch. Tuning this value more efficiently allocates memory to KV cache and executes more requests together.\n\u200b\nmax_prompt_embedding_table_size\n(default:\n0\n)\nMaximum prompt embedding table size for\nprompt tuning\n.\n\u200b\nnum_builder_gpus\n(default:\nauto\n)\nNumber of GPUs to be used at build time, defaults to configured\nresource.accelerator\ncount \u2013 useful for FP8 quantization in particular, when more GPU memory is required at build time relative to memory usage at inference.\n\u200b\nplugin_configuration\nConfig for inserting plugin nodes into network graph definition for execution of user-defined kernels.\n\u200b\nplugin_configuration.paged_kv_cache\n(default:\nTrue\n)\nDecompose KV cache into page blocks. Read more about what this does\nhere\n.\n\u200b\nplugin_configuration.use_paged_context_fmha\n(default:\nTrue\n)\nUtilize paged context for fused multihead attention. This configuration is necessary to enable KV cache reuse. Read more about this configuration\nhere\n.\n\u200b\nplugin_configuration.use_fp8_context_fmha\n(default:\nFalse\n)\nUtilize FP8 quantization for context fused multihead attention to accelerate attention. To use this configuration, also set\nplugin_configuration.use_paged_context_fmha\n. Read more about when to enable this\nhere\n.\n\u200b\nquantization_type\n(default:\nno_quant\n)\nQuantization format with which to build the engine. Supported formats include:\nno_quant\n(meaning bf16)\nfp8\nfp8_kv\nThe following quantization\nsmooth_quant\nweights_int8\nweights_kv_int8\nweights_int4\nweights_int4_kv_int8\nRead more about different post training quantization techniques supported by TRT-LLM\nhere\n.\nAdditionally, refer to the hardware and quantization technique\nsupport matrix\n.\n\u200b\nstrongly_typed\n(default:\nFalse\n)\nWhether to build the engine using strong typing, enabling TensorRT\u2019s optimizer to statically infer intermediate tensor types which can speed up build time for some formats.\nWeak typing enables the optimizer to elect tensor types, which may result in a faster runtime. For more information refer to TensorRT documentation\nhere\n.\n\u200b\ntensor_parallel_count\n(default:\n1\n)\nTensor parallelism count. For more information refer to NVIDIA documentation\nhere\n.\n\u200b\nspeculator\n(default:\nNone\n)\nConfig for inserting optional speculative decoding options.\n\u200b\nSpeculation with lookahead decoding\nSpeculation with lookahead decoding can be used by any model and does not require training.\nThe implemenation is based on the work at\nlmsys.\nWe currently disallow performing structured generation and tool-calling with this optimization.\nCopy\nAsk AI\nmodel_name\n:\nLlama-3.1-8B-Instruct (lookahead decoding)\nresources\n:\naccelerator\n:\nH100\nuse_gpu\n:\ntrue\ntrt_llm\n:\nbuild\n:\nbase_model\n:\nllama\ncheckpoint_repository\n:\nrepo\n:\nmeta-llama/Llama-3.1-8B-Instruct\nsource\n:\nHF\nmax_batch_size\n:\n32\nquantization_type\n:\nfp8_kv\nspeculator\n:\nspeculative_decoding_mode\n:\nLOOKAHEAD_DECODING\nwindows_size\n:\n7\nngram_size\n:\n5\nverification_set_size\n:\n7\nruntime\n:\nkv_cache_free_gpu_mem_fraction\n:\n0.62\n\u200b\nSpeculation with external draft model\nSpeculative decoding with draft models (e.g., using DRAFT_TOKENS_EXTERNAL) is\nan advanced feature that requires careful GPU memory allocation to accommodate\nboth models simultaneously. If you plan to use this technique, consider\nconsulting the Baseten support team for guidance on optimal configuration.\nSpeculative draft model configuration to be used for speculative decoding.", "mimetype": "text/plain", "start_char_idx": 215367, "end_char_idx": 219682, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "afaa625b-12b0-4621-b5ea-fefcdca49e17": {"__data__": {"id_": "afaa625b-12b0-4621-b5ea-fefcdca49e17", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f95f4e3c-7da7-455e-b679-83ca5daa71fc", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "8d7f5e2051fc1007b7a02c056fe0b0cfc1c8c197e8f8eaae22abb29478c9dd17", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "83c4bccb-d8e5-48b6-8465-aade8a02ff5e", "node_type": "1", "metadata": {}, "hash": "098c41f3d2091f902e5973619b71d98137518d669d3095072b2ee56e15635929", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "If you plan to use this technique, consider\nconsulting the Baseten support team for guidance on optimal configuration.\nSpeculative draft model configuration to be used for speculative decoding. By default, the speculator build will attempt to reuse as much of the target model build configuration.\nTo fully specify your own speculator build, define\nspeculator.build\n.\nFor example, here is a sample configuration for utilizing speculative decoding for Llama-3-70B-Instruct:\nCopy\nAsk AI\nmodel_name\n:\nLlama-3.1-70B-Instruct (External Token Spec-Dec)\nresources\n:\naccelerator\n:\nH100\nuse_gpu\n:\ntrue\ntrt_llm\n:\nbuild\n:\nbase_model\n:\nqwen\ncheckpoint_repository\n:\nrepo\n:\nmeta-llama/Llama-3.3-8B-Instruct\nsource\n:\nHF\nmax_batch_size\n:\n32\nquantization_type\n:\nfp8_kv\ntensor_parallel_count\n:\n2\nspeculator\n:\nspeculative_decoding_mode\n:\nDRAFT_TOKENS_EXTERNAL\ncheckpoint_repository\n:\nrepo\n:\nmeta-llama/Llama-3.2-1B-Instruct\nsource\n:\nHF\nnum_draft_tokens\n:\n4\nruntime\n:\nkv_cache_free_gpu_mem_fraction\n:\n0.62\n\u200b\nspeculator.speculative_decoding_mode\nThe type of speculative decoding tactic. Supported are:\n\u201cDRAFT_TOKENS_EXTERNAL\u201d\n\u201cLOOKAHEAD_DECODING\u201d (recommend)\n\u200b\nspeculator.num_draft_tokens\nNumber of draft tokens to sample from the speculative model. This depends on how many tokens are expected to be accepted, a good range of values to start with are between 2-8.\nAutomatically calculated field for lookahead decoding.\n\u200b\nspeculator.checkpoint_repository\nSee\ncheckpoint_repository\nfor details.\n\u200b\nspeculator.lookahead_ngram_size\n,\nspeculator.lookahead_windows_size\n,\nspeculator.lookahead_verification_set_size\nUsage of ngram size, window size, verification_set_size in the lookahead algorithm.\nwindows_size\nis the Jacobi window size, meaning number of n-grams in lookahead branch that explores future draft tokens.\nngram_size\nis the n-gram size, meaning the maximum number of draft tokens accepted per iteration.\nverification_set_size\nis the maximum number of n-grams considered for verification, meaning the number of draft token beam hypotheses.\nA good default value could be [5,5,5]. Often, lookahead_verification_set_size is set to lookahead_windows_size.\nlookahead_ngram_size\nis often increased when the generated tokens are simlar to contents of the prompt, and decreased if dissimilar.\n\u200b\nlora_adapters\n(default:\nNone\n)\nA mapping from LoRA names to checkpoint repositories.\nFor example,\nCopy\nAsk AI\ncheckpoint_repository\n:\nrepo\n:\nmeta-llama/Llama-2-13b-hf\nsource\n:\nHF\nlora_adapters\n:\nlora1\n:\nrepo\n:\nhfl/chinese-llama-2-lora-13b\nsource\n:\nHF\nSee\ncheckpoint_repository\nfor details on how to configure checkpoint repositories.\nIn addition to specifying the LoRAs here, you need to specify the\nserved_model_name\nthat is used to refer to the base model.\nThe\nserved_model_name\nis required for deploying LoRAs.\nThe LoRA name (in the example above, this is \u201clora1\u201d) is used to query the model using the specified LoRA.\n\u200b\ntrt_llm.runtime\nTRT-LLM engine runtime configuration.\n\u200b\nkv_cache_free_gpu_mem_fraction\n(default:\n0.9\n)\nUsed to control the fraction of free gpu memory allocated for the KV cache. For more information, refer to the documentation\nhere\n.\nIf you are using DRAFT_TOKENS_EXTERNAL, we recommend to lower this, depending on the draft model size.\n\u200b\nenable_chunked_context\n(default:\nFalse\n)\nEnables chunked context, increasing the chance of batch processing between context and generation phase \u2013\u00a0which may be useful to increase throughput.\nNote that one must set\nplugin_configuration.use_paged_context_fmha: True\nin order to leverage this feature.\n\u200b\nbatch_scheduler_policy\n(default:\nguaranteed_no_evict\n)\nSupported scheduler policies are as follows:\nguaranteed_no_evict\nmax_utilization\nguaranteed_no_evict\nensures that an in progress request is never evicted by reserving KV cache space for the maximum possible tokens that can be returned for a request.\nmax_utilization\npacks as many requests as possible during scheduling, which may increase throughput at the expense of additional latency.\nFor more information refer to the NVIDIA documentation\nhere\n.", "mimetype": "text/plain", "start_char_idx": 219489, "end_char_idx": 223530, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "83c4bccb-d8e5-48b6-8465-aade8a02ff5e": {"__data__": {"id_": "83c4bccb-d8e5-48b6-8465-aade8a02ff5e", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "afaa625b-12b0-4621-b5ea-fefcdca49e17", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "0366ee669bac5d5d4ed83ccfdc819dcbd557971b85650d99933e4f4f93f79139", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d45b2642-cfd0-467f-85ab-50f4cdd1b8c5", "node_type": "1", "metadata": {}, "hash": "685518a20d51e59656acd6cce050ac5a77f73e0ab360cfb1d138b5486ee1bfdc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "If you are using DRAFT_TOKENS_EXTERNAL, we recommend to lower this, depending on the draft model size.\n\u200b\nenable_chunked_context\n(default:\nFalse\n)\nEnables chunked context, increasing the chance of batch processing between context and generation phase \u2013\u00a0which may be useful to increase throughput.\nNote that one must set\nplugin_configuration.use_paged_context_fmha: True\nin order to leverage this feature.\n\u200b\nbatch_scheduler_policy\n(default:\nguaranteed_no_evict\n)\nSupported scheduler policies are as follows:\nguaranteed_no_evict\nmax_utilization\nguaranteed_no_evict\nensures that an in progress request is never evicted by reserving KV cache space for the maximum possible tokens that can be returned for a request.\nmax_utilization\npacks as many requests as possible during scheduling, which may increase throughput at the expense of additional latency.\nFor more information refer to the NVIDIA documentation\nhere\n.\n\u200b\nrequest_default_max_tokens\n(default:\nNone\n)\nDefault server configuration for the maximum number of tokens to generate for a single sequence, if one is not provided in the request body.\nSensible settings depend on your use case, a general value to set can be around 1000 tokens.\n\u200b\nserved_model_name\n(default:\nNone\n)\nThe name used to refer to the base model when using LoRAs.\nAt least one LoRA must be specified under\nlora_adapters\nto use LoRAs.\nWas this page helpful?\nYes\nNo\nPrevious\nSecurity and secrets\nUsing secrets securely in your ML models\nNext\nOn this page\ntrt_llm.build\nbase_model\ncheckpoint_repository\ncheckpoint_repository.source\ncheckpoint_repository.repo\nmax_batch_size\nmax_seq_len\nmax_num_tokens\nmax_prompt_embedding_table_size\nnum_builder_gpus\nplugin_configuration\nplugin_configuration.paged_kv_cache\nplugin_configuration.use_paged_context_fmha\nplugin_configuration.use_fp8_context_fmha\nquantization_type\nstrongly_typed\ntensor_parallel_count\nspeculator\nSpeculation with lookahead decoding\nSpeculation with external draft model\nspeculator.speculative_decoding_mode\nspeculator.num_draft_tokens\nspeculator.checkpoint_repository\nspeculator.lookahead_ngram_size, speculator.lookahead_windows_size, speculator.lookahead_verification_set_size\nlora_adapters\ntrt_llm.runtime\nkv_cache_free_gpu_mem_fraction\nenable_chunked_context\nbatch_scheduler_policy\nrequest_default_max_tokens\nserved_model_name\nAssistant\nResponses are generated using AI and may contain mistakes.\n\n\nContent from https://docs.baseten.co/development/model/performance/engine-builder-customization:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nOverview\nYour first model\nSetup and dependencies\nImplementation\nImplementation (Advanced)\nDeploy and iterate\nPerformance optimization\nConcepts\nEngine builder overview\nEngine control in Python\nEngine builder configuration\nSecurity and secrets\nData and storage\nPython driven configuration for models \ud83c\udd95\nDeveloping a Chain\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nExporting metrics\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nPerformance optimization\nEngine control in Python\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nWhen you create a new Truss with\ntruss init\n, it creates two files:\nconfig.yaml\nand\nmodel/model.py\n. While you configure the Engine Builder in\nconfig.yaml\n, you may use\nmodel/model.py\nto access and control the engine object during inference.\nYou have two options:\nDelete the\nmodel/model.py\nfile and your TensorRT-LLM engine will run according to its base spec.\nUpdate the code to support TensorRT-LLM.\nYou must either update\nmodel/model.py\nto pass\ntrt_llm\nas an argument to the\n__init__\nmethod OR delete the file. Otherwise you will get an error on deployment as the default\nmodel/model.py\nfile is not written for TensorRT-LLM.", "mimetype": "text/plain", "start_char_idx": 222620, "end_char_idx": 226805, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d45b2642-cfd0-467f-85ab-50f4cdd1b8c5": {"__data__": {"id_": "d45b2642-cfd0-467f-85ab-50f4cdd1b8c5", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "83c4bccb-d8e5-48b6-8465-aade8a02ff5e", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "150cad9142b1e25fc166bd5f01fa605f0d070c4fc6ef9fbbd5c5f228c2b7d570", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "686d51e9-53cc-4358-9569-0decee462e14", "node_type": "1", "metadata": {}, "hash": "ce6943fc02a4f630001f27aa89d15b191217e4aeb3c9362fc7172757ed7f8840", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "While you configure the Engine Builder in\nconfig.yaml\n, you may use\nmodel/model.py\nto access and control the engine object during inference.\nYou have two options:\nDelete the\nmodel/model.py\nfile and your TensorRT-LLM engine will run according to its base spec.\nUpdate the code to support TensorRT-LLM.\nYou must either update\nmodel/model.py\nto pass\ntrt_llm\nas an argument to the\n__init__\nmethod OR delete the file. Otherwise you will get an error on deployment as the default\nmodel/model.py\nfile is not written for TensorRT-LLM.\nThe\nengine\nobject is a property of the\ntrt_llm\nargument and must be initialized in\n__init__\nto be accessed in\nload()\n(which runs once on server start-up) and\npredict()\n(which runs for each request handled by the server).\nThis example applies a chat template with the Llama 3.1 8B tokenizer to the model prompt:\nmodel/model.py\nCopy\nAsk AI\nimport\norjson\n# faster serialization/deserialization than built-in json\nfrom\ntyping\nimport\nAny, AsyncIterator\nfrom\ntransformers\nimport\nAutoTokenizer\nfrom\nfastapi.responses\nimport\nStreamingResponse\nSSE_PREFIX\n=\n\"data: \"\nclass\nModel\n:\ndef\n__init__\n(\nself\n,\ntrt_llm\n,\n**\nkwargs\n) ->\nNone\n:\nself\n._secrets\n=\nkwargs[\n\"secrets\"\n]\nself\n._engine\n=\ntrt_llm[\n\"engine\"\n]\nself\n._model\n=\nNone\nself\n._tokenizer\n=\nNone\ndef\nload\n(\nself\n) ->\nNone\n:\nself\n._tokenizer\n=\nAutoTokenizer.from_pretrained(\n\"meta-llama/Llama-3.1-8B-Instruct\"\n,\ntoken\n=\nself\n._secrets[\n\"hf_access_token\"\n])\nasync\ndef\npredict\n(\nself\n,\nmodel_input\n: Any) -> Any:\n# Apply chat template to prompt\nmodel_input[\n\"prompt\"\n]\n=\nself\n._tokenizer.apply_chat_template(model_input[\n\"prompt\"\n],\ntokenize\n=\nFalse\n)\nresponse\n=\nawait\nself\n._engine.predict(model_input)\n# If the response is streaming, post-process each chunk\nif\nisinstance\n(response, StreamingResponse):\ntoken_gen\n=\nresponse.body_iterator\nasync\ndef\nprocessed_stream\n():\nasync\nfor\nchunk\nin\nsome_post_processing_function(token_gen):\nyield\nchunk\nreturn\nStreamingResponse(processed_stream(),\nmedia_type\n=\n\"text/event-stream\"\n)\n# Otherwise, return the raw output\nelse\n:\nreturn\nresponse\n# --- Post-processing helpers for SSE ---\ndef\nparse_sse_chunk\n(\nchunk\n:\nbytes\n) ->\ndict\n|\nNone\n:\n\"\"\"Parses an SSE-formatted chunk and returns the JSON payload.\"\"\"\ntry\n:\ntext\n=\nchunk.decode(\n\"utf-8\"\n).strip()\nif\nnot\ntext.startswith(\nSSE_PREFIX\n):\nreturn\nNone\nreturn\norjson.loads(text[\nlen\n(\nSSE_PREFIX\n):])\nexcept\nException\n:\nreturn\nNone\ndef\nformat_sse_chunk\n(\npayload\n:\ndict\n) ->\nbytes\n:\n\"\"\"Formats a JSON payload back into an SSE chunk.\"\"\"\nreturn\nf\n\"\n{\nSSE_PREFIX\n}\n\"\n.encode(\n\"utf-8\"\n)\n+\norjson.dumps(payload)\n+\nb\n\"\n\\n\\n\n\"\ndef\ntransform_payload\n(\npayload\n:\ndict\n) ->\ndict\n:\n\"\"\"Add a new field to the SSE payload.\"\"\"\npayload[\n\"my_new_field\"\n]\n=\n\"my_new_value\"\nreturn\npayload\nasync\ndef\nsome_post_processing_function\n(\ntoken_gen\n: AsyncIterator[\nbytes\n]\n) -> AsyncIterator[\nbytes\n]:\n\"\"\"Post-process each SSE chunk in the stream.\"\"\"\nasync\nfor\nchunk\nin\ntoken_gen:\npayload\n=\nparse_sse_chunk(chunk)\nif\npayload\nis\nNone\n:\nyield\nchunk\ncontinue\ntransformed\n=\ntransform_payload(payload)\nyield\nformat_sse_chunk(transformed)\nWas this page helpful?\nYes\nNo\nPrevious\nEngine builder configuration\nConfigure your TensorRT-LLM inference engine\nNext\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 226279, "end_char_idx": 229530, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "686d51e9-53cc-4358-9569-0decee462e14": {"__data__": {"id_": "686d51e9-53cc-4358-9569-0decee462e14", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d45b2642-cfd0-467f-85ab-50f4cdd1b8c5", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "39971ef71c94cbc5a9ff218155bd7be6248987761d225e72d75cc52c8878b2af", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b6606255-69d5-434e-9718-006b5038069d", "node_type": "1", "metadata": {}, "hash": "c24f31c2be84f7f2974352d8e998624fafef58877bd2e056b906f2cf364042c2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "payload[\n\"my_new_field\"\n]\n=\n\"my_new_value\"\nreturn\npayload\nasync\ndef\nsome_post_processing_function\n(\ntoken_gen\n: AsyncIterator[\nbytes\n]\n) -> AsyncIterator[\nbytes\n]:\n\"\"\"Post-process each SSE chunk in the stream.\"\"\"\nasync\nfor\nchunk\nin\ntoken_gen:\npayload\n=\nparse_sse_chunk(chunk)\nif\npayload\nis\nNone\n:\nyield\nchunk\ncontinue\ntransformed\n=\ntransform_payload(payload)\nyield\nformat_sse_chunk(transformed)\nWas this page helpful?\nYes\nNo\nPrevious\nEngine builder configuration\nConfigure your TensorRT-LLM inference engine\nNext\nAssistant\nResponses are generated using AI and may contain mistakes.\n\n\nContent from https://docs.baseten.co/development/model/performance/engine-builder-overview:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nOverview\nYour first model\nSetup and dependencies\nImplementation\nImplementation (Advanced)\nDeploy and iterate\nPerformance optimization\nConcepts\nEngine builder overview\nEngine control in Python\nEngine builder configuration\nSecurity and secrets\nData and storage\nPython driven configuration for models \ud83c\udd95\nDeveloping a Chain\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nExporting metrics\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nPerformance optimization\nEngine builder overview\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nIf you have a foundation model like Llama 3 or a fine-tuned variant and want to create a low-latency, high-throughput model inference server, TensorRT-LLM via the Engine Builder is likely the tool for you.\nTensorRT-LLM is an open source performance optimization toolbox created by NVIDIA. It helps you build TensorRT engines for large language models like Llama and Mistral as well as certain other models like Whisper and large vision models.\nBaseten\u2019s TensorRT-LLM Engine Builder simplifies and automates the process of using TensorRT-LLM for development and production. All you need to do is write a few lines of configuration and an optimized model serving engine will be built automatically during the model deployment process.\n\u200b\nFAQs\n\u200b\nWhere are the engines stored?\nThe engines are stored in Baseten but owned by the user \u2014 we\u2019re working on a mechanism for downloading them. In the meantime, reach out if you need access to an engine that you created using the Engine Builder.\n\u200b\nDoes the Engine Builder support quantization?\nYes. The Engine Builder can perform post-training quantization during the building process. For supported options, see\nquantization in the config reference\n.\n\u200b\nCan I customize the engine behavior?\nFor further control over the TensorRT-LLM engine during inference, use the\nmodel/model.py\nfile to access the engine object at runtime. See\ncontrolling engines with Python\nfor details.\nWas this page helpful?\nYes\nNo\nPrevious\nEngine control in Python\nUse `model.py` to customize engine behavior\nNext\nOn this page\nFAQs\nWhere are the engines stored?\nDoes the Engine Builder support quantization?\nCan I customize the engine behavior?\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 228949, "end_char_idx": 232424, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b6606255-69d5-434e-9718-006b5038069d": {"__data__": {"id_": "b6606255-69d5-434e-9718-006b5038069d", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "686d51e9-53cc-4358-9569-0decee462e14", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "9fde3d62ce8edcd5fe7c591b6e4b7f7d956349bc8fd1c5331926a01dcc77ffe7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "776e2399-4d16-496a-ae37-6293812c0aa7", "node_type": "1", "metadata": {}, "hash": "fe4b41ce591bacfbebedf8cf382e47ce9911eb1c78c15d9d8e38f44cbcd9f3b3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Content from https://docs.baseten.co/development/model/private-registries:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nOverview\nYour first model\nSetup and dependencies\nConfiguration\nCustom build commands\nBase Docker images\nPrivate Docker Registries\nImplementation\nImplementation (Advanced)\nDeploy and iterate\nPerformance optimization\nSecurity and secrets\nData and storage\nPython driven configuration for models \ud83c\udd95\nDeveloping a Chain\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nExporting metrics\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nSetup and dependencies\nPrivate Docker Registries\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nTruss uses containerized environments to ensure consistent model execution across deployments. When deploying a custom base image or a custom server from a private registry, you must grant Baseten access to download that image.\n\u200b\nAWS Elastic Cloud Registry (ECR)\nAWS supports using either\nservice accounts\n, or\naccess tokens\nfor short lived access for container registry authentication.\n\u200b\nAWS IAM Service accounts\nTo use an IAM service account for long-lived access, you can use the\nAWS_IAM\nauthentication method in Truss.\nGet an AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY from the AWS dashboard\nAdd these as\nsecrets\nin Baseten. These should be named\naws_access_key_id\nand\naws_secret_access_key\nrespectively.\nChoose the\nAWS_IAM\nauthentication method when setting up your Truss. The\nconfig.yaml\nfile should look something like this:\nCopy\nAsk AI\n...\nbase_image:\nimage: <aws account id>.dkr.ecr.<region>.amazonaws.com/path/to/image\ndocker_auth:\nauth_method: AWS_IAM\nregistry: <aws account id>.dkr.ecr.<region>.amazonaws.com\nsecrets:\naws_access_key_id: null\naws_secret_access_key: null\n...\nNote here that you need to specify the registry and image separately.\nIf you\u2019d like to use different secret names besides the default, you can configure these using the\naws_access_key_id_secret_name\nand\naws_secret_access_key_secret_name\noptions\nunder\ndocker_auth\n:\nCopy\nAsk AI\n...\nbase_image:\n...\ndocker_auth:\nauth_method: AWS_IAM\nregistry: <aws account id>.dkr.ecr.<region>.amazonaws.com\naws_access_key_id_secret_name: custom_aws_access_key_secret\naws_secret_access_key_secret_name: custom_aws_secret_key_secret\nsecrets:\ncustom_aws_access_key_secret: null\ncustom_aws_secret_key_secret: null\n\u200b\nAccess Token\nGet the a\nBase64-encoded\nsecret:\nCopy\nAsk AI\nPASSWORD\n=\n`\naws\necr get-login-password\n--region\n{us-east-1}`\necho\n-n\n\"AWS:\n$PASSWORD\n\"\n|\nbase64\nAdd a new\nsecret\nto Baseten named\nDOCKER_REGISTRY_{aws account id}.dkr.ecr.{us-east-1}.amazonaws.com\nwith the\nBase64-encoded secret\nas the value.\nAdd the secret name to the\nsecrets\nsection of the\nconfig.yaml\nto allow this model to access the secret when it is pushed.\nconfig.yaml\nCopy\nAsk AI\nsecrets\n:\nDOCKER_REGISTRY_{aws account id}.dkr.ecr.{us-east-1}.amazonaws.com\n:\nnull\n\u200b\nGoogle Cloud Artifact Registry\nGCP supports using either\naccess tokens\nfor short lived access or\nservice accounts\nfor container registry authentication.\n\u200b\nService Account\nGet your\nservice account key\nas a JSON key blob.\nAdd a new\nsecret\nto Baseten named\ngcp-service-account\n(or similar) with the JSON key blob as the value.\nAdd the secret name that you used to the\nsecrets\nsection of the\nconfig.yaml\nto allow this model to access the secret when it is pushed.", "mimetype": "text/plain", "start_char_idx": 232427, "end_char_idx": 236244, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "776e2399-4d16-496a-ae37-6293812c0aa7": {"__data__": {"id_": "776e2399-4d16-496a-ae37-6293812c0aa7", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b6606255-69d5-434e-9718-006b5038069d", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "5c5cff23a1c44a7351fab4011850b158fb8bfdbfed5c067e31862ef3113e8313", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f84eacb8-56f7-435e-a192-160e385b8f3a", "node_type": "1", "metadata": {}, "hash": "637198dec8cc50afcef92855e3dade122bfe7faa8c79b89a117aea421504a5ff", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "{us-east-1}.amazonaws.com\nwith the\nBase64-encoded secret\nas the value.\nAdd the secret name to the\nsecrets\nsection of the\nconfig.yaml\nto allow this model to access the secret when it is pushed.\nconfig.yaml\nCopy\nAsk AI\nsecrets\n:\nDOCKER_REGISTRY_{aws account id}.dkr.ecr.{us-east-1}.amazonaws.com\n:\nnull\n\u200b\nGoogle Cloud Artifact Registry\nGCP supports using either\naccess tokens\nfor short lived access or\nservice accounts\nfor container registry authentication.\n\u200b\nService Account\nGet your\nservice account key\nas a JSON key blob.\nAdd a new\nsecret\nto Baseten named\ngcp-service-account\n(or similar) with the JSON key blob as the value.\nAdd the secret name that you used to the\nsecrets\nsection of the\nconfig.yaml\nto allow this model to access the secret when it is pushed.\nconfig.yaml\nCopy\nAsk AI\nsecrets\n:\ngcp-service-account\n:\nnull\nConfigure the\ndocker_auth\nsection of your\nbase_image:\nto ensure that the service account authentication method will be used.\nCopy\nAsk AI\nbase_image:\n...\ndocker_auth:\nauth_method: GCP_SERVICE_ACCOUNT_JSON\nsecret_name: gcp-service-account\nregistry: {us-west2}-docker.pkg.dev\nNote that here,\nsecret_name\nshould match the name of the secret that is contains the JSON key blob.\n\u200b\nAccess Token\nGet your\naccess token\nAdd a new\nsecret\nto Baseten named\nDOCKER_REGISTRY_{us-west2}-docker.pkg.dev\nwith the\nBase64-encoded secret\nas the value.\nAdd the secret name to the\nsecrets\nsection of the\nconfig.yaml\nto allow this model to access the secret when it is pushed.\nconfig.yaml\nCopy\nAsk AI\nsecrets\n:\nDOCKER_REGISTRY_{us-west2}-docker.pkg.dev\n:\nnull\n\u200b\nDocker Hub\nGet the a\nBase64-encoded\nsecret:\nCopy\nAsk AI\necho\n-n\n'username:password'\n|\nbase64\nAdd a new\nsecret\nto Baseten named\nDOCKER_REGISTRY_https://index.docker.io/v1/\nwith the\nBase64-encoded secret\nas the value.\nAdd the secret name to the\nsecrets\nsection of the\nconfig.yaml\nto allow this model to access the secret when it is pushed.\nCopy\nAsk AI\nName: DOCKER_REGISTRY_https://index.docker.io/v1/\nToken: <Base64-encoded secret>\nThen, this to\nconfig.yaml\n:\nconfig.yaml\nCopy\nAsk AI\nsecrets\n:\nDOCKER_REGISTRY_https://index.docker.io/v1/\n:\nnull\nWas this page helpful?\nYes\nNo\nPrevious\nImplementation\nHow to implement your model.\nNext\nOn this page\nAWS Elastic Cloud Registry (ECR)\nAWS IAM Service accounts\nAccess Token\nGoogle Cloud Artifact Registry\nService Account\nAccess Token\nDocker Hub\nAssistant\nResponses are generated using AI and may contain mistakes.\n\n\nContent from https://docs.baseten.co/development/model/requests:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nOverview\nYour first model\nSetup and dependencies\nImplementation\nImplementation (Advanced)\nRequest handling\nCustom Responses\nCustom servers\nCustom health checks \ud83c\udd95\nCached weights \ud83c\udd95\nAccess model environments\nRequest concurrency\nStreaming output\nDeploy and iterate\nPerformance optimization\nSecurity and secrets\nData and storage\nPython driven configuration for models \ud83c\udd95\nDeveloping a Chain\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nExporting metrics\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nImplementation (Advanced)\nUsing request objects / cancellation\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nTruss processes client requests by extracting and validating payloads. For\nadvanced use cases\n, you can access the raw request object to:\nCustomize payload deserialization\n(e.g., binary protocol buffers).\nHandle disconnections & cancel long-running predictions.\nYou can mix request objects with standard inputs or use requests exclusively for performance optimization.", "mimetype": "text/plain", "start_char_idx": 235482, "end_char_idx": 239489, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f84eacb8-56f7-435e-a192-160e385b8f3a": {"__data__": {"id_": "f84eacb8-56f7-435e-a192-160e385b8f3a", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "776e2399-4d16-496a-ae37-6293812c0aa7", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "8ccb3eb3e90a6df55a9ada6d75b5755a5ace5d28d4e20908db1eb9fd3d33ca11", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "111b2ba6-d6f3-46eb-b2cd-fcb08e1173c3", "node_type": "1", "metadata": {}, "hash": "b7de93217690db5271947574c231bc042ae661a560729f5dc2ace70ceff3d132", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For\nadvanced use cases\n, you can access the raw request object to:\nCustomize payload deserialization\n(e.g., binary protocol buffers).\nHandle disconnections & cancel long-running predictions.\nYou can mix request objects with standard inputs or use requests exclusively for performance optimization.\n\u200b\nUsing Request Objects in Truss\nYou can define request objects in\npreprocess\n,\npredict\n, and\npostprocess\n:\nCopy\nAsk AI\nimport\nfastapi\nclass\nModel\n:\ndef\npreprocess\n(\nself\n,\nrequest\n: fastapi.Request):\n...\ndef\npredict\n(\nself\n,\ninputs\n,\nrequest\n: fastapi.Request):\n...\ndef\npostprocess\n(\nself\n,\ninputs\n,\nrequest\n: fastapi.Request):\n...\n\u200b\nRules for Using Requests\nThe request must be\ntype-annotated\nas\nfastapi.Request\n.\nIf\nonly\nrequests are used, Truss\nskips payload extraction\nfor better performance.\nIf\nboth\nrequest objects and standard inputs are used:\nRequest\nmust be the second argument\n.\nPreprocessing transforms inputs\n, but the request object remains unchanged.\npostprocess\ncannot use only the request\u2014it must receive the model\u2019s output.\nIf\npredict\nonly uses the request,\npreprocess\ncannot be used.\nCopy\nAsk AI\nimport\nfastapi, asyncio, logging\nclass\nModel\n:\nasync\ndef\npredict\n(\nself\n,\ninputs\n,\nrequest\n: fastapi.Request):\nawait\nasyncio.sleep(\n1\n)\nif\nawait\nrequest.is_disconnected():\nlogging.warning(\n\"Cancelled before generation.\"\n)\nreturn\n# Cancel request on the model engine here.\nfor\ni\nin\nrange\n(\n5\n):\nawait\nasyncio.sleep(\n1.0\n)\nlogging.warning(i)\nyield\nstr\n(i)\n# Streaming response\nif\nawait\nrequest.is_disconnected():\nlogging.warning(\n\"Cancelled during generation.\"\n)\nreturn\n# Cancel request on the model engine here.\nYou must implement request cancellation at the model level, which varies by framework.\n\u200b\nCancelling Requests in Specific Frameworks\n\u200b\nTRT-LLM (Polling-Based Cancellation)\nFor\nTensorRT-LLM\n, use\nresponse_iterator.cancel()\nto terminate streaming requests:\nCopy\nAsk AI\nasync\nfor\nrequest_output\nin\nresponse_iterator:\nif\nawait\nis_cancelled_fn():\nlogging.info(\n\"Request cancelled. Cancelling Triton request.\"\n)\nresponse_iterator.cancel()\nreturn\nSee full example in\nTensorRT-LLM Docs\n.\n\u200b\nvLLM (Abort API)\nFor\nvLLM\n, use\nengine.abort()\nto stop processing:\nCopy\nAsk AI\nasync\nfor\nrequest_output\nin\nresults_generator:\nif\nawait\nrequest.is_disconnected():\nawait\nengine.abort(request_id)\nreturn\nSee full example in\nvLLM Docs\n.\n\u200b\nUnsupported Request Features\nStreaming file uploads\n\u2013 Use URLs instead of embedding large data in the request.\nClient-side headers\n\u2013 Most headers are stripped; include necessary metadata in the payload.\nWas this page helpful?\nYes\nNo\nPrevious\nCustom Responses\nGet more control by directly creating the response object.\nNext\nOn this page\nUsing Request Objects in Truss\nRules for Using Requests\nCancelling Requests in Specific Frameworks\nTRT-LLM (Polling-Based Cancellation)\nvLLM (Abort API)\nUnsupported Request Features\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 239192, "end_char_idx": 242117, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "111b2ba6-d6f3-46eb-b2cd-fcb08e1173c3": {"__data__": {"id_": "111b2ba6-d6f3-46eb-b2cd-fcb08e1173c3", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f84eacb8-56f7-435e-a192-160e385b8f3a", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "6ff3842d10d49058e804137e54e7062b9e4a7c777473b4d98f3bfac044309ec8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0092c886-f6c2-40a9-814a-498b1bd8b1ff", "node_type": "1", "metadata": {}, "hash": "fcc9a465ab8e094738cc6a7db2e1a471d0d6510bd7e74196bfb39f54b64946d0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u200b\nvLLM (Abort API)\nFor\nvLLM\n, use\nengine.abort()\nto stop processing:\nCopy\nAsk AI\nasync\nfor\nrequest_output\nin\nresults_generator:\nif\nawait\nrequest.is_disconnected():\nawait\nengine.abort(request_id)\nreturn\nSee full example in\nvLLM Docs\n.\n\u200b\nUnsupported Request Features\nStreaming file uploads\n\u2013 Use URLs instead of embedding large data in the request.\nClient-side headers\n\u2013 Most headers are stripped; include necessary metadata in the payload.\nWas this page helpful?\nYes\nNo\nPrevious\nCustom Responses\nGet more control by directly creating the response object.\nNext\nOn this page\nUsing Request Objects in Truss\nRules for Using Requests\nCancelling Requests in Specific Frameworks\nTRT-LLM (Polling-Based Cancellation)\nvLLM (Abort API)\nUnsupported Request Features\nAssistant\nResponses are generated using AI and may contain mistakes.\n\n\nContent from https://docs.baseten.co/development/model/responses:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nOverview\nYour first model\nSetup and dependencies\nImplementation\nImplementation (Advanced)\nRequest handling\nCustom Responses\nCustom servers\nCustom health checks \ud83c\udd95\nCached weights \ud83c\udd95\nAccess model environments\nRequest concurrency\nStreaming output\nDeploy and iterate\nPerformance optimization\nSecurity and secrets\nData and storage\nPython driven configuration for models \ud83c\udd95\nDeveloping a Chain\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nExporting metrics\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nImplementation (Advanced)\nCustom Responses\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nBy default, Truss wraps prediction results into an HTTP response. For\nadvanced use cases\n, you can create response objects manually to:\nControl HTTP status codes.\nUse server-sent events (SSEs) for streaming responses.\nYou can return a response from predict or postprocess, but not both.\n\u200b\nReturning Custom Response Objects\nAny subclass of starlette.responses.Response is supported.\nCopy\nAsk AI\nimport\nfastapi\nclass\nModel\n:\ndef\npredict\n(\nself\n,\ninputs\n) -> fastapi.Response:\nreturn\nfastapi.Response(\n...\n)\nIf\npredict\nreturns a response,\npostprocess\ncannot be used.\n\u200b\nExample: Streaming with SSEs\nFor\nserver-sent events (SSEs)\n, use\nStreamingResponse\n:\nCopy\nAsk AI\nimport\ntime\nfrom\nstarlette.responses\nimport\nStreamingResponse\nclass\nModel\n:\ndef\npredict\n(\nself\n,\nmodel_input\n):\ndef\nevent_stream\n():\nwhile\nTrue\n:\ntime.sleep(\n1\n)\nyield\nf\n\"data: Server Time:\n{\ntime.strftime(\n'%Y-%m-\n%d\n%H:%M:%S'\n)\n}\n\\n\\n\n\"\nreturn\nStreamingResponse(event_stream(),\nmedia_type\n=\n\"text/event-stream\"\n)\n\u200b\nLimitations\nResponse headers are not fully propagated\n\u2013 include metadata in the response body.\nAlso see\nUsing Request Objects\nfor handling raw requests.\nWas this page helpful?\nYes\nNo\nPrevious\nCustom servers\nA config.yaml is all you need\nNext\nOn this page\nReturning Custom Response Objects\nExample: Streaming with SSEs\nLimitations\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 241295, "end_char_idx": 244698, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0092c886-f6c2-40a9-814a-498b1bd8b1ff": {"__data__": {"id_": "0092c886-f6c2-40a9-814a-498b1bd8b1ff", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "111b2ba6-d6f3-46eb-b2cd-fcb08e1173c3", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "6d17a4a0c6d861257fca40f941da2d7eef0660d0c7ab698bee244a9be9e14eb0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "92e92808-bffe-463a-81d9-ff4ae347fed9", "node_type": "1", "metadata": {}, "hash": "f6d6fb7bd637e6dd2223a1ea3613171da5063df598ba5460913f98c027c3efc2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Content from https://docs.baseten.co/development/model/secrets:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nOverview\nYour first model\nSetup and dependencies\nImplementation\nImplementation (Advanced)\nDeploy and iterate\nPerformance optimization\nSecurity and secrets\nData and storage\nPython driven configuration for models \ud83c\udd95\nDeveloping a Chain\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nExporting metrics\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nDeveloping a model\nSecurity and secrets\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nTruss allows you to securely manage\nAPI keys\n,\naccess tokens\n,\npasswords\n,\nand other secrets\nwithout exposing them in code.\n\u200b\n1. Define Secrets in\nconfig.yaml\nAdd secrets with\nplaceholder\nvalues in\nconfig.yaml\n:\nCopy\nAsk AI\nsecrets\n:\nhf_access_token\n:\nnull\nNever store actual secret values in\nconfig.yaml\n. Store secrets in the\nworkspace settings\n.\n\u200b\n2. Access Secrets in\nmodel.py\nSecrets are passed as\nkeyword arguments\nto the\nModel\nclass:\nCopy\nAsk AI\ndef\n__init__\n(\nself\n,\n**\nkwargs\n):\nself\n._secrets\n=\nkwargs[\n\"secrets\"\n]\nUse secrets inside load or predict:\nCopy\nAsk AI\ndef\nload\n(\nself\n):\nself\n._model\n=\npipeline(\n\"fill-mask\"\n,\nmodel\n=\n\"baseten/docs-example-gated-model\"\n,\nuse_auth_token\n=\nself\n._secrets[\n\"hf_access_token\"\n]\n)\n\u200b\n3. Store Secrets on Your Remote\nOn\nBaseten\n, add secrets in the\nworkspace settings\n.\nUse the\nexact name\nfrom\nconfig.yaml\n(case-sensitive).\n\u200b\n4. Deploying with Secrets\nBy default, models have access to any secrets on a workspace.\nWas this page helpful?\nYes\nNo\nPrevious\nData and storage\nLoad model weights without Hugging Face or S3\nNext\nOn this page\n1. Define Secrets in config.yaml\n2. Access Secrets in model.py\n3. Store Secrets on Your Remote\n4. Deploying with Secrets\nAssistant\nResponses are generated using AI and may contain mistakes.\n\n\nContent from https://docs.baseten.co/development/model/streaming:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nOverview\nYour first model\nSetup and dependencies\nImplementation\nImplementation (Advanced)\nRequest handling\nCustom Responses\nCustom servers\nCustom health checks \ud83c\udd95\nCached weights \ud83c\udd95\nAccess model environments\nRequest concurrency\nStreaming output\nDeploy and iterate\nPerformance optimization\nSecurity and secrets\nData and storage\nPython driven configuration for models \ud83c\udd95\nDeveloping a Chain\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nExporting metrics\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nImplementation (Advanced)\nStreaming output\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nStreaming output significantly reduces wait time for generative AI models by returning results as they are generated instead of waiting for the full response.\n\u200b\nWhy Streaming?\n\u2705\nFaster response time\n\u2013 Get initial results in under\n1 second\ninstead of waiting\n10+ seconds\n.\n\u2705\nImproved user experience\n\u2013 Partial outputs are\nimmediately usable\n.\nThis guide walks through\ndeploying Falcon 7B\nwith streaming enabled.\n\u200b\n1.", "mimetype": "text/plain", "start_char_idx": 244701, "end_char_idx": 248633, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "92e92808-bffe-463a-81d9-ff4ae347fed9": {"__data__": {"id_": "92e92808-bffe-463a-81d9-ff4ae347fed9", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0092c886-f6c2-40a9-814a-498b1bd8b1ff", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ac5a3cd10ea6599fd1a03374cc077cf73ee083adc3ee63936dc84d3f6d557298", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1da0ed9f-881e-4323-9ed3-c2fb9a1ff2e1", "node_type": "1", "metadata": {}, "hash": "e6704afeaa28e80ba41af8a9c17a3930e9c279ede2103f6b62d810beb6480fd0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u200b\nWhy Streaming?\n\u2705\nFaster response time\n\u2013 Get initial results in under\n1 second\ninstead of waiting\n10+ seconds\n.\n\u2705\nImproved user experience\n\u2013 Partial outputs are\nimmediately usable\n.\nThis guide walks through\ndeploying Falcon 7B\nwith streaming enabled.\n\u200b\n1. Initialize Truss\nCopy\nAsk AI\ntruss\ninit\nfalcon-7b\n&&\ncd\nfalcon-7b\n\u200b\n2: Implement Model (Non-Streaming)\nThis first version loads the Falcon 7B model\nwithout\nstreaming:\nmodel/model.py\nCopy\nAsk AI\nimport\ntorch\nfrom\ntransformers\nimport\nAutoTokenizer, AutoModelForCausalLM, GenerationConfig\nfrom\ntyping\nimport\nDict\nCHECKPOINT\n=\n\"tiiuae/falcon-7b-instruct\"\nDEFAULT_MAX_NEW_TOKENS\n=\n150\nDEFAULT_TOP_P\n=\n0.95\nclass\nModel\n:\ndef\n__init__\n(\nself\n,\n**\nkwargs\n) ->\nNone\n:\nself\n.tokenizer\n=\nNone\nself\n.model\n=\nNone\ndef\nload\n(\nself\n):\nself\n.tokenizer\n=\nAutoTokenizer.from_pretrained(\nCHECKPOINT\n)\nself\n.model\n=\nAutoModelForCausalLM.from_pretrained(\nCHECKPOINT\n,\ntorch_dtype\n=\ntorch.bfloat16,\ntrust_remote_code\n=\nTrue\n,\ndevice_map\n=\n\"auto\"\n)\ndef\npredict\n(\nself\n,\nrequest\n: Dict) -> Dict:\nprompt\n=\nrequest[\n\"prompt\"\n]\ninputs\n=\nself\n.tokenizer(prompt,\nreturn_tensors\n=\n\"pt\"\n,\nmax_length\n=\n512\n,\ntruncation\n=\nTrue\n,\npadding\n=\nTrue\n)\ninput_ids\n=\ninputs[\n\"input_ids\"\n].to(\n\"cuda\"\n)\ngeneration_config\n=\nGenerationConfig(\ntemperature\n=\n1\n,\ntop_p\n=\nDEFAULT_TOP_P\n,\ntop_k\n=\n40\n)\nwith\ntorch.no_grad():\nreturn\nself\n.model.generate(\ninput_ids\n=\ninput_ids,\ngeneration_config\n=\ngeneration_config,\nreturn_dict_in_generate\n=\nTrue\n,\noutput_scores\n=\nTrue\n,\npad_token_id\n=\nself\n.tokenizer.eos_token_id,\nmax_new_tokens\n=\nDEFAULT_MAX_NEW_TOKENS\n,\n)\n\u200b\n3. Add Streaming Support\nTo enable streaming, we:\nUse\nTextIteratorStreamer\nto stream tokens as they are generated.\nRun\ngenerate()\nin a\nseparate thread\nto prevent blocking.\nReturn a\ngenerator\nthat streams results.\nmodel/model.py\nCopy\nAsk AI\nimport\ntorch\nfrom\ntransformers\nimport\nAutoTokenizer, AutoModelForCausalLM, GenerationConfig, TextIteratorStreamer\nfrom\nthreading\nimport\nThread\nfrom\ntyping\nimport\nDict\nCHECKPOINT\n=\n\"tiiuae/falcon-7b-instruct\"\nclass\nModel\n:\ndef\n__init__\n(\nself\n,\n**\nkwargs\n) ->\nNone\n:\nself\n.tokenizer\n=\nNone\nself\n.model\n=\nNone\ndef\nload\n(\nself\n):\nself\n.tokenizer\n=\nAutoTokenizer.from_pretrained(\nCHECKPOINT\n)\nself\n.model\n=\nAutoModelForCausalLM.from_pretrained(\nCHECKPOINT\n,\ntorch_dtype\n=\ntorch.bfloat16,\ntrust_remote_code\n=\nTrue\n,\ndevice_map\n=\n\"auto\"\n)\ndef\npredict\n(\nself\n,\nrequest\n: Dict):\nprompt\n=\nrequest[\n\"prompt\"\n]\ninputs\n=\nself\n.tokenizer(prompt,\nreturn_tensors\n=\n\"pt\"\n,\nmax_length\n=\n512\n,\ntruncation\n=\nTrue\n,\npadding\n=\nTrue\n)\ninput_ids\n=\ninputs[\n\"input_ids\"\n].to(\n\"cuda\"\n)\nstreamer\n=\nTextIteratorStreamer(\nself\n.tokenizer)\ngeneration_config\n=\nGenerationConfig(\ntemperature\n=\n1\n,\ntop_p\n=\n0.95\n,\ntop_k\n=\n40\n)\ndef\ngenerate\n():\nself\n.model.generate(\ninput_ids\n=\ninput_ids,\ngeneration_config\n=\ngeneration_config,\nreturn_dict_in_generate\n=\nTrue\n,\noutput_scores\n=\nTrue\n,\npad_token_id\n=\nself\n.tokenizer.eos_token_id,\nmax_new_tokens\n=\n150\n,\nstreamer\n=\nstreamer,\n)\nthread\n=\nThread(\ntarget\n=\ngenerate)\nthread.start()\ndef\nstream_output\n():\nfor\ntext\nin\nstreamer:\nyield\ntext\nthread.join()\nreturn\nstream_output()\n\u200b\n4.", "mimetype": "text/plain", "start_char_idx": 248377, "end_char_idx": 251477, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1da0ed9f-881e-4323-9ed3-c2fb9a1ff2e1": {"__data__": {"id_": "1da0ed9f-881e-4323-9ed3-c2fb9a1ff2e1", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "92e92808-bffe-463a-81d9-ff4ae347fed9", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "afba58b9d6951b4faf49df55ede55d8564a9e51de3d10b83f394ccc63d646b7e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "01a9cec3-ae8b-45e3-86df-2de432dae135", "node_type": "1", "metadata": {}, "hash": "92e121269ab1df38c949cc02ec649e4a1e330866bde047c3c87157bf87f4f7ae", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Configure\nconfig.yaml\nconfig.yaml\nCopy\nAsk AI\nmodel_name\n:\nfalcon-streaming\nrequirements\n:\n-\ntorch==2.0.1\n-\npeft==0.4.0\n-\nscipy==1.11.1\n-\nsentencepiece==0.1.99\n-\naccelerate==0.21.0\n-\nbitsandbytes==0.41.1\n-\neinops==0.6.1\n-\ntransformers==4.31.0\nresources\n:\ncpu\n:\n\"3\"\nmemory\n:\n14Gi\nuse_gpu\n:\ntrue\naccelerator\n:\nA10G\n\u200b\n5. Deploy & Invoke\nDeploy the model:\nCopy\nAsk AI\ntruss\npush\nInvoke with:\nCopy\nAsk AI\ntruss\npredict\n-d\n'{\"prompt\": \"Tell me about falcons\", \"do_sample\": true}'\nWas this page helpful?\nYes\nNo\nPrevious\nDeploy and iterate\nDeploy your model and quickly iterate on it.\nNext\nOn this page\nWhy Streaming?\n1. Initialize Truss\n2: Implement Model (Non-Streaming)\n3. Add Streaming Support\n4. Configure config.yaml\n5. Deploy & Invoke\nAssistant\nResponses are generated using AI and may contain mistakes.\n\n\nContent from https://docs.baseten.co/examples/bei:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nExamples\nOverview\nDeploy your first model\nFast LLMs with TensorRT-LLM\nRun any LLM with vLLM\nDeploy LLMs with SGLang\nRAG pipeline with Chains\nTranscribe audio with Chains\nImage generation\nDeploy a ComfyUI project\nEmbeddings with BEI\nDockerized model\nLLM with Streaming\nText to speech\nModel library\nOverview\nDeepseek\nLlama\nQwen\nGemma\nStable Diffusion\nFlux\nKokoro\nMicrosoft\nNomic\nWhisper\nMars\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nExamples\nEmbeddings with BEI\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nBaseten Embeddings Inference is Baseten\u2019s solution for production grade inference on embedding, classification and reranking models using TensorRT-LLM.\nWith Baseten Embeddings Inference you get the following benefits:\nLowest-latency inference across any embedding solution (vLLM, SGlang, Infinity, TEI, Ollama)\n1\nHighest-throughput inference across any embedding solution (vLLM, SGlang, Infinity, TEI, Ollama) - thanks to XQA kernels, FP8 and dynamic batching.\n2\nHigh parallelism: up to 1400 client embeddings per second\nCached model weights for fast vertical scaling and high availability - no Hugging Face hub dependency at runtime\nAhead-of-time compilation, memory allocation and fp8 post-training quantization\n\u200b\nGetting started with embedding models:\nEmbedding models are LLMs without a lm_head for language generation.\nTypical architectures that are supported for embeddings are\nLlamaModel\n,\nBertModel\n,\nRobertaModel\nor\nGemma2Model\n, and contain the safetensors, config, tokenizer and sentence-transformer config files.\nA good example is the repo\nBAAI/bge-multilingual-gemma2\n.\nTo deploy a model for embeddings, set the following config in your local directory.\nconfig.yaml\nCopy\nAsk AI\nmodel_name\n:\nBEI-Linq-Embed-Mistral\nresources\n:\naccelerator\n:\nH100_40GB\nuse_gpu\n:\ntrue\ntrt_llm\n:\nbuild\n:\nbase_model\n:\nencoder\ncheckpoint_repository\n:\n# for a different model, change the repo to e.g. to \"Salesforce/SFR-Embedding-Mistral\"\n# \"BAAI/bge-en-icl\" or \"BAAI/bge-m3\"\nrepo\n:\n\"Linq-AI-Research/Linq-Embed-Mistral\"\nrevision\n:\nmain\nsource\n:\nHF\n# only Llama, Mistral and Qwen Models support quantization.\n# others, use: \"quantization_type: no_quant\"\nquantization_type\n:\nfp8\nWith\nconfig.yaml\nin your local directory, you can deploy the model to Baseten.\nCopy\nAsk AI\ntruss\npush\n--publish\n--promote\nDeployed embedding models are OpenAI compatible without any additional settings.\nYou may use the client code below to consume the model.", "mimetype": "text/plain", "start_char_idx": 251478, "end_char_idx": 254877, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "01a9cec3-ae8b-45e3-86df-2de432dae135": {"__data__": {"id_": "01a9cec3-ae8b-45e3-86df-2de432dae135", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1da0ed9f-881e-4323-9ed3-c2fb9a1ff2e1", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "8685f8378f0c77773fc3f433fd0b43b3e8968b5b38d5f530022d3b937f2efc4a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "34fc1ecc-efc6-42ef-b75c-306da43e9a43", "node_type": "1", "metadata": {}, "hash": "63b2c500553704903ceb8e1c319548af8d9e346f83480646c4ebfb165b14f924", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "to \"Salesforce/SFR-Embedding-Mistral\"\n# \"BAAI/bge-en-icl\" or \"BAAI/bge-m3\"\nrepo\n:\n\"Linq-AI-Research/Linq-Embed-Mistral\"\nrevision\n:\nmain\nsource\n:\nHF\n# only Llama, Mistral and Qwen Models support quantization.\n# others, use: \"quantization_type: no_quant\"\nquantization_type\n:\nfp8\nWith\nconfig.yaml\nin your local directory, you can deploy the model to Baseten.\nCopy\nAsk AI\ntruss\npush\n--publish\n--promote\nDeployed embedding models are OpenAI compatible without any additional settings.\nYou may use the client code below to consume the model.\nCopy\nAsk AI\nfrom\nopenai\nimport\nOpenAI\nimport\nos\nclient\n=\nOpenAI(\napi_key\n=\nos.environ[\n'BASETEN_API_KEY'\n],\n# add the deployment URL\nbase_url\n=\n\"https://model-xxxxxx.api.baseten.co/environments/production/sync/v1\"\n)\nembedding\n=\nclient.embeddings.create(\ninput\n=\n[\n\"Baseten Embeddings are fast.\"\n,\n\"Embed this sentence!\"\n],\nmodel\n=\n\"not-required\"\n)\n\u200b\nExample deployment of a classification, reranking and classification models\nBesides embedding models, BEI deploys high-throughput rerank and classification models.\nYou can identify suitable architectures by their\nForSequenceClassification\nsuffix in the huggingface repo.\nThe use-case for these models is either Reward Modeling, Reranking documents in RAG or tasks like content moderation.\nCopy\nAsk AI\nmodel_name\n:\nBEI-mixedbread-rerank-large-v2-fp8\nresources\n:\naccelerator\n:\nH100_40GB\ncpu\n:\n'1'\nmemory\n:\n10Gi\nuse_gpu\n:\ntrue\ntrt_llm\n:\nbuild\n:\nbase_model\n:\nencoder\ncheckpoint_repository\n:\nrepo\n:\nmichaelfeil/mxbai-rerank-large-v2-seq\nrevision\n:\nmain\nsource\n:\nHF\n# only Llama, Mistral and Qwen Models support quantization\nquantization_type\n:\nfp8\nAs OpenAI does not offer reranking or classification, we are sending a simple request to the endpoint.\nDepending on the model, you might want to apply a specific prompt template first.\nCopy\nAsk AI\nimport\nrequests\nimport\nos\nheaders\n=\n{\nf\n\"Authorization\"\n:\nf\n\"Api-Key\n{\nos.environ[\n'BASETEN_API_KEY'\n]\n}\n\"\n}\n# model specific prompt for mixedbread's reranker v2.\nprompt\n=\n(\n\"<|endoftext|><|im_start|>system\n\\n\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.\n\\n\n<|im_end|>\n\\n\n<|im_start|>user\n\\n\n\"\n\"query:\n{query}\n\\n\ndocument:\n{doc}\n\\n\nYou are a search relevance expert who evaluates how well documents match search queries. For each query-document pair, carefully analyze the semantic relationship between them, then provide your binary relevance judgment (0 for not relevant, 1 for relevant).\n\\n\nRelevance:<|im_end|>\n\\n\n<|im_start|>assistant\n\\n\n\"\n).format(\nquery\n=\n\"What is Baseten?\"\n,\ndoc\n=\n\"Baseten is a fast inference provider.\"\n)\nrequests.post(\nheaders\n=\nheaders,\nurl\n=\n\"https://model-xxxxxx.api.baseten.co/environments/production/sync/predict\"\n,\njson\n=\n{\n\"inputs\"\n: prompt,\n\"raw_scores\"\n:\nTrue\n,\n}\n)\n\u200b\nBenchmarks and Performance optimizations\nEmbedding models on BEI are fast, and offer currently the fastest implementation for embeddings across all open-source and closed-source providers.\nThe team behind the implementation are the authors of\ninfinity\n.\nWe recommend using fp8 quantization for LLama, Mistral and Qwen2 models on L4 or newer (L4, H100, H200 and B200).\nQuality difference between fp8 and bfloat16 is often negligible - embedding models often retentain of >99% cosine simalarity between both presisions,\nand reranking models retain the ranking order - despite a difference in the retained output.\nFor more details, check out the\ntechnical launch post\n.\nThe team at Baseten has additional options for sharing cached model weights across replicas - for very fast horizontal scaling.\nPlease contact us to enable this option.\n\u200b\nDeploy custom or fine-tuned models on BEI:\nWe support the deployment of of the below models, as well all finetuned variants of these models (same architecture & customized weights).", "mimetype": "text/plain", "start_char_idx": 254342, "end_char_idx": 258116, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "34fc1ecc-efc6-42ef-b75c-306da43e9a43": {"__data__": {"id_": "34fc1ecc-efc6-42ef-b75c-306da43e9a43", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "01a9cec3-ae8b-45e3-86df-2de432dae135", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "557217cb679be6f95a978099694f2cbabc1d5d27df64588e5610a9bf4720c4ce", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f0d36da2-9328-47af-9b4a-4b9f02ae76ec", "node_type": "1", "metadata": {}, "hash": "d6b999eea2e735efefbdd052a273f58225f988d9e8aefa0e289a6418964e712a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The team behind the implementation are the authors of\ninfinity\n.\nWe recommend using fp8 quantization for LLama, Mistral and Qwen2 models on L4 or newer (L4, H100, H200 and B200).\nQuality difference between fp8 and bfloat16 is often negligible - embedding models often retentain of >99% cosine simalarity between both presisions,\nand reranking models retain the ranking order - despite a difference in the retained output.\nFor more details, check out the\ntechnical launch post\n.\nThe team at Baseten has additional options for sharing cached model weights across replicas - for very fast horizontal scaling.\nPlease contact us to enable this option.\n\u200b\nDeploy custom or fine-tuned models on BEI:\nWe support the deployment of of the below models, as well all finetuned variants of these models (same architecture & customized weights).\nThe following repositories are supported - this list is not exhaustive.\nModel Repository\nArchitecture\nFunction\nSalesforce/SFR-Embedding-Mistral\nMistralModel\nembedding\nBAAI/bge-m3\nBertModel\nembedding\nBAAI/bge-multilingual-gemma2\nGemma2Model\nembedding\nmixedbread-ai/mxbai-embed-large-v1\nBertModel\nembedding\nBAAI/bge-large-en-v1.5\nBertModel\nembedding\nallenai/Llama-3.1-Tulu-3-8B-RM\nLlamaForSequenceClassification\nclassifier\nncbi/MedCPT-Cross-Encoder\nBertForSequenceClassification\nreranker/classifier\nSamLowe/roberta-base-go_emotions\nXLMRobertaForSequenceClassification\nclassifier\nmixedbread/mxbai-rerank-large-v2-seq\nQwen2ForSequenceClassification\nreranker/classifier\nBAAI/bge-en-icl\nLlamaModel\nembedding\nBAAI/bge-reranker-v2-m3\nBertForSequenceClassification\nreranker/classifier\nSkywork/Skywork-Reward-Llama-3.1-8B-v0.2\nLlamaForSequenceClassification\nclassifier\nSnowflake/snowflake-arctic-embed-l\nBertModel\nembedding\nnomic-ai/nomic-embed-code\nQwen2Model\nembedding\n1\nmeasured on H100-HBM3 (bert-large-335M, for BAAI/bge-en-icl: 9ms)\n2\nmeasured on H100-HBM3 (leading model architecture on MTEB, MistralModel-7B)\nWas this page helpful?\nYes\nNo\nPrevious\nDockerized model\nDeploy any model in a pre-built Docker container\nNext\nOn this page\nGetting started with embedding models:\nExample deployment of a classification, reranking and classification models\nBenchmarks and Performance optimizations\nDeploy custom or fine-tuned models on BEI:\nAssistant\nResponses are generated using AI and may contain mistakes.\n\n\nContent from https://docs.baseten.co/examples/chains-audio-transcription:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nExamples\nOverview\nDeploy your first model\nFast LLMs with TensorRT-LLM\nRun any LLM with vLLM\nDeploy LLMs with SGLang\nRAG pipeline with Chains\nTranscribe audio with Chains\nImage generation\nDeploy a ComfyUI project\nEmbeddings with BEI\nDockerized model\nLLM with Streaming\nText to speech\nModel library\nOverview\nDeepseek\nLlama\nQwen\nGemma\nStable Diffusion\nFlux\nKokoro\nMicrosoft\nNomic\nWhisper\nMars\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nExamples\nTranscribe audio with Chains\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nView example on GitHub\nThis guide walks through building an audio transcription pipeline using Chains. You\u2019ll break down large media files, distribute transcription tasks across autoscaling deployments, and leverage high-performance GPUs for rapid inference.\n\u200b\n1. Overview\nThis Chain enables fast, high-quality transcription by:\nPartitioning\nlong files (10+ hours) into smaller segments.\nDetecting silence\nto optimize split points.\nParallelizing inference\nacross multiple GPU-backed deployments.\nBatching requests\nto maximize throughput.\nUsing range downloads\nfor efficient data streaming.\nLeveraging\nasyncio\nfor concurrent execution.\n\u200b\n2. Chain Structure\nTranscription is divided into two processing layers:\nMacro chunks:\nLarge segments (~300s) split from the source media file. These are processed in parallel to handle massive files efficiently.", "mimetype": "text/plain", "start_char_idx": 257286, "end_char_idx": 261166, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f0d36da2-9328-47af-9b4a-4b9f02ae76ec": {"__data__": {"id_": "f0d36da2-9328-47af-9b4a-4b9f02ae76ec", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "34fc1ecc-efc6-42ef-b75c-306da43e9a43", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "da91f0b4b910bb9e087a96cf06ca5a6078b2c3f26cc1a2511905b93eee9a346e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "129c499f-6718-4409-a666-0dc9c1678513", "node_type": "1", "metadata": {}, "hash": "eef3a1834a9c6bd3421a3e6c5604a880ea5af497ba2b9585c9b67f53635799c6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "You\u2019ll break down large media files, distribute transcription tasks across autoscaling deployments, and leverage high-performance GPUs for rapid inference.\n\u200b\n1. Overview\nThis Chain enables fast, high-quality transcription by:\nPartitioning\nlong files (10+ hours) into smaller segments.\nDetecting silence\nto optimize split points.\nParallelizing inference\nacross multiple GPU-backed deployments.\nBatching requests\nto maximize throughput.\nUsing range downloads\nfor efficient data streaming.\nLeveraging\nasyncio\nfor concurrent execution.\n\u200b\n2. Chain Structure\nTranscription is divided into two processing layers:\nMacro chunks:\nLarge segments (~300s) split from the source media file. These are processed in parallel to handle massive files efficiently.\nMicro chunks:\nSmaller segments (~5\u201330s) extracted from macro chunks and sent to the Whisper model for transcription.\n\u200b\n3. Implementing the Chainlets\n\u200b\nTranscribe\n(Entrypoint Chainlet)\nHandles transcription requests and dispatches tasks to worker Chainlets.\nFunction signature:\nCopy\nAsk AI\nasync\ndef\nrun_remote\n(\nself\n,\nmedia_url\n:\nstr\n,\nparams\n: data_types.TranscribeParams\n) -> data_types.TranscribeOutput:\nSteps:\nValidates that the media source supports\nrange downloads\n.\nUses\nFFmpeg\nto extract metadata and duration.\nSplits the file into\nmacro chunks\n, optimizing split points at silent sections.\nDispatches\nmacro chunk tasks\nto the MacroChunkWorker for processing.\nCollects\nmicro chunk transcriptions\n, merges results, and returns the final text.\nExample request:\nCopy\nAsk AI\ncurl\n-X\nPOST\n$INVOCATION_URL\n\\\n-H\n\"Authorization: Api-Key\n$BASETEN_API_KEY\n\"\n\\\n-d\n'<JSON_INPUT>'\nCopy\nAsk AI\n{\n\"media_url\"\n:\n\"http://commondatastorage.googleapis.com/gtv-videos-bucket/sample/TearsOfSteel.mp4\"\n,\n\"params\"\n: {\n\"micro_chunk_size_sec\"\n:\n30\n,\n\"macro_chunk_size_sec\"\n:\n300\n}\n}\n\u200b\nMacroChunkWorker\n(Processing Chainlet)\nProcesses\nmacro chunks\nby:\nExtracting\nrelevant time segments using\nFFmpeg\n.\nStreaming audio\ninstead of downloading full files for low latency.\nSplitting segments\nat silent points.\nEncoding\naudio in base64 for efficient transfer.\nDistributing micro chunks\nto the Whisper model for transcription.\nThis Chainlet\nruns in parallel\nwith multiple instances autoscaled dynamically.\n\u200b\nWhisperModel\n(Inference Model)\nA separately deployed\nWhisper\nmodel Chainlet handles speech-to-text transcription.\nDeployed\nindependently\nto allow fast iteration on business logic without redeploying the model.\nUsed\nacross different Chains\nor accessed directly as a standalone model.\nSupports\nmultiple environments\n(e.g., dev, prod) using the same instance.\nWhisper can also be deployed as a\nstandard Truss model\n, separate from the Chain.\n\u200b\n4. Optimizing Performance\nEven for very large files,\nprocessing time remains bounded\nby parallel execution.\n\u200b\nKey performance tuning parameters:\nmicro_chunk_size_sec\n\u2192 Balance GPU utilization and inference latency.\nmacro_chunk_size_sec\n\u2192 Adjust chunk size for optimal parallelism.\nAutoscaling settings\n\u2192 Tune concurrency and replica counts for load balancing.\nExample speedup:\nCopy\nAsk AI\n{\n\"input_duration_sec\"\n:\n734.26\n,\n\"processing_duration_sec\"\n:\n82.42\n,\n\"speedup\"\n:\n8.9\n}\n\u200b\n5. Deploy & Run the Chain\n\u200b\nDeploy WhisperModel first:\nCopy\nAsk AI\ntruss\nchains\npush\nwhisper_chainlet.py\nCopy the\ninvocation URL\nand update\nWHISPER_URL\nin\ntranscribe.py\n.\n\u200b\nDeploy the transcription Chain:\nCopy\nAsk AI\ntruss\nchains\npush\ntranscribe.py\n\u200b\nRun transcription on a sample file:\nCopy\nAsk AI\ncurl\n-X\nPOST\n$INVOCATION_URL\n\\\n-H\n\"Authorization: Api-Key\n$BASETEN_API_KEY\n\"\n\\\n-d\n'<JSON_INPUT>'\n\u200b\nNext Steps\nLearn more about\nChains\n.\nOptimize GPU\nautoscaling\nfor peak efficiency.\nExtend the pipeline with\ncustom business logic\n.\nWas this page helpful?\nYes\nNo\nPrevious\nImage generation\nBuilding a text-to-image model with Flux Schnell\nNext\nOn this page\n1. Overview\n2. Chain Structure\n3. Implementing the Chainlets\nTranscribe (Entrypoint Chainlet)\nMacroChunkWorker (Processing Chainlet)\nWhisperModel (Inference Model)\n4. Optimizing Performance\nKey performance tuning parameters:\n5.", "mimetype": "text/plain", "start_char_idx": 260421, "end_char_idx": 264451, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "129c499f-6718-4409-a666-0dc9c1678513": {"__data__": {"id_": "129c499f-6718-4409-a666-0dc9c1678513", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f0d36da2-9328-47af-9b4a-4b9f02ae76ec", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "e7afac6374c4a9ee5f8f6eac490f479e6f95a809461d3df543c86710980582f9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "70b981ce-d788-42eb-a096-77f20488dc39", "node_type": "1", "metadata": {}, "hash": "7adf1f9ff826bc78ef406ceb735e8fdcb0da4139bfca869591e36c786b2f564b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u200b\nDeploy the transcription Chain:\nCopy\nAsk AI\ntruss\nchains\npush\ntranscribe.py\n\u200b\nRun transcription on a sample file:\nCopy\nAsk AI\ncurl\n-X\nPOST\n$INVOCATION_URL\n\\\n-H\n\"Authorization: Api-Key\n$BASETEN_API_KEY\n\"\n\\\n-d\n'<JSON_INPUT>'\n\u200b\nNext Steps\nLearn more about\nChains\n.\nOptimize GPU\nautoscaling\nfor peak efficiency.\nExtend the pipeline with\ncustom business logic\n.\nWas this page helpful?\nYes\nNo\nPrevious\nImage generation\nBuilding a text-to-image model with Flux Schnell\nNext\nOn this page\n1. Overview\n2. Chain Structure\n3. Implementing the Chainlets\nTranscribe (Entrypoint Chainlet)\nMacroChunkWorker (Processing Chainlet)\nWhisperModel (Inference Model)\n4. Optimizing Performance\nKey performance tuning parameters:\n5. Deploy & Run the Chain\nDeploy WhisperModel first:\nDeploy the transcription Chain:\nRun transcription on a sample file:\nNext Steps\nAssistant\nResponses are generated using AI and may contain mistakes.\n\n\nContent from https://docs.baseten.co/examples/chains-build-rag:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nExamples\nOverview\nDeploy your first model\nFast LLMs with TensorRT-LLM\nRun any LLM with vLLM\nDeploy LLMs with SGLang\nRAG pipeline with Chains\nTranscribe audio with Chains\nImage generation\nDeploy a ComfyUI project\nEmbeddings with BEI\nDockerized model\nLLM with Streaming\nText to speech\nModel library\nOverview\nDeepseek\nLlama\nQwen\nGemma\nStable Diffusion\nFlux\nKokoro\nMicrosoft\nNomic\nWhisper\nMars\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nExamples\nRAG pipeline with Chains\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nLearn more about Chains\n\u200b\nPrerequisites\nTo use Chains, install a recent Truss version and ensure pydantic is v2:\nCopy\nAsk AI\npip\ninstall\n--upgrade\ntruss\n'pydantic>=2.0.0'\nHelp for setting up a clean development environment\nTruss requires python\n>=3.8,<3.13\n. To set up a fresh development environment,\nyou can use the following commands, creating a environment named\nchains_env\nusing\npyenv\n:\nCopy\nAsk AI\ncurl\nhttps://pyenv.run\n|\nbash\necho\n'export PYENV_ROOT=\"$HOME/.pyenv\"'\n>>\n~/.bashrc\necho\n'[[ -d $PYENV_ROOT/bin ]] && export PATH=\"$PYENV_ROOT/bin:$PATH\"'\n>>\n~/.bashrc\necho\n'eval \"$(pyenv init -)\"'\n>>\n~/.bashrc\nsource\n~/.bashrc\npyenv\ninstall\n3.11.0\nENV_NAME\n=\n\"chains_env\"\npyenv\nvirtualenv\n3.11.0\n$ENV_NAME\npyenv\nactivate\n$ENV_NAME\npip\ninstall\n--upgrade\ntruss\n'pydantic>=2.0.0'\nTo deploy Chains remotely, you also need a\nBaseten account\n.\nIt is handy to export your API key to the current shell session or permanently in your\n.bashrc\n:\n~/.bashrc\nCopy\nAsk AI\nexport\nBASETEN_API_KEY\n=\n\"nPh8...\"\nIf you want to run this example in\nlocal debugging mode\n, you\u2019ll also need to\ninstall chromadb:\nCopy\nAsk AI\npip\ninstall\nchromadb\nThe complete code used in this tutorial can also be found in the\nChains examples repo\n.\n\u200b\nOverview\nRetrieval-augmented generation (RAG) is a multi-model pipeline for generating\ncontext-aware answers from LLMs.\nThere are a number of ways to build a RAG system. This tutorial shows a minimum\nviable implementation with a basic vector store and retrieval function. It\u2019s\nintended as a starting point to show how Chains helps you flexibly combine model\ninference and business logic.\nIn this tutorial, we\u2019ll build a simple RAG pipeline for a hypothetical alumni\nmatching service for a university. The system:\nTakes a bio with information about a new graduate\nUses a vector database to retrieve semantically similar bios of other alums\nUses an LLM to explain why the new graduate should meet the selected alums\nReturns the writeup from the LLM\nLet\u2019s dive in!", "mimetype": "text/plain", "start_char_idx": 263742, "end_char_idx": 267315, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "70b981ce-d788-42eb-a096-77f20488dc39": {"__data__": {"id_": "70b981ce-d788-42eb-a096-77f20488dc39", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "129c499f-6718-4409-a666-0dc9c1678513", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "6cf8ee966535f35c4a2f1942697af80e29505bf87a49bf8c0749025c6edf3366", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "48c996ef-173a-48d0-8f63-5c1b94cc568a", "node_type": "1", "metadata": {}, "hash": "054e62f1309c681ebbcb7e066b914bdd989cdf73ddfe20ac41e15f30358d7ef7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u200b\nOverview\nRetrieval-augmented generation (RAG) is a multi-model pipeline for generating\ncontext-aware answers from LLMs.\nThere are a number of ways to build a RAG system. This tutorial shows a minimum\nviable implementation with a basic vector store and retrieval function. It\u2019s\nintended as a starting point to show how Chains helps you flexibly combine model\ninference and business logic.\nIn this tutorial, we\u2019ll build a simple RAG pipeline for a hypothetical alumni\nmatching service for a university. The system:\nTakes a bio with information about a new graduate\nUses a vector database to retrieve semantically similar bios of other alums\nUses an LLM to explain why the new graduate should meet the selected alums\nReturns the writeup from the LLM\nLet\u2019s dive in!\n\u200b\nBuilding the Chain\nCreate a file\nrag.py\nin a new directory with:\nCopy\nAsk AI\nmkdir\nrag\ntouch\nrag/rag.py\ncd\nrag\nOur RAG Chain is composed of three parts:\nVectorStore\n, a Chainlet that implements a vector database with a retrieval\nfunction.\nLLMClient\n, a Stub for connecting to a deployed LLM.\nRAG\n, the entrypoint Chainlet that orchestrates the RAG pipeline and\nhas\nVectorStore\nand\nLLMClient\nas dependencies.\nWe\u2019ll examine these components one by one and then see how they all work\ntogether.\n\u200b\nVector store Chainlet\nA real production RAG system would use a hosted vector database with a massive\nnumber of stored embeddings. For this example, we\u2019re using a small local vector\nstore built with\nchromadb\nto stand in for a more complex system.\nThe Chainlet has three parts:\nremote_config\n, which\nconfigures a Docker image on deployment with dependencies.\n__init__()\n, which runs once when the Chainlet is spun up, and creates the\nvector database with ten sample bios.\nrun_remote()\n, which runs\neach time the Chainlet is called and is the sole public interface for the\nChainlet.\nrag/rag.py\nCopy\nAsk AI\nimport\ntruss_chains\nas\nchains\n# Create a Chainlet to serve as our vector database.\nclass\nVectorStore\n(\nchains\n.\nChainletBase\n):\n# Add chromadb as a dependency for deployment.\nremote_config\n=\nchains.RemoteConfig(\ndocker_image\n=\nchains.DockerImage(\npip_requirements\n=\n[\n\"chromadb\"\n]\n)\n)\n# Runs once when the Chainlet is deployed or scaled up.\ndef\n__init__\n(\nself\n):\n# Import Chainlet-specific dependencies in init, not at the top of\n# the file.\nimport\nchromadb\nself\n._chroma_client\n=\nchromadb.EphemeralClient()\nself\n._collection\n=\nself\n._chroma_client.create_collection(\nname\n=\n\"bios\"\n)\n# Sample documents are hard-coded for your convenience\ndocuments\n=\n[\n\"Angela Martinez is a tech entrepreneur based in San Francisco. As the founder and CEO of a successful AI startup, she is a leading figure in the tech community. Outside of work, Angela enjoys hiking the trails around the Bay Area and volunteering at local animal shelters.\"\n,\n\"Ravi Patel resides in New York City, where he works as a financial analyst. Known for his keen insight into market trends, Ravi spends his weekends playing chess in Central Park and exploring the city's diverse culinary scene.\"\n,\n\"Sara Kim is a digital marketing specialist living in San Francisco. She helps brands build their online presence with creative strategies. Outside of work, Sara is passionate about photography and enjoys hiking the trails around the Bay Area.\"\n,\n\"David O'Connor calls New York City his home and works as a high school teacher. He is dedicated to inspiring the next generation through education. In his free time, David loves running along the Hudson River and participating in local theater productions.\"\n,\n\"Lena Rossi is an architect based in San Francisco. She designs sustainable and innovative buildings that contribute to the city's skyline. When she's not working, Lena enjoys practicing yoga and exploring art galleries.\"\n,\n\"Akio Tanaka lives in Tokyo and is a software developer specializing in mobile apps. Akio is an avid gamer and enjoys attending eSports tournaments. He also has a passion for cooking and often experiments with new recipes in his spare time.\"\n,\n\"Maria Silva is a nurse residing in New York City. She is dedicated to providing compassionate care to her patients. Maria finds joy in gardening and often spends her weekends tending to her vibrant flower beds and vegetable garden.\"\n,\n\"John Smith is a journalist based in San Francisco. He reports on international politics and has a knack for uncovering compelling stories.", "mimetype": "text/plain", "start_char_idx": 266552, "end_char_idx": 270927, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "48c996ef-173a-48d0-8f63-5c1b94cc568a": {"__data__": {"id_": "48c996ef-173a-48d0-8f63-5c1b94cc568a", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "70b981ce-d788-42eb-a096-77f20488dc39", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "3fecc1fbd9ad81495e311a6d144cc745abf300f4082e886cec87512dbacfda57", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ed3c8710-e4da-4eab-a25c-57425ea642ec", "node_type": "1", "metadata": {}, "hash": "33eb9257ac403bb6919afaab46b7918afe3a62102aa7e099c0271b0411ed1d88", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In his free time, David loves running along the Hudson River and participating in local theater productions.\"\n,\n\"Lena Rossi is an architect based in San Francisco. She designs sustainable and innovative buildings that contribute to the city's skyline. When she's not working, Lena enjoys practicing yoga and exploring art galleries.\"\n,\n\"Akio Tanaka lives in Tokyo and is a software developer specializing in mobile apps. Akio is an avid gamer and enjoys attending eSports tournaments. He also has a passion for cooking and often experiments with new recipes in his spare time.\"\n,\n\"Maria Silva is a nurse residing in New York City. She is dedicated to providing compassionate care to her patients. Maria finds joy in gardening and often spends her weekends tending to her vibrant flower beds and vegetable garden.\"\n,\n\"John Smith is a journalist based in San Francisco. He reports on international politics and has a knack for uncovering compelling stories. Outside of work, John is a history buff who enjoys visiting museums and historical sites.\"\n,\n\"Aisha Mohammed lives in Tokyo and works as a graphic designer. She creates visually stunning graphics for a variety of clients. Aisha loves to paint and often showcases her artwork in local exhibitions.\"\n,\n\"Carlos Mendes is an environmental engineer in San Francisco. He is passionate about developing sustainable solutions for urban areas. In his leisure time, Carlos enjoys surfing and participating in beach clean-up initiatives.\"\n]\n# Add all documents to the database\nself\n._collection.add(\ndocuments\n=\ndocuments,\nids\n=\n[\nf\n\"id\n{\nn\n}\n\"\nfor\nn\nin\nrange\n(\nlen\n(documents))]\n)\n# Runs each time the Chainlet is called\nasync\ndef\nrun_remote\n(\nself\n,\nquery\n:\nstr\n) -> list[\nstr\n]:\n# This call to includes embedding the query string.\nresults\n=\nself\n._collection.query(\nquery_texts\n=\n[query],\nn_results\n=\n2\n)\nif\nresults\nis\nNone\nor\nnot\nresults:\nraise\nValueError\n(\n\"No bios returned from the query\"\n)\nif\nnot\nresults[\n\"documents\"\n]\nor\nnot\nresults[\n\"documents\"\n][\n0\n]:\nraise\nValueError\n(\n\"Bios are empty\"\n)\nreturn\nresults[\n\"documents\"\n][\n0\n]\n\u200b\nLLM inference stub\nNow that we can retrieve relevant bios from the vector database, we need to pass\nthat information to an LLM to generate our final output.\nChains can integrate previously deployed models using a Stub. Like Chainlets,\nStubs implement\nrun_remote()\n, but as a call\nto the deployed model.\nFor our LLM, we\u2019ll use Phi-3 Mini Instruct, a small-but-mighty open source LLM.\nDeploy Phi-3 Mini Instruct 4k\nOne-click model deployment from Baseten\u2019s model library.\nWhile the model is deploying, be sure to note down the models\u2019 invocation URL from\nthe model dashboard for use in the next step.\nTo use our deployed LLM in the RAG Chain, we define a Stub:\nrag/rag.py\nCopy\nAsk AI\nclass\nLLMClient\n(\nchains\n.\nStubBase\n):\n# Runs each time the Stub is called\nasync\ndef\nrun_remote\n(\nself\n,\nnew_bio\n:\nstr\n,\nbios\n: list[\nstr\n]) ->\nstr\n:\n# Use the retrieved bios to augment the prompt -- here's the \"A\" in RAG!\nprompt\n=\nf\n\"\"\"You are matching alumni of a college to help them make connections. Explain why the person described first would want to meet the people selected from the matching database.\nPerson you're matching:\n{\nnew_bio\n}\nPeople from database:\n{\n\" \"\n.join(bios)\n}\n\"\"\"\n# Call the deployed model.\nresp\n=\nawait\nself\n._remote.predict_async(\njson_payload\n=\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: prompt}],\n\"stream\"\n:\nFalse\n})\nreturn\nresp[\n\"output\"\n][\nlen\n(prompt) :].strip()\n\u200b\nRAG entrypoint Chainlet\nThe entrypoint to a Chain is the Chainlet that specifies the public-facing input\nand output of the Chain and orchestrates calls to dependencies.\nThe\n__init__\nfunction in this Chainlet takes two new arguments:\nAdd dependencies to any Chainlet with\nchains.depends()\n. Only\nChainlets, not Stubs, need to be added in this fashion.\nUse\nchains.depends_context()\nto inject a context object at runtime. This context object is required to\ninitialize the\nLLMClient\nstub.", "mimetype": "text/plain", "start_char_idx": 269972, "end_char_idx": 273928, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ed3c8710-e4da-4eab-a25c-57425ea642ec": {"__data__": {"id_": "ed3c8710-e4da-4eab-a25c-57425ea642ec", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "48c996ef-173a-48d0-8f63-5c1b94cc568a", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "968f99a3bbfe2baa40a42435ce0854d39e288eca71f7474391eed36e7ca602e9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "56b40446-0053-4d08-b4a6-dda9318adf30", "node_type": "1", "metadata": {}, "hash": "da156d7df71a9cb9ef0af343feeeec97ee2d30207c00f81b842782a63f919f0d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "resp\n=\nawait\nself\n._remote.predict_async(\njson_payload\n=\n{\n\"messages\"\n: [{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: prompt}],\n\"stream\"\n:\nFalse\n})\nreturn\nresp[\n\"output\"\n][\nlen\n(prompt) :].strip()\n\u200b\nRAG entrypoint Chainlet\nThe entrypoint to a Chain is the Chainlet that specifies the public-facing input\nand output of the Chain and orchestrates calls to dependencies.\nThe\n__init__\nfunction in this Chainlet takes two new arguments:\nAdd dependencies to any Chainlet with\nchains.depends()\n. Only\nChainlets, not Stubs, need to be added in this fashion.\nUse\nchains.depends_context()\nto inject a context object at runtime. This context object is required to\ninitialize the\nLLMClient\nstub.\nVisit your\nbaseten workspace\nto find your\nthe URL of the previously deployed Phi-3 model and insert if as value\nfor\nLLM_URL\n.\nrag/rag.py\nCopy\nAsk AI\n# Insert the URL from the previously deployed Phi-3 model.\nLLM_URL\n=\n...\n@chains.mark_entrypoint\nclass\nRAG\n(\nchains\n.\nChainletBase\n):\n# Runs once when the Chainlet is spun up\ndef\n__init__\n(\nself\n,\n# Declare dependency chainlets.\nvector_store\n: VectorStore\n=\nchains.depends(VectorStore),\ncontext\n: chains.DeploymentContext\n=\nchains.depends_context(),\n):\nself\n._vector_store\n=\nvector_store\n# The stub needs the context for setting up authentication.\nself\n._llm\n=\nLLMClient.from_url(\nLLM_URL\n, context)\n# Runs each time the Chain is called\nasync\ndef\nrun_remote\n(\nself\n,\nnew_bio\n:\nstr\n) ->\nstr\n:\n# Use the VectorStore Chainlet for context retrieval.\nbios\n=\nawait\nself\n._vector_store.run_remote(new_bio)\n# Use the LLMClient Stub for augmented generation.\ncontacts\n=\nawait\nself\n._llm.run_remote(new_bio, bios)\nreturn\ncontacts\n\u200b\nTesting locally\nBecause our Chain uses a Stub for the LLM call, we can run the whole Chain\nlocally without any GPU resources.\nBefore running the Chainlet, make sure to set your Baseten API key as an\nenvironment variable\nBASETEN_API_KEY\n.\nrag/rag.py\nCopy\nAsk AI\nif\n__name__\n==\n\"__main__\"\n:\nimport\nos\nimport\nasyncio\nwith\nchains.run_local(\n# This secret is needed even locally, because part of this chain\n# calls the separately deployed Phi-3 model. Only the Chainlets\n# actually run locally.\nsecrets\n=\n{\n\"baseten_chain_api_key\"\n: os.environ[\n\"BASETEN_API_KEY\"\n]}\n):\nrag_client\n=\nRAG()\nresult\n=\nasyncio.get_event_loop().run_until_complete(\nrag_client.run_remote(\n\"\"\"\nSam just moved to Manhattan for his new job at a large bank.\nIn college, he enjoyed building sets for student plays.\n\"\"\"\n)\n)\nprint\n(result)\nWe can run our Chain locally:\nCopy\nAsk AI\npython\nrag.py\nAfter a few moments, we should get a recommendation for why Sam should meet the\nalumni selected from the database.\n\u200b\nDeploying to production\nOnce we\u2019re satisfied with our Chain\u2019s local behavior, we can deploy it to\nproduction on Baseten. To deploy the Chain, run:\nCopy\nAsk AI\ntruss\nchains\npush\nrag.py\nThis will deploy our Chain as a development deployment. Once the Chain is\ndeployed, we can call it from its API endpoint.\nYou can do this in the console with cURL:\nCopy\nAsk AI\ncurl\n-X\nPOST\n'https://chain-5wo86nn3.api.baseten.co/development/run_remote'\n\\\n-H\n\"Authorization: Api-Key\n$BASETEN_API_KEY\n\"\n\\\n-d\n'{\"new_bio\": \"Sam just moved to Manhattan for his new job at a large bank.In college, he enjoyed building sets for student plays.\"}'\nAlternatively, you can also integrate this in a Python application:\ncall_chain.py\nCopy\nAsk AI\nimport\nrequests\nimport\nos\n# Insert the URL from the deployed rag chain. You can get it from the CLI\n# output or the status page, e.g.\n# \"https://chain-6wgeygoq.api.baseten.co/production/run_remote\".", "mimetype": "text/plain", "start_char_idx": 273258, "end_char_idx": 276791, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "56b40446-0053-4d08-b4a6-dda9318adf30": {"__data__": {"id_": "56b40446-0053-4d08-b4a6-dda9318adf30", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ed3c8710-e4da-4eab-a25c-57425ea642ec", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "b90774cf022d452ea107168bdc23458a29f22dc90777ac52df4ff1303ba68e5a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "26e27990-9b99-4df3-9f9f-b0312d1bcf1b", "node_type": "1", "metadata": {}, "hash": "425f1ba9df3547af84a23a7851c4f91796f40bf027f4c72fbf71f5054a8a76fb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Once the Chain is\ndeployed, we can call it from its API endpoint.\nYou can do this in the console with cURL:\nCopy\nAsk AI\ncurl\n-X\nPOST\n'https://chain-5wo86nn3.api.baseten.co/development/run_remote'\n\\\n-H\n\"Authorization: Api-Key\n$BASETEN_API_KEY\n\"\n\\\n-d\n'{\"new_bio\": \"Sam just moved to Manhattan for his new job at a large bank.In college, he enjoyed building sets for student plays.\"}'\nAlternatively, you can also integrate this in a Python application:\ncall_chain.py\nCopy\nAsk AI\nimport\nrequests\nimport\nos\n# Insert the URL from the deployed rag chain. You can get it from the CLI\n# output or the status page, e.g.\n# \"https://chain-6wgeygoq.api.baseten.co/production/run_remote\".\nRAG_CHAIN_URL\n=\n\"\"\nbaseten_api_key\n=\nos.environ[\n\"BASETEN_API_KEY\"\n]\nif\nnot\nRAG_CHAIN_URL\n:\nraise\nValueError\n(\n\"Please insert the URL for the RAG chain.\"\n)\nresp\n=\nrequests.post(\nRAG_CHAIN_URL\n,\nheaders\n=\n{\n\"Authorization\"\n:\nf\n\"Api-Key\n{\nbaseten_api_key\n}\n\"\n},\njson\n=\n{\n\"new_bio\"\n: new_bio},\n)\nprint\n(resp.json())\nWhen we\u2019re happy with the deployed Chain, we can promote it to production via\nthe UI or by running:\nCopy\nAsk AI\ntruss\nchains\npush\n--promote\nrag.py\nOnce in production, the Chain will have access to full autoscaling settings.\nBoth the development and production deployments will scale to zero when not in\nuse.\nWas this page helpful?\nYes\nNo\nPrevious\nTranscribe audio with Chains\nProcess hours of audio in seconds using efficient chunking, distributed inference, and optimized GPU resources.\nNext\nOn this page\nPrerequisites\nOverview\nBuilding the Chain\nVector store Chainlet\nLLM inference stub\nRAG entrypoint Chainlet\nTesting locally\nDeploying to production\nAssistant\nResponses are generated using AI and may contain mistakes.\n\n\nContent from https://docs.baseten.co/examples/comfyui:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nExamples\nOverview\nDeploy your first model\nFast LLMs with TensorRT-LLM\nRun any LLM with vLLM\nDeploy LLMs with SGLang\nRAG pipeline with Chains\nTranscribe audio with Chains\nImage generation\nDeploy a ComfyUI project\nEmbeddings with BEI\nDockerized model\nLLM with Streaming\nText to speech\nModel library\nOverview\nDeepseek\nLlama\nQwen\nGemma\nStable Diffusion\nFlux\nKokoro\nMicrosoft\nNomic\nWhisper\nMars\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nExamples\nDeploy a ComfyUI project\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nView example on GitHub\nIn this example, we\u2019ll deploy an\nanime style transfer\nComfyUI workflow using truss.\nThis example won\u2019t require any Python code, but there are a few pre-requisites in order to get started.\nPre-Requisites:\nConvert your ComfyUI workflow to an\nAPI compatible JSON format\n. The regular JSON format that is used to export Comfy workflows will not work here.\nHave a list of the models your workflow requires along with URLs to where each model can be downloaded\n\u200b\nSetup\nClone the truss-examples repository and navigate to the\ncomfyui-truss\ndirectory\nCopy\nAsk AI\ngit\nclone\nhttps://github.com/basetenlabs/truss-examples.git\ncd\ntruss-examples/comfyui-truss\nThis repository already contains all the files we need to deploy our ComfyUI workflow.\nThere are just two files we need to modify:\nconfig.yaml\ndata/comfy_ui_workflow.json\n\u200b\nSetting up the\nconfig.yaml\nCopy\nAsk AI\nbuild_commands\n:\n-\ngit clone https://github.com/comfyanonymous/ComfyUI.git\n-\ncd ComfyUI && git checkout b1fd26fe9e55163f780bf9e5f56bf9bf5f035c93 && pip install -r requirements.txt\n-\ncd ComfyUI/custom_nodes && git clone https://github.com/LykosAI/ComfyUI-Inference-Core-Nodes --recursive && cd ComfyUI-Inference-Core-Nodes && pip install -e .", "mimetype": "text/plain", "start_char_idx": 276117, "end_char_idx": 279730, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "26e27990-9b99-4df3-9f9f-b0312d1bcf1b": {"__data__": {"id_": "26e27990-9b99-4df3-9f9f-b0312d1bcf1b", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "56b40446-0053-4d08-b4a6-dda9318adf30", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "0654f378189bfc3cec4193c25478587f9c7a8313003a635825571d5c21012fa8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c0488e72-595b-481a-a556-36580943f430", "node_type": "1", "metadata": {}, "hash": "8a4fb187293f0a631fbbb4fa593fdec9cd57811a161f7b45a43949e5ee08d4b4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "There are just two files we need to modify:\nconfig.yaml\ndata/comfy_ui_workflow.json\n\u200b\nSetting up the\nconfig.yaml\nCopy\nAsk AI\nbuild_commands\n:\n-\ngit clone https://github.com/comfyanonymous/ComfyUI.git\n-\ncd ComfyUI && git checkout b1fd26fe9e55163f780bf9e5f56bf9bf5f035c93 && pip install -r requirements.txt\n-\ncd ComfyUI/custom_nodes && git clone https://github.com/LykosAI/ComfyUI-Inference-Core-Nodes --recursive && cd ComfyUI-Inference-Core-Nodes && pip install -e .[cuda12]\n-\ncd ComfyUI/custom_nodes && git clone https://github.com/ZHO-ZHO-ZHO/ComfyUI-Gemini --recursive && cd ComfyUI-Gemini && pip install -r requirements.txt\n-\ncd ComfyUI/custom_nodes && git clone https://github.com/kijai/ComfyUI-Marigold --recursive && cd ComfyUI-Marigold && pip install -r requirements.txt\n-\ncd ComfyUI/custom_nodes && git clone https://github.com/omar92/ComfyUI-QualityOfLifeSuit_Omar92 --recursive\n-\ncd ComfyUI/custom_nodes && git clone https://github.com/Fannovel16/comfyui_controlnet_aux --recursive && cd comfyui_controlnet_aux && pip install -r requirements.txt\n-\ncd ComfyUI/models/controlnet && wget -O control-lora-canny-rank256.safetensors https://huggingface.co/stabilityai/control-lora/resolve/main/control-LoRAs-rank256/control-lora-canny-rank256.safetensors\n-\ncd ComfyUI/models/controlnet && wget -O control-lora-depth-rank256.safetensors https://huggingface.co/stabilityai/control-lora/resolve/main/control-LoRAs-rank256/control-lora-depth-rank256.safetensors\n-\ncd ComfyUI/models/checkpoints && wget -O dreamshaperXL_v21TurboDPMSDE.safetensors https://civitai.com/api/download/models/351306\n-\ncd ComfyUI/models/loras && wget -O StudioGhibli.Redmond-StdGBRRedmAF-StudioGhibli.safetensors https://huggingface.co/artificialguybr/StudioGhibli.Redmond-V2/resolve/main/StudioGhibli.Redmond-StdGBRRedmAF-StudioGhibli.safetensors\nenvironment_variables\n: {}\nexternal_package_dirs\n: []\nmodel_metadata\n: {}\nmodel_name\n:\nAnime Style Transfer\npython_version\n:\npy310\nrequirements\n:\n-\nwebsocket-client\n-\naccelerate\n-\nopencv-python\nresources\n:\naccelerator\n:\nH100\nuse_gpu\n:\ntrue\nsecrets\n: {}\nsystem_packages\n:\n-\nwget\n-\nffmpeg\n-\nlibgl1-mesa-glx\nThe main part that needs to get filled out is under\nbuild_commands\n. Build commands are shell commands that get run during the build stage of the docker image.\nIn this example, the first two lines clone the ComfyUI repository and install the python requirements.\nThe latter commands install various custom nodes and models and place them in their respective directory within the ComfyUI repository.\n\u200b\nModifying\ndata/comfy_ui_workflow.json\nThe\ncomfy_ui_workflow.json\ncontains the entire ComfyUI workflow in an API compatible format. This is the workflow that will get executed by the ComfyUI server.\nHere is the workflow we will be using for this example.\nAnime Style Transfer Workflow\nCopy\nAsk AI\n{\n\"1\"\n: {\n\"inputs\"\n: {\n\"ckpt_name\"\n:\n\"dreamshaperXL_v21TurboDPMSDE.safetensors\"\n},\n\"class_type\"\n:\n\"CheckpointLoaderSimple\"\n,\n\"_meta\"\n: {\n\"title\"\n:\n\"Load Checkpoint\"\n}\n},\n\"3\"\n: {\n\"inputs\"\n: {\n\"image\"\n:\n\"{{input_image}}\"\n,\n\"upload\"\n:\n\"image\"\n},\n\"class_type\"\n:\n\"LoadImage\"\n,\n\"_meta\"\n: {\n\"title\"\n:\n\"Load Image\"\n}\n},\n\"4\"\n: {\n\"inputs\"\n: {\n\"text\"\n: [\n\"160\"\n,\n0\n],\n\"clip\"\n: [\n\"154\"\n,\n1\n]\n},\n\"class_type\"\n:\n\"CLIPTextEncode\"\n,\n\"_meta\"\n: {\n\"title\"\n:\n\"CLIP Text Encode (Prompt)\"\n}\n},\n\"12\"\n: {\n\"inputs\"\n: {\n\"strength\"\n:\n0.8\n,\n\"conditioning\"\n: [\n\"131\"\n,\n0\n],", "mimetype": "text/plain", "start_char_idx": 279264, "end_char_idx": 282635, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c0488e72-595b-481a-a556-36580943f430": {"__data__": {"id_": "c0488e72-595b-481a-a556-36580943f430", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "26e27990-9b99-4df3-9f9f-b0312d1bcf1b", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "c6ca865d82782faf95b5f5c7f777e0445056252efe094b69db9c512813ba1364", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0e1a68a1-350c-4eaf-afdb-f91a5ecbc9bd", "node_type": "1", "metadata": {}, "hash": "f4d97c7315e316186ae063f06e69f66695ba926d9e6cb674c59c22bf614c2586", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "safetensors\"\n},\n\"class_type\"\n:\n\"CheckpointLoaderSimple\"\n,\n\"_meta\"\n: {\n\"title\"\n:\n\"Load Checkpoint\"\n}\n},\n\"3\"\n: {\n\"inputs\"\n: {\n\"image\"\n:\n\"{{input_image}}\"\n,\n\"upload\"\n:\n\"image\"\n},\n\"class_type\"\n:\n\"LoadImage\"\n,\n\"_meta\"\n: {\n\"title\"\n:\n\"Load Image\"\n}\n},\n\"4\"\n: {\n\"inputs\"\n: {\n\"text\"\n: [\n\"160\"\n,\n0\n],\n\"clip\"\n: [\n\"154\"\n,\n1\n]\n},\n\"class_type\"\n:\n\"CLIPTextEncode\"\n,\n\"_meta\"\n: {\n\"title\"\n:\n\"CLIP Text Encode (Prompt)\"\n}\n},\n\"12\"\n: {\n\"inputs\"\n: {\n\"strength\"\n:\n0.8\n,\n\"conditioning\"\n: [\n\"131\"\n,\n0\n],\n\"control_net\"\n: [\n\"13\"\n,\n0\n],\n\"image\"\n: [\n\"71\"\n,\n0\n]\n},\n\"class_type\"\n:\n\"ControlNetApply\"\n,\n\"_meta\"\n: {\n\"title\"\n:\n\"Apply ControlNet\"\n}\n},\n\"13\"\n: {\n\"inputs\"\n: {\n\"control_net_name\"\n:\n\"control-lora-canny-rank256.safetensors\"\n},\n\"class_type\"\n:\n\"ControlNetLoader\"\n,\n\"_meta\"\n: {\n\"title\"\n:\n\"Load ControlNet Model\"\n}\n},\n\"15\"\n: {\n\"inputs\"\n: {\n\"strength\"\n:\n0.8\n,\n\"conditioning\"\n: [\n\"12\"\n,\n0\n],\n\"control_net\"\n: [\n\"16\"\n,\n0\n],\n\"image\"\n: [\n\"18\"\n,\n0\n]\n},\n\"class_type\"\n:\n\"ControlNetApply\"\n,\n\"_meta\"\n: {\n\"title\"\n:\n\"Apply ControlNet\"\n}\n},\n\"16\"\n: {\n\"inputs\"\n: {\n\"control_net_name\"\n:\n\"control-lora-depth-rank256.safetensors\"\n},\n\"class_type\"\n:\n\"ControlNetLoader\"\n,\n\"_meta\"\n: {\n\"title\"\n:\n\"Load ControlNet Model\"\n}\n},\n\"18\"\n: {\n\"inputs\"\n: {\n\"seed\"\n:\n995352869972963\n,\n\"denoise_steps\"\n:\n4\n,\n\"n_repeat\"\n:\n10\n,\n\"regularizer_strength\"\n:\n0.02\n,\n\"reduction_method\"\n:\n\"median\"\n,\n\"max_iter\"\n:\n5\n,\n\"tol\"\n:\n0.001\n,\n\"invert\"\n:\ntrue\n,\n\"keep_model_loaded\"\n:\ntrue\n,\n\"n_repeat_batch_size\"\n:\n2\n,\n\"use_fp16\"\n:\ntrue\n,\n\"scheduler\"\n:\n\"LCMScheduler\"\n,\n\"normalize\"\n:\ntrue\n,\n\"model\"\n:\n\"marigold-lcm-v1-0\"\n,\n\"image\"\n: [\n\"3\"\n,\n0\n]\n},\n\"class_type\"\n:\n\"MarigoldDepthEstimation\"\n,\n\"_meta\"\n: {\n\"title\"\n:\n\"MarigoldDepthEstimation\"\n}\n},\n\"19\"\n: {\n\"inputs\"\n: {\n\"images\"\n: [\n\"71\"\n,\n0\n]\n},\n\"class_type\"\n:\n\"PreviewImage\"\n,\n\"_meta\"\n: {\n\"title\"\n:\n\"Preview Image\"\n}\n},\n\"20\"\n: {\n\"inputs\"\n: {\n\"images\"\n: [\n\"18\"\n,\n0\n]\n},\n\"class_type\"\n:\n\"PreviewImage\"\n,\n\"_meta\"\n: {\n\"title\"\n:\n\"Preview Image\"\n}\n},\n\"21\"\n: {\n\"inputs\"\n: {\n\"seed\"\n:\n358881677137626\n,\n\"steps\"\n:\n20\n,\n\"cfg\"\n:\n7\n,\n\"sampler_name\"\n:\n\"dpmpp_2m_sde\"\n,\n\"scheduler\"\n:\n\"karras\"\n,\n\"denoise\"\n:\n0.7000000000000001\n,\n\"model\"\n: [\n\"154\"\n,\n0\n],\n\"positive\"\n: [\n\"15\"\n,\n0\n],\n\"negative\"\n: [\n\"4\"\n,\n0\n],\n\"latent_image\"\n: [\n\"25\"\n,\n0\n]\n},\n\"class_type\"\n:\n\"KSampler\"\n,\n\"_meta\"\n: {\n\"title\"\n:\n\"KSampler\"\n}\n},\n\"25\"\n: {\n\"inputs\"\n: {\n\"pixels\"\n: [\n\"70\"\n,\n0\n],\n\"vae\"\n: [\n\"1\"\n,\n2\n]\n},\n\"class_type\"\n:\n\"VAEEncode\"\n,\n\"_meta\"\n: {\n\"title\"\n:\n\"VAE Encode\"\n}\n},", "mimetype": "text/plain", "start_char_idx": 282158, "end_char_idx": 284547, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0e1a68a1-350c-4eaf-afdb-f91a5ecbc9bd": {"__data__": {"id_": "0e1a68a1-350c-4eaf-afdb-f91a5ecbc9bd", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c0488e72-595b-481a-a556-36580943f430", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "b07a15084a8e62a0ac1c3f477d530718ac7339644f30cfd5e6b1a00ab457e374", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1babe9c5-3d0a-4c4c-be56-02235674699d", "node_type": "1", "metadata": {}, "hash": "713ebb4d1e29c8ae5975a499f5f176bb14ea3b41a1ccdd118b9a777c18e8de91", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\"cfg\"\n:\n7\n,\n\"sampler_name\"\n:\n\"dpmpp_2m_sde\"\n,\n\"scheduler\"\n:\n\"karras\"\n,\n\"denoise\"\n:\n0.7000000000000001\n,\n\"model\"\n: [\n\"154\"\n,\n0\n],\n\"positive\"\n: [\n\"15\"\n,\n0\n],\n\"negative\"\n: [\n\"4\"\n,\n0\n],\n\"latent_image\"\n: [\n\"25\"\n,\n0\n]\n},\n\"class_type\"\n:\n\"KSampler\"\n,\n\"_meta\"\n: {\n\"title\"\n:\n\"KSampler\"\n}\n},\n\"25\"\n: {\n\"inputs\"\n: {\n\"pixels\"\n: [\n\"70\"\n,\n0\n],\n\"vae\"\n: [\n\"1\"\n,\n2\n]\n},\n\"class_type\"\n:\n\"VAEEncode\"\n,\n\"_meta\"\n: {\n\"title\"\n:\n\"VAE Encode\"\n}\n},\n\"27\"\n: {\n\"inputs\"\n: {\n\"samples\"\n: [\n\"21\"\n,\n0\n],\n\"vae\"\n: [\n\"1\"\n,\n2\n]\n},\n\"class_type\"\n:\n\"VAEDecode\"\n,\n\"_meta\"\n: {\n\"title\"\n:\n\"VAE Decode\"\n}\n},\n\"70\"\n: {\n\"inputs\"\n: {\n\"upscale_method\"\n:\n\"lanczos\"\n,\n\"megapixels\"\n:\n1\n,\n\"image\"\n: [\n\"3\"\n,\n0\n]\n},\n\"class_type\"\n:\n\"ImageScaleToTotalPixels\"\n,\n\"_meta\"\n: {\n\"title\"\n:\n\"ImageScaleToTotalPixels\"\n}\n},\n\"71\"\n: {\n\"inputs\"\n: {\n\"low_threshold\"\n:\n50\n,\n\"high_threshold\"\n:\n150\n,\n\"resolution\"\n:\n1024\n,\n\"image\"\n: [\n\"3\"\n,\n0\n]\n},\n\"class_type\"\n:\n\"CannyEdgePreprocessor\"\n,\n\"_meta\"\n: {\n\"title\"\n:\n\"Canny Edge\"\n}\n},\n\"123\"\n: {\n\"inputs\"\n: {\n\"images\"\n: [\n\"27\"\n,\n0\n]\n},\n\"class_type\"\n:\n\"PreviewImage\"\n,\n\"_meta\"\n: {\n\"title\"\n:\n\"Preview Image\"\n}\n},\n\"131\"\n: {\n\"inputs\"\n: {\n\"text\"\n: [\n\"159\"\n,\n0\n],\n\"clip\"\n: [\n\"154\"\n,\n1\n]\n},\n\"class_type\"\n:\n\"CLIPTextEncode\"\n,\n\"_meta\"\n: {\n\"title\"\n:\n\"CLIP Text Encode (Prompt)\"\n}\n},\n\"152\"\n: {\n\"inputs\"\n: {\n\"text\"\n:\n\"{{prompt}}\"\n},\n\"class_type\"\n:\n\"Text _O\"\n,\n\"_meta\"\n: {\n\"title\"\n:\n\"Text_1\"\n}\n},\n\"154\"\n: {\n\"inputs\"\n: {\n\"lora_name\"\n:\n\"StudioGhibli.Redmond-StdGBRRedmAF-StudioGhibli.safetensors\"\n,\n\"strength_model\"\n:\n0.6\n,\n\"strength_clip\"\n:\n1\n,\n\"model\"\n: [\n\"1\"\n,\n0\n],\n\"clip\"\n: [\n\"1\"\n,\n1\n]\n},\n\"class_type\"\n:\n\"LoraLoader\"\n,\n\"_meta\"\n: {\n\"title\"\n:\n\"Load LoRA\"\n}\n},\n\"156\"\n: {\n\"inputs\"\n: {\n\"text_1\"\n: [\n\"152\"\n,\n0\n],\n\"text_2\"\n: [\n\"158\"\n,\n0\n]\n},\n\"class_type\"\n:\n\"ConcatText_Zho\"\n,\n\"_meta\"\n: {\n\"title\"\n:\n\"\u2728ConcatText_Zho\"\n}\n},\n\"157\"\n: {\n\"inputs\"\n: {\n\"text\"\n:\n\"StdGBRedmAF,Studio Ghibli,\"\n},\n\"class_type\"\n:\n\"Text _O\"\n,\n\"_meta\"\n: {\n\"title\"\n:\n\"Text _2\"\n}\n},\n\"158\"\n: {\n\"inputs\"\n: {\n\"text\"\n:\n\"looking at viewer, anime artwork, anime style, key visual, vibrant, studio anime, highly detailed\"\n},\n\"class_type\"\n:\n\"Text _O\"\n,\n\"_meta\"\n: {\n\"title\"\n:\n\"Text _O\"\n}\n},\n\"159\"\n: {\n\"inputs\"\n: {\n\"text_1\"\n: [\n\"156\"\n,\n0\n],\n\"text_2\"\n: [\n\"157\"\n,\n0\n]\n},\n\"class_type\"\n:\n\"ConcatText_Zho\"\n,\n\"_meta\"\n: {\n\"title\"\n:\n\"\u2728ConcatText_Zho\"\n}\n},\n\"160\"\n: {\n\"inputs\"\n: {\n\"text\"\n:\n\"photo, deformed, black and white, realism, disfigured,", "mimetype": "text/plain", "start_char_idx": 284128, "end_char_idx": 286466, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1babe9c5-3d0a-4c4c-be56-02235674699d": {"__data__": {"id_": "1babe9c5-3d0a-4c4c-be56-02235674699d", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0e1a68a1-350c-4eaf-afdb-f91a5ecbc9bd", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "5b2ba468ebd1ba6661ef1b7d93f66f00e69325e6016db6b4291fb2bc9a3a19b5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "53c686da-c604-4300-a874-43c9f167fad1", "node_type": "1", "metadata": {}, "hash": "9536f224a4aa9e2511586a59af7258639269eb1e9a19ca836e21bd99f77b7174", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Studio Ghibli,\"\n},\n\"class_type\"\n:\n\"Text _O\"\n,\n\"_meta\"\n: {\n\"title\"\n:\n\"Text _2\"\n}\n},\n\"158\"\n: {\n\"inputs\"\n: {\n\"text\"\n:\n\"looking at viewer, anime artwork, anime style, key visual, vibrant, studio anime, highly detailed\"\n},\n\"class_type\"\n:\n\"Text _O\"\n,\n\"_meta\"\n: {\n\"title\"\n:\n\"Text _O\"\n}\n},\n\"159\"\n: {\n\"inputs\"\n: {\n\"text_1\"\n: [\n\"156\"\n,\n0\n],\n\"text_2\"\n: [\n\"157\"\n,\n0\n]\n},\n\"class_type\"\n:\n\"ConcatText_Zho\"\n,\n\"_meta\"\n: {\n\"title\"\n:\n\"\u2728ConcatText_Zho\"\n}\n},\n\"160\"\n: {\n\"inputs\"\n: {\n\"text\"\n:\n\"photo, deformed, black and white, realism, disfigured, low contrast\"\n},\n\"class_type\"\n:\n\"Text _O\"\n,\n\"_meta\"\n: {\n\"title\"\n:\n\"Text _O\"\n}\n}\n}\nImportant:\nIf you look at the JSON file above, you\u2019ll notice we have templatized a few items using the\n{{handlebars}}\ntemplating style.\nIf there are any inputs in your ComfyUI workflow that should be variables such as input prompts, images, etc, you should templatize them using the handlebars format.\nIn this example workflow, there are two inputs:\n{{input_image}}\nand\n{{prompt}}\nWhen making an API call to this workflow, we will be able to pass in any variable for these two inputs.\n\u200b\nDeploying the Workflow to Baseten\nOnce you have both your\nconfig.yaml\nand\ndata/comfy_ui_workflow.json\nfilled out we can deploy this workflow just like any other model on Baseten.\npip install truss --upgrade\ntruss push --publish\n\u200b\nRunning Inference\nWhen you deploy the truss, it will spin up a new deployment in your Baseten account. Each deployment will expose a REST API endpoint which we can use to call this workflow.\nCopy\nAsk AI\nimport\nrequests\nimport\nos\nimport\nbase64\nfrom\nPIL\nimport\nImage\nfrom\nio\nimport\nBytesIO\n# Replace the empty string with your model id below\nmodel_id\n=\n\"\"\nbaseten_api_key\n=\nos.environ[\n\"BASETEN_API_KEY\"\n]\nBASE64_PREAMBLE\n=\n\"data:image/png;base64,\"\ndef\npil_to_b64\n(\npil_img\n):\nbuffered\n=\nBytesIO()\npil_img.save(buffered,\nformat\n=\n\"PNG\"\n)\nimg_str\n=\nbase64.b64encode(buffered.getvalue()).decode(\n\"utf-8\"\n)\nreturn\nimg_str\ndef\nb64_to_pil\n(\nb64_str\n):\nreturn\nImage.open(BytesIO(base64.b64decode(b64_str.replace(\nBASE64_PREAMBLE\n,\n\"\"\n))))\nvalues\n=\n{\n\"prompt\"\n:\n\"american Shorthair\"\n,\n\"input_image\"\n: {\n\"type\"\n:\n\"image\"\n,\n\"data\"\n: pil_to_b64(Image.open(\n\"/path/to/cat.png\"\n))}\n}\nresp\n=\nrequests.post(\nf\n\"https://model-\n{\nmodel_id\n}\n.api.baseten.co/production/predict\"\n,\nheaders\n=\n{\n\"Authorization\"\n:\nf\n\"Api-Key\n{\nbaseten_api_key\n}\n\"\n},\njson\n=\n{\n\"workflow_values\"\n: values}\n)\nres\n=\nresp.json()\nresults\n=\nres.get(\n\"result\"\n)\nfor\nitem\nin\nresults:\nif\nitem.get(\n\"format\"\n)\n==\n\"png\"\n:\ndata\n=\nitem.get(\n\"data\"\n)\nimg\n=\nb64_to_pil(data)\nimg.save(\nf\n\"pet-style-transfer-1.png\"\n)\nIf you recall, we templatized two variables in our workflow:\nprompt\nand\ninput_image\n. In our API call we can specify the values for these two variables like so:\nCopy\nAsk AI\nvalues = {\n\"prompt\"\n:\n\"Maltipoo\"\n,\n\"input_image\"\n: {\n\"type\"\n:\n\"image\"\n,\n\"data\"\n:\npil_to_b\n64\n(Image.open(\n\"/path/to/dog.png\"\n))\n}\n}\nIf your workflow contains more variables, simply add them to the dictionary above.\nThe API call returns an image in the form of a base64 string, which we convert to a PNG image.\nWas this page helpful?", "mimetype": "text/plain", "start_char_idx": 285941, "end_char_idx": 289031, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "53c686da-c604-4300-a874-43c9f167fad1": {"__data__": {"id_": "53c686da-c604-4300-a874-43c9f167fad1", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1babe9c5-3d0a-4c4c-be56-02235674699d", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "d6df0a52b15505a4e92ab8d73ed6b9d511c1bffd54e03b36e3778e5bc0c7d207", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0c38e4bb-32e3-42a8-b310-51f2e43ed43e", "node_type": "1", "metadata": {}, "hash": "6a9baabf34ef9b631ccb1047e1293d31af836bbe2309b37fd8de25c23e51a93e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In our API call we can specify the values for these two variables like so:\nCopy\nAsk AI\nvalues = {\n\"prompt\"\n:\n\"Maltipoo\"\n,\n\"input_image\"\n: {\n\"type\"\n:\n\"image\"\n,\n\"data\"\n:\npil_to_b\n64\n(Image.open(\n\"/path/to/dog.png\"\n))\n}\n}\nIf your workflow contains more variables, simply add them to the dictionary above.\nThe API call returns an image in the form of a base64 string, which we convert to a PNG image.\nWas this page helpful?\nYes\nNo\nPrevious\nEmbeddings with BEI\nServe embedding, reranking, and classification models\nNext\nOn this page\nSetup\nSetting up the config.yaml\nModifying data/comfy_ui_workflow.json\nDeploying the Workflow to Baseten\nRunning Inference\nAssistant\nResponses are generated using AI and may contain mistakes.\n\n\nContent from https://docs.baseten.co/examples/deploy-your-first-model:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nExamples\nOverview\nDeploy your first model\nFast LLMs with TensorRT-LLM\nRun any LLM with vLLM\nDeploy LLMs with SGLang\nRAG pipeline with Chains\nTranscribe audio with Chains\nImage generation\nDeploy a ComfyUI project\nEmbeddings with BEI\nDockerized model\nLLM with Streaming\nText to speech\nModel library\nOverview\nDeepseek\nLlama\nQwen\nGemma\nStable Diffusion\nFlux\nKokoro\nMicrosoft\nNomic\nWhisper\nMars\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nExamples\nDeploy your first model\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nThis guide walks through packaging and deploying\nPhi-3-mini-4k-instruct\n, a 3.8B parameter LLM, as a production-ready API endpoint.\nWe\u2019ll cover:\nLoading model weights\nfrom Hugging Face\nRunning inference\non a GPU\nConfiguring dependencies and infrastructure\nIterating with live reload development\nDeploying to production with autoscaling\nBy the end, you\u2019ll have an AI model running on scalable infrastructure, callable via an API.\n\u200b\n1. Setup\nBefore you begin:\nSign up\nor\nsign in\nto Baseten\nGenerate an\nAPI key\nand store it securely\nInstall\nTruss\n, our model packaging framework\nCopy\nAsk AI\npip\ninstall\n--upgrade\ntruss\nNew accounts include free credits\u2014this guide should use less than $1 in GPU\ncosts.\n\u200b\n2. Create a Truss\nA\nTruss\npackages your model into a\ndeployable container\nwith all dependencies and configurations.\nCreate a new Truss:\nCopy\nAsk AI\ntruss\ninit\nphi-3-mini\n&&\ncd\nphi-3-mini\nWhen prompted, give your Truss a name like\nPhi 3 Mini\n.\nYou should see the following file structure:\nCopy\nAsk AI\nphi-3-mini/\ndata/\nmodel/\n__init__.py\nmodel.py\npackages/\nconfig.yaml\nYou\u2019ll primarily edit\nmodel/model.py\nand\nconfig.yaml\n.\n\u200b\n3. Load model weights\nPhi-3-mini-4k-instruct\nis available on Hugging Face. We\u2019ll\nload its weights using\ntransformers.\nEdit\nmodel/model.py\n:\nmodel/model.py\nCopy\nAsk AI\nimport\ntorch\nfrom\ntransformers\nimport\nAutoModelForCausalLM, AutoTokenizer\nclass\nModel\n:\ndef\n__init__\n(\nself\n,\n**\nkwargs\n):\nself\n._model\n=\nNone\nself\n._tokenizer\n=\nNone\ndef\nload\n(\nself\n):\nself\n._model\n=\nAutoModelForCausalLM.from_pretrained(\n\"microsoft/Phi-3-mini-4k-instruct\"\n,\ndevice_map\n=\n\"cuda\"\n,\ntorch_dtype\n=\n\"auto\"\n)\nself\n._tokenizer\n=\nAutoTokenizer.from_pretrained(\n\"microsoft/Phi-3-mini-4k-instruct\"\n)\n\u200b\n4.", "mimetype": "text/plain", "start_char_idx": 288612, "end_char_idx": 291732, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "0c38e4bb-32e3-42a8-b310-51f2e43ed43e": {"__data__": {"id_": "0c38e4bb-32e3-42a8-b310-51f2e43ed43e", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "53c686da-c604-4300-a874-43c9f167fad1", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "3aa5458ffd71f6935cd96a51fc600dea174386992569b826b9c952e46646d330", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4c798e2c-ceca-4a11-ad12-d0f1ef390274", "node_type": "1", "metadata": {}, "hash": "4c23409d4b4e80cdd051b8ef0cf61217c37d73c00cae811b92651af384a9297a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u200b\n3. Load model weights\nPhi-3-mini-4k-instruct\nis available on Hugging Face. We\u2019ll\nload its weights using\ntransformers.\nEdit\nmodel/model.py\n:\nmodel/model.py\nCopy\nAsk AI\nimport\ntorch\nfrom\ntransformers\nimport\nAutoModelForCausalLM, AutoTokenizer\nclass\nModel\n:\ndef\n__init__\n(\nself\n,\n**\nkwargs\n):\nself\n._model\n=\nNone\nself\n._tokenizer\n=\nNone\ndef\nload\n(\nself\n):\nself\n._model\n=\nAutoModelForCausalLM.from_pretrained(\n\"microsoft/Phi-3-mini-4k-instruct\"\n,\ndevice_map\n=\n\"cuda\"\n,\ntorch_dtype\n=\n\"auto\"\n)\nself\n._tokenizer\n=\nAutoTokenizer.from_pretrained(\n\"microsoft/Phi-3-mini-4k-instruct\"\n)\n\u200b\n4. Implement Model Inference\nDefine how the model processes incoming requests by implementing the\npredict()\nfunction:\nmodel/model.py\nCopy\nAsk AI\nclass\nModel\n:\n...\ndef\npredict\n(\nself\n,\nrequest\n):\nmessages\n=\nrequest.pop(\n\"messages\"\n)\nmodel_inputs\n=\nself\n._tokenizer.apply_chat_template(\nmessages,\ntokenize\n=\nFalse\n,\nadd_generation_prompt\n=\nTrue\n)\ninputs\n=\nself\n._tokenizer(model_inputs,\nreturn_tensors\n=\n\"pt\"\n).to(\n\"cuda\"\n)\nwith\ntorch.no_grad():\noutputs\n=\nself\n._model.generate(\ninput_ids\n=\ninputs[\n\"input_ids\"\n],\nmax_length\n=\n256\n)\nreturn\n{\n\"output\"\n:\nself\n._tokenizer.decode(outputs[\n0\n],\nskip_special_tokens\n=\nTrue\n)}\nThis function:\n\u2705 Accepts a list of messages\n\u2705 Uses Hugging Face\u2019s tokenizer\n\u2705 Generates a response with max 256 tokens\n\u200b\n5. Configure Dependencies & GPU\nIn\nconfig.yaml\n, define the\nPython environment\nand\ncompute resources\n:\n\u200b\nSet Dependencies\nconfig.yaml\nCopy\nAsk AI\nrequirements\n:\n-\nsix==1.17.0\n-\naccelerate==0.30.1\n-\neinops==0.8.0\n-\ntransformers==4.41.2\n-\ntorch==2.3.0\n\u200b\nAllocate a GPU\nPhi-3-mini needs ~7.6GB VRAM. A T4 GPU (16GB VRAM) is a good choice.\nconfig.yaml\nCopy\nAsk AI\nresources\n:\naccelerator\n:\nT4\nuse_gpu\n:\ntrue\n\u200b\n6. Deploy the Model\n\u200b\n1. Get Your API Key\n\ud83d\udd17 Generate an API Key\nYou can generate the API key from the Baseten UI. Click on the User icon at the top-right, then click API keys. Save your API-key, because we will use it in the next step.\n\u200b\n2. Push Your Model to Baseten\nCopy\nAsk AI\ntruss\npush\nSince this is a first-time deployment,\ntruss\nwill ask for your API-key and save it for future runs.\nMonitor the deployment from\nyour Baseten dashboard\n.\n\u200b\n7. Call the Model API\nAfter the deployment is complete, we can call the model API. First, store the Baseten API key as an environment variable:\nCopy\nAsk AI\nexport\nBASETEN_API_KEY\n=<\nyour_api_key\n>\nBelow is the client code. Be sure to replace\nmodel_id\nfrom your deployment.\nCopy\nAsk AI\nimport\nrequests\nimport\nos\nmodel_id\n=\n\"your_model_id\"\nbaseten_api_key\n=\nos.environ[\n\"BASETEN_API_KEY\"\n]\nresp\n=\nrequests.post(\nf\n\"https://model-\n{\nmodel_id\n}\n.api.baseten.co/development/predict\"\n,\nheaders\n=\n{\n\"Authorization\"\n:\nf\n\"Api-Key\n{\nbaseten_api_key\n}\n\"\n},\njson\n=\n{\n\"messages\"\n: [\n\"What is AGI?\"\n]}\n)\nprint\n(resp.json())\n\u200b\n8. Live Reload for Development\nAvoid long deploy times when testing changes\u2014use\nlive reload\n:\nCopy\nAsk AI\ntruss\nwatch\nSaves time by\npatching only the updated code\nSkips rebuilding Docker containers\nKeeps the model server running while iterating\nMake changes to\nmodel.py\n, save, and test the API again.\n\u200b\n9.", "mimetype": "text/plain", "start_char_idx": 291151, "end_char_idx": 294240, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4c798e2c-ceca-4a11-ad12-d0f1ef390274": {"__data__": {"id_": "4c798e2c-ceca-4a11-ad12-d0f1ef390274", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0c38e4bb-32e3-42a8-b310-51f2e43ed43e", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "b227ae2631a4fa982948f07cd796d6691c9ad02375cbd6d9df45d79e4434a808", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2fb9f671-7b94-49c7-b67d-33381b9687f4", "node_type": "1", "metadata": {}, "hash": "d68dab97df479efae49b32214a50ea45adbe1800689c45f8a4ef00243c6b91e2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Copy\nAsk AI\nimport\nrequests\nimport\nos\nmodel_id\n=\n\"your_model_id\"\nbaseten_api_key\n=\nos.environ[\n\"BASETEN_API_KEY\"\n]\nresp\n=\nrequests.post(\nf\n\"https://model-\n{\nmodel_id\n}\n.api.baseten.co/development/predict\"\n,\nheaders\n=\n{\n\"Authorization\"\n:\nf\n\"Api-Key\n{\nbaseten_api_key\n}\n\"\n},\njson\n=\n{\n\"messages\"\n: [\n\"What is AGI?\"\n]}\n)\nprint\n(resp.json())\n\u200b\n8. Live Reload for Development\nAvoid long deploy times when testing changes\u2014use\nlive reload\n:\nCopy\nAsk AI\ntruss\nwatch\nSaves time by\npatching only the updated code\nSkips rebuilding Docker containers\nKeeps the model server running while iterating\nMake changes to\nmodel.py\n, save, and test the API again.\n\u200b\n9. Promote to Production\nOnce you\u2019re happy with the model, deploy it to production:\nCopy\nAsk AI\ntruss\npush\n--publish\nThis updates the\nAPI endpoint\nfrom:\n\u274c\nDevelopment\n: /development/predict\n\u2705\nProduction\n: /production/predict\nCopy\nAsk AI\nresp\n=\nrequests.post(\nf\n\"https://model-\n{\nmodel_id\n}\n.api.baseten.co/production/predict\"\n,\nheaders\n=\n{\n\"Authorization\"\n:\nf\n\"Api-Key\n{\nbaseten_api_key\n}\n\"\n},\njson\n=\n{\n\"messages\"\n: [\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"What is AGI?\"\n}\n],\n}\n)\n\u200b\nNext Steps\n\ud83d\ude80 You\u2019ve successfully packaged, deployed, and invoked an AI model with Truss!\nExplore more:\nLearning more about\nmodel serving with Truss\n.\nExample implementations\nfor dozens of open source models.\nInference examples\nand\nBaseten integrations\n.\nUsing\nautoscaling settings\nto spin up and down multiple GPU replicas.\nWas this page helpful?\nYes\nNo\nPrevious\nFast LLMs with TensorRT-LLM\nOptimize LLMs for low latency and high throughput\nNext\nOn this page\n1. Setup\n2. Create a Truss\n3. Load model weights\n4. Implement Model Inference\n5. Configure Dependencies & GPU\nSet Dependencies\nAllocate a GPU\n6. Deploy the Model\n1. Get Your API Key\n2. Push Your Model to Baseten\n7. Call the Model API\n8. Live Reload for Development\n9. Promote to Production\nNext Steps\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 293595, "end_char_idx": 295542, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2fb9f671-7b94-49c7-b67d-33381b9687f4": {"__data__": {"id_": "2fb9f671-7b94-49c7-b67d-33381b9687f4", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4c798e2c-ceca-4a11-ad12-d0f1ef390274", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "68c6e47da02c2a48dd54718c702ff77b62b183fea0971f198567401c10b8b9cf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e523a814-0c35-43b8-a648-ac76fe673583", "node_type": "1", "metadata": {}, "hash": "dbc29dad0ec5650d04158a6cd38d456287a19458698a11c30a506a508380ecb2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Explore more:\nLearning more about\nmodel serving with Truss\n.\nExample implementations\nfor dozens of open source models.\nInference examples\nand\nBaseten integrations\n.\nUsing\nautoscaling settings\nto spin up and down multiple GPU replicas.\nWas this page helpful?\nYes\nNo\nPrevious\nFast LLMs with TensorRT-LLM\nOptimize LLMs for low latency and high throughput\nNext\nOn this page\n1. Setup\n2. Create a Truss\n3. Load model weights\n4. Implement Model Inference\n5. Configure Dependencies & GPU\nSet Dependencies\nAllocate a GPU\n6. Deploy the Model\n1. Get Your API Key\n2. Push Your Model to Baseten\n7. Call the Model API\n8. Live Reload for Development\n9. Promote to Production\nNext Steps\nAssistant\nResponses are generated using AI and may contain mistakes.\n\n\nContent from https://docs.baseten.co/examples/docker:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nExamples\nOverview\nDeploy your first model\nFast LLMs with TensorRT-LLM\nRun any LLM with vLLM\nDeploy LLMs with SGLang\nRAG pipeline with Chains\nTranscribe audio with Chains\nImage generation\nDeploy a ComfyUI project\nEmbeddings with BEI\nDockerized model\nLLM with Streaming\nText to speech\nModel library\nOverview\nDeepseek\nLlama\nQwen\nGemma\nStable Diffusion\nFlux\nKokoro\nMicrosoft\nNomic\nWhisper\nMars\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nExamples\nDockerized model\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nView on Github\nIn this example, we deploy a dockerized model for\ninfinity embedding server\n, a high-throughput, low-latency REST API server for serving vector embeddings.\n\u200b\nSetting up the\nconfig.yaml\nTo deploy a dockerized model, all you need is a\nconfig.yaml\n. It specifies how to build your Docker image, start the server, and manage resources. Let\u2019s break down each section.\n\u200b\nBase Image\nSets the foundational Docker image to a lightweight Python 3.11 environment.\nconfig.yaml\nCopy\nAsk AI\nbase_image\n:\nimage\n:\npython:3.11-slim\n\u200b\nDocker Server Configuration\nConfigures the server\u2019s startup command, health check endpoints, prediction endpoint, and the port on which the server will run.\nconfig.yaml\nCopy\nAsk AI\ndocker_server\n:\nstart_command\n:\nsh -c \"HF_TOKEN=$(cat /secrets/hf_access_token) infinity_emb v2 --batch-size 64 --model-id BAAI/bge-small-en-v1.5 --revision main\"\nreadiness_endpoint\n:\n/health\nliveness_endpoint\n:\n/health\npredict_endpoint\n:\n/embeddings\nserver_port\n:\n7997\n\u200b\nBuild Commands (Optional)\nPre-downloads model weights during the build phase to ensure the model is ready at container startup.\nconfig.yaml\nCopy\nAsk AI\nbuild_commands\n:\n# optional step to download the weights of the model into the image\n-\nsh -c \"HF_TOKEN=$(cat /secrets/hf_access_token) infinity_emb v2 --preload-only --no-model-warmup --model-id BAAI/bge-small-en-v1.5 --revision main\"\n\u200b\nConfigure resources\nNote that we need an L4 to run this model.\nconfig.yaml\nCopy\nAsk AI\nresources\n:\naccelerator\n:\nL4\nuse_gpu\n:\ntrue\n\u200b\nRequirements\nLists the Python package dependencies required for the infinity embedding server.\nconfig.yaml\nCopy\nAsk AI\nrequirements\n:\n-\ninfinity-emb[all]==0.0.72\n\u200b\nRuntime Settings\nSets the server to handle up to 40 concurrent inferences to manage load efficiently.\nconfig.yaml\nCopy\nAsk AI\nruntime\n:\npredict_concurrency\n:\n40\n\u200b\nEnvironment Variables\nDefines essential environment variables including the Hugging Face access token, request batch size, queue size limit, and a flag to disable tracking.\nconfig.yaml\nCopy\nAsk AI\nenvironment_variables\n:\nhf_access_token\n:\nnull\n# constrain api to at most 256 sentences per request, for better load-balancing\nINFINITY_MAX_CLIENT_BATCH_SIZE\n:\n256\n# constrain model to a max backpressure of INFINITY_MAX_CLIENT_BATCH_SIZE * predict_concurrency = 10241 requests\nINFINITY_QUEUE_SIZE\n:\n10241\nDO_NOT_TRACK\n:\n1\n\u200b\nDeploy dockerized model\nDeploy the model like you would other Trusses, with:\nCopy\nAsk AI\ntruss\npush\ninfinity-embedding-server\n--publish\nWas this page helpful?\nYes\nNo\nPrevious\nLLM with Streaming\nBuilding an LLM with streaming output\nNext\nOn this page\nSetting up the config.yaml\nBase Image\nDocker Server Configuration\nBuild Commands (Optional)\nConfigure resources\nRequirements\nRuntime Settings\nEnvironment Variables\nDeploy dockerized model\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 294803, "end_char_idx": 299075, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e523a814-0c35-43b8-a648-ac76fe673583": {"__data__": {"id_": "e523a814-0c35-43b8-a648-ac76fe673583", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2fb9f671-7b94-49c7-b67d-33381b9687f4", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "c69ce3a262d9e723a5657a2552403e7e39e7e2221d12435497b49686ab03e226", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "771aaf0c-15b4-45b5-bbe6-e18793a984cc", "node_type": "1", "metadata": {}, "hash": "410da40afb82eba54a21dc22437c622fead9eef3c5f271fd084554bba70721a0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Content from https://docs.baseten.co/examples/image-generation:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nExamples\nOverview\nDeploy your first model\nFast LLMs with TensorRT-LLM\nRun any LLM with vLLM\nDeploy LLMs with SGLang\nRAG pipeline with Chains\nTranscribe audio with Chains\nImage generation\nDeploy a ComfyUI project\nEmbeddings with BEI\nDockerized model\nLLM with Streaming\nText to speech\nModel library\nOverview\nDeepseek\nLlama\nQwen\nGemma\nStable Diffusion\nFlux\nKokoro\nMicrosoft\nNomic\nWhisper\nMars\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nExamples\nImage generation\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nView example on GitHub\nIn this example, we go through a Truss that serves a text-to-image model. We\nuse Flux Schnell, which is one of the highest performing text-to-image models out\nthere today.\n\u200b\nSet up imports and torch settings\nIn this example, we use the Hugging Face diffusers library to build our text-to-image model.\nmodel/model.py\nCopy\nAsk AI\nimport\nbase64\nimport\nrandom\nimport\nlogging\nfrom\nio\nimport\nBytesIO\nimport\nnumpy\nas\nnp\nimport\ntorch\nfrom\ndiffusers\nimport\nFluxPipeline\nfrom\nPIL\nimport\nImage\nlogging.basicConfig(\nlevel\n=\nlogging.\nINFO\n)\nMAX_SEED\n=\nnp.iinfo(np.int32).max\n\u200b\nDefine the\nModel\nclass and load function\nIn the\nload\nfunction of the Truss, we implement logic involved in\ndownloading and setting up the model. For this model, we use the\nFluxPipeline\nclass in\ndiffusers\nto instantiate our Flux pipeline,\nand configure a number of relevant parameters.\nSee the\ndiffusers docs\nfor details\non all of these parameters.\nmodel/model.py\nCopy\nAsk AI\nclass\nModel\n:\ndef\n__init__\n(\nself\n,\n**\nkwargs\n):\nself\n.pipe\n=\nNone\nself\n.repo_id\n=\n\"black-forest-labs/FLUX.1-schnell\"\ndef\nload\n(\nself\n):\nself\n.pipe\n=\nFluxPipeline.from_pretrained(\nself\n.repo_id,\ntorch_dtype\n=\ntorch.bfloat16).to(\n\"cuda\"\n)\nThis is a utility function for converting a PIL image to base64.\nmodel/model.py\nCopy\nAsk AI\ndef\nconvert_to_b64\n(\nself\n,\nimage\n: Image) ->\nstr\n:\nbuffered\n=\nBytesIO()\nimage.save(buffered,\nformat\n=\n\"JPEG\"\n)\nimg_b64\n=\nbase64.b64encode(buffered.getvalue()).decode(\n\"utf-8\"\n)\nreturn\nimg_b64\n\u200b\nDefine the predict function\nThe\npredict\nfunction contains the actual inference logic. The steps here are:\nSetting up the generation params. These include things like the prompt, image width, image height, number of inference steps, etc.", "mimetype": "text/plain", "start_char_idx": 299078, "end_char_idx": 301480, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "771aaf0c-15b4-45b5-bbe6-e18793a984cc": {"__data__": {"id_": "771aaf0c-15b4-45b5-bbe6-e18793a984cc", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e523a814-0c35-43b8-a648-ac76fe673583", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "91ff152e73d0b5fed20beef3f7fafbe5ca0570457b27cedea61a429f7f9e1a7a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d532e660-adfd-4fec-8715-7ffb22cb8528", "node_type": "1", "metadata": {}, "hash": "41ffa1cc6c8bb86cb5049c96fc661942a6b1090593feec28ffa0fd9dd7561b25", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "model/model.py\nCopy\nAsk AI\ndef\nconvert_to_b64\n(\nself\n,\nimage\n: Image) ->\nstr\n:\nbuffered\n=\nBytesIO()\nimage.save(buffered,\nformat\n=\n\"JPEG\"\n)\nimg_b64\n=\nbase64.b64encode(buffered.getvalue()).decode(\n\"utf-8\"\n)\nreturn\nimg_b64\n\u200b\nDefine the predict function\nThe\npredict\nfunction contains the actual inference logic. The steps here are:\nSetting up the generation params. These include things like the prompt, image width, image height, number of inference steps, etc.\nRunning the Diffusion Pipeline\nConvert the resulting image to base64 and return it\nmodel/model.py\nCopy\nAsk AI\ndef\npredict\n(\nself\n,\nmodel_input\n):\nseed\n=\nmodel_input.get(\n\"seed\"\n)\nprompt\n=\nmodel_input.get(\n\"prompt\"\n)\nprompt2\n=\nmodel_input.get(\n\"prompt2\"\n)\nmax_sequence_length\n=\nmodel_input.get(\n\"max_sequence_length\"\n,\n256\n)\n# 256 is max for FLUX.1-schnell\nguidance_scale\n=\nmodel_input.get(\n\"guidance_scale\"\n,\n0.0\n)\n# 0.0 is the only value for FLUX.1-schnell\nnum_inference_steps\n=\nmodel_input.get(\n\"num_inference_steps\"\n,\n4\n)\n# schnell is timestep-distilled\nwidth\n=\nmodel_input.get(\n\"width\"\n,\n1024\n)\nheight\n=\nmodel_input.get(\n\"height\"\n,\n1024\n)\nif\nnot\nmath.isclose(guidance_scale,\n0.0\n):\nlogging.warning(\n\"FLUX.1-schnell does not support guidance_scale other than 0.0\"\n)\nguidance_scale\n=\n0.0\nif\nnot\nseed:\nseed\n=\nrandom.randint(\n0\n,\nMAX_SEED\n)\nif\nlen\n(prompt.split())\n>\nmax_sequence_length:\nlogging.warning(\n\"FLUX.1-schnell does not support prompts longer than 256 tokens, truncating\"\n)\ntokens\n=\nprompt.split()\nprompt\n=\n\" \"\n.join(tokens[:\nmin\n(\nlen\n(tokens), max_sequence_length)])\ngenerator\n=\ntorch.Generator().manual_seed(seed)\nimage\n=\nself\n.pipe(\nprompt\n=\nprompt,\nguidance_scale\n=\nguidance_scale,\nmax_sequence_length\n=\nmax_sequence_length,\nnum_inference_steps\n=\nnum_inference_steps,\nwidth\n=\nwidth,\nheight\n=\nheight,\noutput_type\n=\n\"pil\"\n,\ngenerator\n=\ngenerator,\n).images[\n0\n]\nb64_results\n=\nself\n.convert_to_b64(image)\nreturn\n{\n\"data\"\n: b64_results}\n\u200b\nSetting up the\nconfig.yaml\nRunning Flux Schnell requires a handful of Python libraries, including\ndiffusers\n,\ntransformers\n, and others.\nconfig.yaml\nCopy\nAsk AI\nexternal_package_dirs\n: []\nmodel_cache\n:\n-\nrepo_id\n:\nblack-forest-labs/FLUX.1-schnell\nallow_patterns\n:\n-\n\"*.json\"\n-\n\"*.safetensors\"\nignore_patterns\n:\n-\n\"flux1-schnell.safetensors\"\nmodel_metadata\n:\nexample_model_input\n: {\n\"prompt\"\n:\n'black forest gateau cake spelling out the words \"FLUX SCHNELL\", tasty, food photography, dynamic shot'\n}\nmodel_name\n:\nFlux.1-schnell\npython_version\n:\npy311\nrequirements\n:\n-\ngit+https://github.com/huggingface/diffusers.git@v0.32.2\n-\ntransformers\n-\naccelerate\n-\nsentencepiece\n-\nprotobuf\nresources\n:\naccelerator\n:\nH100_40GB\nuse_gpu\n:\ntrue\nsecrets\n: {}\nsystem_packages\n:\n-\nffmpeg\n-\nlibsm6\n-\nlibxext6\n\u200b\nConfiguring resources for Flux Schnell\nNote that we need an H100 40GB GPU to run this model.\nconfig.yaml\nCopy\nAsk AI\nresources\n:\naccelerator\n:\nH100_40GB\nuse_gpu\n:\ntrue\nsecrets\n: {}\n\u200b\nSystem Packages\nRunning diffusers requires\nffmpeg\nand a couple other system\npackages.\nconfig.yaml\nCopy\nAsk AI\nsystem_packages\n:\n-\nffmpeg\n-\nlibsm6\n-\nlibxext6\n\u200b\nEnabling Caching\nFlux Schnell is a large model, and downloading it could take several minutes. This means\nthat the cold start time for this model is long. We can solve that by using our build\ncaching feature.", "mimetype": "text/plain", "start_char_idx": 301022, "end_char_idx": 304272, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d532e660-adfd-4fec-8715-7ffb22cb8528": {"__data__": {"id_": "d532e660-adfd-4fec-8715-7ffb22cb8528", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "771aaf0c-15b4-45b5-bbe6-e18793a984cc", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "827a3c0f749abab7d3982896c61d13998ef834cf962c9bf3e5391995db365c35", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7097d46d-5944-4b03-8834-601745bf7d7f", "node_type": "1", "metadata": {}, "hash": "9012d0db2712ef6abc62cba834065d8c079fe3f054f0bf4134d0a689b3eba3f0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "config.yaml\nCopy\nAsk AI\nresources\n:\naccelerator\n:\nH100_40GB\nuse_gpu\n:\ntrue\nsecrets\n: {}\n\u200b\nSystem Packages\nRunning diffusers requires\nffmpeg\nand a couple other system\npackages.\nconfig.yaml\nCopy\nAsk AI\nsystem_packages\n:\n-\nffmpeg\n-\nlibsm6\n-\nlibxext6\n\u200b\nEnabling Caching\nFlux Schnell is a large model, and downloading it could take several minutes. This means\nthat the cold start time for this model is long. We can solve that by using our build\ncaching feature. This moves the model download to the build stage of your model\u2014\ncaching the model will take about 15 minutes initially but you will get ~20s cold starts\nsubsequently.\nTo enable caching, add the following to the config:\nCopy\nAsk AI\nmodel_cache\n:\n-\nrepo_id\n:\nblack-forest-labs/FLUX.1-schnell\nallow_patterns\n:\n-\n\"*.json\"\n-\n\"*.safetensors\"\nignore_patterns\n:\n-\n\"flux1-schnell.safetensors\"\n\u200b\nDeploy the model\nDeploy the model like you would other Trusses, with:\nCopy\nAsk AI\ntruss\npush\nflux/schnell\n--publish\n\u200b\nRun an inference\nUse a Python script to call the model once its deployed and parse its response. We parse the resulting base64-encoded string output into an actual image file:\noutput_image.jpg\n.\ninfer.py\nCopy\nAsk AI\nimport\nhttpx\nimport\nos\nimport\nbase64\nfrom\nPIL\nimport\nImage\nfrom\nio\nimport\nBytesIO\n# Replace the empty string with your model id below\nmodel_id\n=\n\"\"\nbaseten_api_key\n=\nos.environ[\n\"BASETEN_API_KEY\"\n]\n# Function used to convert a base64 string to a PIL image\ndef\nb64_to_pil\n(\nb64_str\n):\nreturn\nImage.open(BytesIO(base64.b64decode(b64_str)))\ndata\n=\n{\n\"prompt\"\n:\n'red velvet cake spelling out the words \"FLUX SCHNELL\", tasty, food photography, dynamic shot'\n}\n# Call model endpoint\nres\n=\nhttpx.post(\nf\n\"https://model-\n{\nmodel_id\n}\n.api.baseten.co/production/predict\"\n,\nheaders\n=\n{\n\"Authorization\"\n:\nf\n\"Api-Key\n{\nbaseten_api_key\n}\n\"\n},\njson\n=\ndata\n)\n# Get output image\nres\n=\nres.json()\noutput\n=\nres.get(\n\"data\"\n)\n# Convert the base64 model output to an image\nimg\n=\nb64_to_pil(output)\nimg.save(\n\"output_image.jpg\"\n)\nWas this page helpful?\nYes\nNo\nPrevious\nDeploy a ComfyUI project\nDeploy your ComfyUI workflow as an API endpoint\nNext\nOn this page\nSet up imports and torch settings\nDefine the Model class and load function\nDefine the predict function\nSetting up the config.yaml\nConfiguring resources for Flux Schnell\nSystem Packages\nEnabling Caching\nDeploy the model\nRun an inference\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 303815, "end_char_idx": 306236, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7097d46d-5944-4b03-8834-601745bf7d7f": {"__data__": {"id_": "7097d46d-5944-4b03-8834-601745bf7d7f", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d532e660-adfd-4fec-8715-7ffb22cb8528", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "4f797921ecf584e4d0f46c99f2770bd00415e7bece560be1eea949a1883980ea", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e331819e-6621-42c7-80d8-993fb11103b1", "node_type": "1", "metadata": {}, "hash": "5e8978048119a0f147d9832941fdd3ecc862284be958d8eb5aff4ed42ee8d92b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Yes\nNo\nPrevious\nDeploy a ComfyUI project\nDeploy your ComfyUI workflow as an API endpoint\nNext\nOn this page\nSet up imports and torch settings\nDefine the Model class and load function\nDefine the predict function\nSetting up the config.yaml\nConfiguring resources for Flux Schnell\nSystem Packages\nEnabling Caching\nDeploy the model\nRun an inference\nAssistant\nResponses are generated using AI and may contain mistakes.\n\n\nContent from https://docs.baseten.co/examples/models/deepseek/deepseek-r1:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nExamples\nOverview\nDeploy your first model\nFast LLMs with TensorRT-LLM\nRun any LLM with vLLM\nDeploy LLMs with SGLang\nRAG pipeline with Chains\nTranscribe audio with Chains\nImage generation\nDeploy a ComfyUI project\nEmbeddings with BEI\nDockerized model\nLLM with Streaming\nText to speech\nModel library\nOverview\nDeepseek\nDeepseek R1\nDeepSeek-R1 Qwen 7B\nLlama\nQwen\nGemma\nStable Diffusion\nFlux\nKokoro\nMicrosoft\nNomic\nWhisper\nMars\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nDeepseek\nDeepseek R1\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nDeploy Deepseek R1\n\u200b\nExample usage\nDeepSeek-R1 is optimized using SGLang and uses an OpenAI-compatible API endpoint.\n\u200b\nInput\nCopy\nAsk AI\nimport\nhttpx\nimport\nos\nMODEL_ID\n=\n\"abcd1234\"\n# Replace with your model ID\nDEPLOYMENT_ID\n=\n\"abcd1234\"\n# [Optional] Replace with your deployment ID\nAPI_KEY\n=\nos.environ[\n\"BASETEN_API_KEY\"\n]\nresp\n=\nhttpx.post(\nf\n\"https://model-\n{\nMODEL_ID\n}\n.api.baseten.co/environments/production/sync/v1/chat/completions\"\n,\nheaders\n=\n{\n\"Authorization\"\n:\nf\n\"Api-Key\n{\nAPI_KEY\n}\n\"\n},\njson\n=\n{\n\"model\"\n:\n\"deepseek_v3\"\n,\n\"messages\"\n: [\n{\n\"role\"\n:\n\"system\"\n,\n\"content\"\n:\n\"You are a helpful AI assistant.\"\n},\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"What weighs more, a pound of bricks or a pound of feathers?\"\n},\n],\n\"max_tokens\"\n:\n1024\n,\n},\ntimeout\n=\nNone\n)\nprint\n(resp.json())\n\u200b\nOutput\nCopy\nAsk AI\n{\n\"id\"\n:\n\"8456fe51db3548789f199cfb8c8efd35\"\n,\n\"object\"\n:\n\"text_completion\"\n,\n\"created\"\n:\n1735236968\n,\n\"model\"\n:\n\"/models/deepseek_r1\"\n,\n\"choices\"\n: [\n{\n\"index\"\n:\n0\n,\n\"text\"\n:\n\"Let's think through this step by step...\"\n,\n\"logprobs\"\n:\nnull\n,\n\"finish_reason\"\n:\n\"stop\"\n,\n\"matched_stop\"\n:\n1\n}\n],\n\"usage\"\n: {\n\"prompt_tokens\"\n:\n14\n,\n\"total_tokens\"\n:\n240\n,\n\"completion_tokens\"\n:\n226\n,\n\"prompt_tokens_details\"\n:\nnull\n}\n}\nWas this page helpful?\nYes\nNo\nPrevious\nDeepSeek-R1 Qwen 7B\nQwen 7B fine-tuned for CoT reasoning capabilities with DeepSeek R1\nNext\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 305825, "end_char_idx": 308372, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e331819e-6621-42c7-80d8-993fb11103b1": {"__data__": {"id_": "e331819e-6621-42c7-80d8-993fb11103b1", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7097d46d-5944-4b03-8834-601745bf7d7f", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "1821da0bde954dd8c1ecefaa2c65643a9bdceb7d4f40a84e3380edd1fec8e180", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "84fe1523-aa71-40ee-9f02-284730404b4b", "node_type": "1", "metadata": {}, "hash": "660a999ffc5b4ae1b2088dfe33bf472c7f37c7efd9bab4c4aa47c2194a3ae6c1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Content from https://docs.baseten.co/examples/models/deepseek/deepseek-r1-qwen-7b:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nExamples\nOverview\nDeploy your first model\nFast LLMs with TensorRT-LLM\nRun any LLM with vLLM\nDeploy LLMs with SGLang\nRAG pipeline with Chains\nTranscribe audio with Chains\nImage generation\nDeploy a ComfyUI project\nEmbeddings with BEI\nDockerized model\nLLM with Streaming\nText to speech\nModel library\nOverview\nDeepseek\nDeepseek R1\nDeepSeek-R1 Qwen 7B\nLlama\nQwen\nGemma\nStable Diffusion\nFlux\nKokoro\nMicrosoft\nNomic\nWhisper\nMars\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nDeepseek\nDeepSeek-R1 Qwen 7B\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nDeploy DeepSeek-R1 Qwen 7B\n\u200b\nExample usage\nThe fine-tuned version of Qwen is OpenAI compatible and can be called using the OpenAI client.\nCopy\nAsk AI\nimport\nos\nfrom\nopenai\nimport\nOpenAI\n# https://model-XXXXXXX.api.baseten.co/environments/production/sync/v1\nmodel_url\n=\n\"\"\nclient\n=\nOpenAI(\nbase_url\n=\nmodel_url,\napi_key\n=\nos.environ.get(\n\"BASETEN_API_KEY\"\n),\n)\nstream\n=\nclient.chat.completions.create(\nmodel\n=\n\"baseten\"\n,\nmessages\n=\n[\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"Which weighs more, a pound of bricks or a pound of feathers?\"\n},\n],\nstream\n=\nTrue\n,\n)\nfor\nchunk\nin\nstream:\nif\nchunk.choices[\n0\n].delta.content\nis\nnot\nNone\n:\nprint\n(chunk.choices[\n0\n].delta.content,\nend\n=\n\"\"\n)\n\u200b\nJSON output\nCopy\nAsk AI\n[\n\"streaming\"\n,\n\"output\"\n,\n\"text\"\n]\nWas this page helpful?\nYes\nNo\nPrevious\nLlama 3.3 70B Instruct\nLlama 3.3 70B Instruct is a large language model that is optimized for instruction following.\nNext\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 308375, "end_char_idx": 310075, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "84fe1523-aa71-40ee-9f02-284730404b4b": {"__data__": {"id_": "84fe1523-aa71-40ee-9f02-284730404b4b", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e331819e-6621-42c7-80d8-993fb11103b1", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "1026dd992719faa8c4023b343165544f9b111ee8eb062a6282a4b8bd4d396da4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "83ef953c-4b55-4200-a1a0-36c4c18fa0e7", "node_type": "1", "metadata": {}, "hash": "d8ba6e54befe5c694261f34830a542eaf380695b5b5d7ac3e2f0ad682502f617", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Content from https://docs.baseten.co/examples/models/flux/flux-schnell:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nExamples\nOverview\nDeploy your first model\nFast LLMs with TensorRT-LLM\nRun any LLM with vLLM\nDeploy LLMs with SGLang\nRAG pipeline with Chains\nTranscribe audio with Chains\nImage generation\nDeploy a ComfyUI project\nEmbeddings with BEI\nDockerized model\nLLM with Streaming\nText to speech\nModel library\nOverview\nDeepseek\nLlama\nQwen\nGemma\nStable Diffusion\nFlux\nFlux-Schnell\nKokoro\nMicrosoft\nNomic\nWhisper\nMars\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nFlux\nFlux-Schnell\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nDeploy Flux-Schnell\n\u200b\nExample usage\nThe model accepts a\nprompt\nwhich is some text describing the image you want to generate. The output images tend to get better as you add more descriptive words to the prompt.\nThe output JSON object contains a key called\ndata\nwhich represents the generated image as a base64 string.\n\u200b\nInput\nCopy\nAsk AI\nimport\nhttpx\nimport\nos\nimport\nbase64\nfrom\nPIL\nimport\nImage\nfrom\nio\nimport\nBytesIO\n# Replace the empty string with your model id below\nmodel_id\n=\n\"\"\nbaseten_api_key\n=\nos.environ[\n\"BASETEN_API_KEY\"\n]\n# Function used to convert a base64 string to a PIL image\ndef\nb64_to_pil\n(\nb64_str\n):\nreturn\nImage.open(BytesIO(base64.b64decode(b64_str)))\ndata\n=\n{\n\"prompt\"\n:\n'red velvet cake spelling out the words \"FLUX SCHNELL\", tasty, food photography, dynamic shot'\n}\n# Call model endpoint\nres\n=\nhttpx.post(\nf\n\"https://model-\n{\nmodel_id\n}\n.api.baseten.co/production/predict\"\n,\nheaders\n=\n{\n\"Authorization\"\n:\nf\n\"Api-Key\n{\nbaseten_api_key\n}\n\"\n},\njson\n=\ndata\n)\n# Get output image\nres\n=\nres.json()\noutput\n=\nres.get(\n\"data\"\n)\n# Convert the base64 model output to an image\nimg\n=\nb64_to_pil(output)\nimg.save(\n\"output_image.jpg\"\n)\n\u200b\nJSON output\nCopy\nAsk AI\n{\n\"output\"\n:\n\"iVBORw0KGgoAAAANSUhEUgAABAAAAAQACAIAAA...\"\n}\nWas this page helpful?\nYes\nNo\nPrevious\nKokoro\nKokoro is a frontier TTS model for its size of 82 million parameters (text in/audio out).\nNext\nOn this page\nExample usage\nInput\nJSON output\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 310078, "end_char_idx": 312256, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "83ef953c-4b55-4200-a1a0-36c4c18fa0e7": {"__data__": {"id_": "83ef953c-4b55-4200-a1a0-36c4c18fa0e7", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "84fe1523-aa71-40ee-9f02-284730404b4b", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "611ffc298a4c5b447d8fe49c9b5f7e2f9b2f9f69fac1dbb11062402361e81ab6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5dff88aa-0d11-4688-a1a4-605ff622dbd4", "node_type": "1", "metadata": {}, "hash": "266df31ab9e9df7a187eb804d6e480d642ea29a58793506d9586fa4bda530934", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Content from https://docs.baseten.co/examples/models/gemma/gemma-3-27b-it:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nExamples\nOverview\nDeploy your first model\nFast LLMs with TensorRT-LLM\nRun any LLM with vLLM\nDeploy LLMs with SGLang\nRAG pipeline with Chains\nTranscribe audio with Chains\nImage generation\nDeploy a ComfyUI project\nEmbeddings with BEI\nDockerized model\nLLM with Streaming\nText to speech\nModel library\nOverview\nDeepseek\nLlama\nQwen\nGemma\nGemma 3 27B IT\nStable Diffusion\nFlux\nKokoro\nMicrosoft\nNomic\nWhisper\nMars\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nGemma\nGemma 3 27B IT\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nDeploy Gemma 3 27B IT\n\u200b\nExample usage\nGemma 3 is an OpenAI-compatible model and can be called using the OpenAI SDK in any language.\nCopy\nAsk AI\nfrom\nopenai\nimport\nOpenAI\nimport\nos\nmodel_url\n=\n\"\"\n# Copy in from API pane in Baseten model dashboard\nclient\n=\nOpenAI(\napi_key\n=\nos.environ[\n'BASETEN_API_KEY'\n],\nbase_url\n=\nmodel_url\n)\n# Chat completion\nresponse_chat\n=\nclient.chat.completions.create(\nmodel\n=\n\"\"\n,\nmessages\n=\n[{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n: [\n{\n\"type\"\n:\n\"text\"\n,\n\"text\"\n:\n\"What's in this image?\"\n},\n{\n\"type\"\n:\n\"image_url\"\n,\n\"image_url\"\n: {\n\"url\"\n:\n\"https://picsum.photos/id/237/200/300\"\n,\n},\n},\n],\n}],\ntemperature\n=\n0.3\n,\nmax_tokens\n=\n512\n,\n)\nprint\n(response_chat)\nJSON Output\nCopy\nAsk AI\n{\n\"id\"\n:\n\"143\"\n,\n\"choices\"\n: [\n{\n\"finish_reason\"\n:\n\"stop\"\n,\n\"index\"\n:\n0\n,\n\"logprobs\"\n:\nnull\n,\n\"message\"\n: {\n\"content\"\n:\n\"[Model output here]\"\n,\n\"role\"\n:\n\"assistant\"\n,\n\"audio\"\n:\nnull\n,\n\"function_call\"\n:\nnull\n,\n\"tool_calls\"\n:\nnull\n}\n}\n],\n\"created\"\n:\n1741224586\n,\n\"model\"\n:\n\"\"\n,\n\"object\"\n:\n\"chat.completion\"\n,\n\"service_tier\"\n:\nnull\n,\n\"system_fingerprint\"\n:\nnull\n,\n\"usage\"\n: {\n\"completion_tokens\"\n:\n145\n,\n\"prompt_tokens\"\n:\n38\n,\n\"total_tokens\"\n:\n183\n,\n\"completion_tokens_details\"\n:\nnull\n,\n\"prompt_tokens_details\"\n:\nnull\n}\n}\nWas this page helpful?\nYes\nNo\nPrevious\nSDXL Lightning\nA variant of Stable Diffusion XL that generates 1024x1024 px images in 4 UNet steps, enabling near real-time image creation.\nNext\nOn this page\nExample usage\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 312259, "end_char_idx": 314457, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5dff88aa-0d11-4688-a1a4-605ff622dbd4": {"__data__": {"id_": "5dff88aa-0d11-4688-a1a4-605ff622dbd4", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "83ef953c-4b55-4200-a1a0-36c4c18fa0e7", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "aa1ee82c5ba2d5ad982b91daaaa6bbb0ead972c32a4e5bee953fb3f4038e276f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a299032c-9a01-403d-ae35-126803f5e4e6", "node_type": "1", "metadata": {}, "hash": "e1f40f2021af638569ff83af058081348d9ef2487d16731e7e5112ccde68bb96", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Content from https://docs.baseten.co/examples/models/kokoro/kokoro:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nExamples\nOverview\nDeploy your first model\nFast LLMs with TensorRT-LLM\nRun any LLM with vLLM\nDeploy LLMs with SGLang\nRAG pipeline with Chains\nTranscribe audio with Chains\nImage generation\nDeploy a ComfyUI project\nEmbeddings with BEI\nDockerized model\nLLM with Streaming\nText to speech\nModel library\nOverview\nDeepseek\nLlama\nQwen\nGemma\nStable Diffusion\nFlux\nKokoro\nKokoro\nMicrosoft\nNomic\nWhisper\nMars\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nKokoro\nKokoro\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nDeploy Kokoro\n\u200b\nExample usage\nKokoro uses the following request and response format:\nCopy\nAsk AI\nrequest:\n{\"text\": \"Hello\", \"voice\": \"af\", \"speed\": 1.0}\ntext: str = defaults to \"Hi, I'm kokoro\"\nvoice: str = defaults to \"af\", available options: \"af\", \"af_bella\", \"af_sarah\", \"am_adam\", \"am_michael\", \"bf_emma\", \"bf_isabella\", \"bm_george\", \"bm_lewis\", \"af_nicole\", \"af_sky\"\nspeed: float = defaults to 1.0. The speed of the audio generated\nresponse:\n{\"base64\": \"base64 encoded bytestring\"}\nCopy\nAsk AI\nimport\nhttpx\nimport\nbase64\n# Replace the empty string with your model id below\nmodel_id\n=\n\"\"\nbaseten_api_key\n=\nos.environ[\n\"BASETEN_API_KEY\"\n]\nwith\nhttpx.Client()\nas\nclient:\n# Make the API request\nresp\n=\nclient.post(\nf\n\"https://model-\n{\nmodel_id\n}\n.api.baseten.co/production/predict\"\n,\nheaders\n=\n{\n\"Authorization\"\n:\nf\n\"Api-Key\n{\nAPI_KEY\n}\n\"\n},\njson\n=\n{\n\"text\"\n:\n\"Hello world\"\n,\n\"voice\"\n:\n\"af\"\n,\n\"speed\"\n:\n1.0\n},\ntimeout\n=\nNone\n,\n)\n# Get the base64 encoded audio\nresponse_data\n=\nresp.json()\naudio_base64\n=\nresponse_data[\n\"base64\"\n]\n# Decode the base64 string\naudio_bytes\n=\nbase64.b64decode(audio_base64)\n# Write to a WAV file\nwith\nopen\n(\n\"output.wav\"\n,\n\"wb\"\n)\nas\nf:\nf.write(audio_bytes)\nprint\n(\n\"Audio saved to output.wav\"\n)\nJSON Output\nCopy\nAsk AI\nnull\nWas this page helpful?\nYes\nNo\nPrevious\nAll MPNet Base V2\nA text embedding model with a context window of 384 tokens and a dimensionality of 768 values.\nNext\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 314460, "end_char_idx": 316615, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a299032c-9a01-403d-ae35-126803f5e4e6": {"__data__": {"id_": "a299032c-9a01-403d-ae35-126803f5e4e6", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5dff88aa-0d11-4688-a1a4-605ff622dbd4", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "3eee98bd172e575ac4da8c410c0e43f64f591c82c812475fc9d12f413320f1c8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d59c3720-e45d-4bd5-bc13-e16de66ce155", "node_type": "1", "metadata": {}, "hash": "871d842c1834b8c7e51c14f4e60bd9da0ef73a530f9f6f60309abba9fc728cae", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Content from https://docs.baseten.co/examples/models/llama/llama-3.3-70B-instruct:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nExamples\nOverview\nDeploy your first model\nFast LLMs with TensorRT-LLM\nRun any LLM with vLLM\nDeploy LLMs with SGLang\nRAG pipeline with Chains\nTranscribe audio with Chains\nImage generation\nDeploy a ComfyUI project\nEmbeddings with BEI\nDockerized model\nLLM with Streaming\nText to speech\nModel library\nOverview\nDeepseek\nLlama\nLlama 3.3 70B Instruct\nQwen\nGemma\nStable Diffusion\nFlux\nKokoro\nMicrosoft\nNomic\nWhisper\nMars\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nLlama\nLlama 3.3 70B Instruct\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nDeploy Llama 3.3 70B Instruct\n\u200b\nExample usage\nLlama is OpenAI compatible and can be called using the OpenAI client.\nCopy\nAsk AI\nimport\nos\nfrom\nopenai\nimport\nOpenAI\n# https://model-XXXXXXX.api.baseten.co/environments/production/sync/v1\nmodel_url\n=\n\"\"\nclient\n=\nOpenAI(\nbase_url\n=\nmodel_url,\napi_key\n=\nos.environ.get(\n\"BASETEN_API_KEY\"\n),\n)\nstream\n=\nclient.chat.completions.create(\nmodel\n=\n\"baseten\"\n,\nmessages\n=\n[\n{\n\"role\"\n:\n\"system\"\n,\n\"content\"\n:\n\"You are a helpful assistant.\"\n},\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"What was the role of Llamas in the Inca empire?\"\n}\n],\nstream\n=\nTrue\n,\n)\nfor\nchunk\nin\nstream:\nif\nchunk.choices[\n0\n].delta.content\nis\nnot\nNone\n:\nprint\n(chunk.choices[\n0\n].delta.content,\nend\n=\n\"\"\n)\nJSON Output\nCopy\nAsk AI\n[\n\"streaming\"\n,\n\"output\"\n,\n\"text\"\n]\nWas this page helpful?\nYes\nNo\nPrevious\nQwen-2-5-32B-Coder-Instruct\nQwen 2.5 32B Coder is an OpenAI-compatible model and can be called using the OpenAI SDK in any language.\nNext\nOn this page\nExample usage\nAssistant\nResponses are generated using AI and may contain mistakes.\n\n\nContent from https://docs.baseten.co/examples/models/mars/MARS6:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nExamples\nOverview\nDeploy your first model\nFast LLMs with TensorRT-LLM\nRun any LLM with vLLM\nDeploy LLMs with SGLang\nRAG pipeline with Chains\nTranscribe audio with Chains\nImage generation\nDeploy a ComfyUI project\nEmbeddings with BEI\nDockerized model\nLLM with Streaming\nText to speech\nModel library\nOverview\nDeepseek\nLlama\nQwen\nGemma\nStable Diffusion\nFlux\nKokoro\nMicrosoft\nNomic\nWhisper\nMars\nMARS6\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nMars\nMARS6\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nDeploy MARS6\n\u200b\nExample usage\nThis model requires at least four inputs:\ntext\n: The input text that needs to be spoken\naudio_ref\n: An audio file containing the audio of a single person\nref_text\n: What is spoken in audio_ref\nlanguage\n: The language code for the target language\nThe model will try to output an audio stream containing the speech in the reference audio\u2019s style. The output is by default an HTTP1.1 chunked encoding response of an encoded audio file using an ADTS AAC stream, but can be configured to stream using flac format, or to not stream at all and return the entire response as a base64 encoded flac file.\nCopy\nAsk AI\ndata = {\"text\": \"The quick brown fox jumps over the lazy dog\",\n\"audio_ref\": encoded_str,\n\"ref_text\": prompt_txt,\n\"language\": 'en-us', # Target language, in this case english.", "mimetype": "text/plain", "start_char_idx": 316618, "end_char_idx": 319866, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d59c3720-e45d-4bd5-bc13-e16de66ce155": {"__data__": {"id_": "d59c3720-e45d-4bd5-bc13-e16de66ce155", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a299032c-9a01-403d-ae35-126803f5e4e6", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "b14b61cafc23ec00da9328c9e7a19d2188d73bb62ab6572ff7cbec9c8fbeb6c2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "34063b81-c7fa-4e45-a358-58c98359efe0", "node_type": "1", "metadata": {}, "hash": "11510a27291c9777571bc7708d2ea15fc008474e2fb44b32b0a53b1e16ff283e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The output is by default an HTTP1.1 chunked encoding response of an encoded audio file using an ADTS AAC stream, but can be configured to stream using flac format, or to not stream at all and return the entire response as a base64 encoded flac file.\nCopy\nAsk AI\ndata = {\"text\": \"The quick brown fox jumps over the lazy dog\",\n\"audio_ref\": encoded_str,\n\"ref_text\": prompt_txt,\n\"language\": 'en-us', # Target language, in this case english.\n# \"top_p\": 0.7, # Optionally specify a top_p (default 0.7)\n# \"temperature\": 0.7, # Optionally specify a temperature (default 0.7)\n# \"chunk_length\": 200, # Optional text chunk length for splitting long pieces of input text. Default 200\n# \"max_new_tokens\": 0, # Optional limit on max number of new tokens, default is zero (unlimited)\n# \"repetition_penalty\": 1.5 # Optional rep penalty, default 1.5\n}\n\u200b\nInput\nCopy\nAsk AI\nimport\nbase64\nimport\ntime\nimport\ntorchaudio\nimport\nrequests\nimport\nIPython.display\nas\nipd\nimport\nlibrosa, librosa.display\nimport\ntorch\nimport\nio\nfrom\ntorchaudio.io\nimport\nStreamReader\n# Step 1: set endpoint url and api key:\nurl\n=\n\"<YOUR PREDICTION ENDPOINT>\"\nheaders\n=\n{\n\"Authorization\"\n:\n\"Api-Key <YOUR API KEY>\"\n}\n# Step 2: pick reference audio to clone, encode it as base64\nfile_path\n=\n\"ref_debug.flac\"\n# any valid audio filepath, ideally between 6s-90s.\nwav, sr\n=\nlibrosa.load(file_path,\nsr\n=\nNone\n,\nmono\n=\nTrue\n,\noffset\n=\n0\n,\nduration\n=\n5\n)\nio_data\n=\nio.BytesIO()\ntorchaudio.save(io_data, torch.from_numpy(wav)[\nNone\n],\nsample_rate\n=\nsr,\nformat\n=\n\"wav\"\n)\nio_data.seek(\n0\n)\nencoded_data\n=\nbase64.b64encode(io_data.read())\nencoded_str\n=\nencoded_data.decode(\n\"utf-8\"\n)\n# OPTIONAL: specify the transcript of the reference/prompt (slightly speeds up inference, and may make it sound a bit better).\nprompt_txt\n=\nNone\n# if unspecified, can be left as None\n# Step 3: define other inference settings:\ndata\n=\n{\n\"text\"\n:\n\"The quick brown fox jumps over the lazy dog\"\n,\n\"audio_ref\"\n: encoded_str,\n\"ref_text\"\n: prompt_txt,\n\"language\"\n:\n\"en-us\"\n,\n# Target language, in this case english.\n# \"top_p\": 0.7, # Optionally specify a top_p (default 0.7)\n# \"temperature\": 0.7, # Optionally specify a temperature (default 0.7)\n# \"chunk_length\": 200, # Optional text chunk length for splitting long pieces of input text. Default 200\n# \"max_new_tokens\": 0, # Optional limit on max number of new tokens, default is zero (unlimited)\n# \"repetition_penalty\": 1.5 # Optional rep penalty, default 1.5\n# stream: bool = True # whether to stream the response back as an HTTP1.1 chunked encoding response, or run to completion and return the base64 encoded file.\n# stream_format: str = \"adts\" # 'adts' or 'flac' for stream format.", "mimetype": "text/plain", "start_char_idx": 319430, "end_char_idx": 322085, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "34063b81-c7fa-4e45-a358-58c98359efe0": {"__data__": {"id_": "34063b81-c7fa-4e45-a358-58c98359efe0", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d59c3720-e45d-4bd5-bc13-e16de66ce155", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "d2e867f5c9f215ef453940450d2dcf0ccbfa43928e4fdbc98c79ced59a6a2ad0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3b202ff2-8981-41f2-bb59-59d0568b22d0", "node_type": "1", "metadata": {}, "hash": "7678b2fc046e471ef222f6c815a6914b365f6f74a5f2c7c5761f252b95235485", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "# \"top_p\": 0.7, # Optionally specify a top_p (default 0.7)\n# \"temperature\": 0.7, # Optionally specify a temperature (default 0.7)\n# \"chunk_length\": 200, # Optional text chunk length for splitting long pieces of input text. Default 200\n# \"max_new_tokens\": 0, # Optional limit on max number of new tokens, default is zero (unlimited)\n# \"repetition_penalty\": 1.5 # Optional rep penalty, default 1.5\n# stream: bool = True # whether to stream the response back as an HTTP1.1 chunked encoding response, or run to completion and return the base64 encoded file.\n# stream_format: str = \"adts\" # 'adts' or 'flac' for stream format. Default 'adts'\n}\nst\n=\ntime.time()\nclass\nUnseekableWrapper\n:\ndef\n__init__\n(\nself\n,\nobj\n):\nself\n.obj\n=\nobj\ndef\nread\n(\nself\n,\nn\n):\nreturn\nself\n.obj.read(n)\n# Step 4: Send the POST request (note the first request might be a bit slow, but following requests should be fast)\nresponse\n=\nrequests.post(url,\nheaders\n=\nheaders,\njson\n=\ndata,\nstream\n=\nTrue\n,\ntimeout\n=\n300\n)\nstreamer\n=\nStreamReader(UnseekableWrapper(response.raw))\nstreamer.add_basic_audio_stream(\n11025\n,\nbuffer_chunk_size\n=\n3\n,\nsample_rate\n=\n44100\n,\nnum_channels\n=\n1\n)\n# Step 4.1: check the header format of the returned stream response\nfor\ni\nin\nrange\n(streamer.num_src_streams):\nprint\n(streamer.get_src_stream_info(i))\n# Step 5: stream the response back and decode it on-the-fly\naudio_samples\n=\n[]\nfor\nchunks\nin\nstreamer.stream():\naudio_chunk\n=\nchunks[\n0\n]\naudio_samples.append(\naudio_chunk._elem.squeeze()\n)\n# this is now just a (T,) float waveform, however you can set your own output format bove.\nprint\n(\nf\n\"Playing audio chunk of size\n{\naudio_chunk._elem.squeeze().shape\n}\nat\n{\ntime.time()\n-\nst\n:.2f}\ns.\"\n)\n# If you wish, you can also play each chunk as you receive it, e.g. using IPython:\n# ipd.display(ipd.Audio(audio_chunk._elem.squeeze().numpy(), rate=44100, autoplay=True))\n# Step 6: concatenate all the audio chunks and play the full audio (if you didn't play them on the fly above)\nfinal_full_audio\n=\ntorch.concat(audio_samples,\ndim\n=\n0\n)\n# (T,) float waveform @ 44.1kHz\n# ipd.display(ipd.Audio(final_full_audio.numpy(), rate=44100))\n\u200b\nOutput\nCopy\nAsk AI\n{\n\"reuslt\"\n:\n\"base64 encoded audio data\"\n,\n\\\n}\nWas this page helpful?\nYes\nNo\nPrevious\nWhisper V3\nWhisper V3 is a fast and accurate speech recognition model.\nOn this page\nExample usage\nInput\nOutput\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 321464, "end_char_idx": 323875, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3b202ff2-8981-41f2-bb59-59d0568b22d0": {"__data__": {"id_": "3b202ff2-8981-41f2-bb59-59d0568b22d0", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "34063b81-c7fa-4e45-a358-58c98359efe0", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "528867a203f75fc9aa95d989c8549bcf5076ad7176ec38752e0d3fff3c953250", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2e0951a7-b6ff-44ae-907d-de3eab8cd06e", "node_type": "1", "metadata": {}, "hash": "92204876dd82ece34ec7977985e645bd1b42cebc06560301669d9d4aa1c3ad78", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": ")\n# If you wish, you can also play each chunk as you receive it, e.g. using IPython:\n# ipd.display(ipd.Audio(audio_chunk._elem.squeeze().numpy(), rate=44100, autoplay=True))\n# Step 6: concatenate all the audio chunks and play the full audio (if you didn't play them on the fly above)\nfinal_full_audio\n=\ntorch.concat(audio_samples,\ndim\n=\n0\n)\n# (T,) float waveform @ 44.1kHz\n# ipd.display(ipd.Audio(final_full_audio.numpy(), rate=44100))\n\u200b\nOutput\nCopy\nAsk AI\n{\n\"reuslt\"\n:\n\"base64 encoded audio data\"\n,\n\\\n}\nWas this page helpful?\nYes\nNo\nPrevious\nWhisper V3\nWhisper V3 is a fast and accurate speech recognition model.\nOn this page\nExample usage\nInput\nOutput\nAssistant\nResponses are generated using AI and may contain mistakes.\n\n\nContent from https://docs.baseten.co/examples/models/microsoft/all-mpnet-base-v2:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nExamples\nOverview\nDeploy your first model\nFast LLMs with TensorRT-LLM\nRun any LLM with vLLM\nDeploy LLMs with SGLang\nRAG pipeline with Chains\nTranscribe audio with Chains\nImage generation\nDeploy a ComfyUI project\nEmbeddings with BEI\nDockerized model\nLLM with Streaming\nText to speech\nModel library\nOverview\nDeepseek\nLlama\nQwen\nGemma\nStable Diffusion\nFlux\nKokoro\nMicrosoft\nAll MPNet Base V2\nNomic\nWhisper\nMars\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nMicrosoft\nAll MPNet Base V2\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nDeploy All MPNet Base V2\n\u200b\nExample usage\nThis model takes a list of strings and returns a list of embeddings, where each embedding is a list of 768 floating-point number representing the semantic text embedding of the associated string.\nStrings can be up to 384 tokens in length (approximately 280 words). If the strings are longer, they\u2019ll be truncated before being run through the embedding model.\nCopy\nAsk AI\nimport\nrequests\nimport\nos\n# Replace the empty string with your model id below\nmodel_id\n=\n\"\"\nbaseten_api_key\n=\nos.environ[\n\"BASETEN_API_KEY\"\n]\ndata\n=\n{\n\"text\"\n: [\n\"I want to eat pasta\"\n,\n\"I want to eat pizza\"\n],\n}\n# Call model endpoint\nres\n=\nrequests.post(\nf\n\"https://model-\n{\nmodel_id\n}\n.api.baseten.co/production/predict\"\n,\nheaders\n=\n{\n\"Authorization\"\n:\nf\n\"Api-Key\n{\nbaseten_api_key\n}\n\"\n},\njson\n=\ndata\n)\n# Print the output of the model\nprint\n(res.json())\n\u200b\nJSON output\nCopy\nAsk AI\n[\n[\n0.2593194842338562\n,\n\"...\"\n,\n-1.4059709310531616\n],\n[\n0.11028853803873062\n,\n\"...\"\n,\n-0.9492666125297546\n]\n]\nWas this page helpful?\nYes\nNo\nPrevious\nNomic Embed v1.5\nSOTA text embedding model with variable dimensionality \u2014 outperforms OpenAI text-embedding-ada-002 and text-embedding-3-small models.\nNext\nOn this page\nExample usage\nJSON output\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 323153, "end_char_idx": 325903, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2e0951a7-b6ff-44ae-907d-de3eab8cd06e": {"__data__": {"id_": "2e0951a7-b6ff-44ae-907d-de3eab8cd06e", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3b202ff2-8981-41f2-bb59-59d0568b22d0", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "492bc0904f36fd4c92adca1dedf4a7f103e7dd2eeb92b5e203173bbdab5ec63e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "351e2982-4e08-4376-b2d5-07f6b0fe0ea3", "node_type": "1", "metadata": {}, "hash": "307c4fb5b771d407898ab3b764866118eead36cea298d9f126da6dd92bebc70f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Content from https://docs.baseten.co/examples/models/nomic/nomic-embed-v1-5:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nExamples\nOverview\nDeploy your first model\nFast LLMs with TensorRT-LLM\nRun any LLM with vLLM\nDeploy LLMs with SGLang\nRAG pipeline with Chains\nTranscribe audio with Chains\nImage generation\nDeploy a ComfyUI project\nEmbeddings with BEI\nDockerized model\nLLM with Streaming\nText to speech\nModel library\nOverview\nDeepseek\nLlama\nQwen\nGemma\nStable Diffusion\nFlux\nKokoro\nMicrosoft\nNomic\nNomic Embed v1.5\nWhisper\nMars\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nNomic\nNomic Embed v1.5\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nDeploy Nomic Embed v1.5\n\u200b\nExample usage\nNomic Embed v1.5 is a state of the art text embedding model with two special features:\nYou can choose whether to optimize the embeddings for retrieval, search, clustering, or classification.\nYou can trade off between cost and accuracy by choosing your own dimensionality thanks to Matryoshka Representation Learning.\nNomic Embed v1.5 takes the following parameters:\ntexts\nthe strings to embed.\ntask_type\nthe task to optimize the embedding for. Can be\nsearch_document\n(default),\nsearch_query\n,\nclustering\n, or\nclassification\n.\ndimensionality\nthe size of each output vector, any integer between\n64\nand\n768\n(default).\nThis code sample demonstrates embedding a set of sentences for retrieval with a dimensionality of 512.\nCopy\nAsk AI\nimport\nrequests\nimport\nos\n# Replace the empty string with your model id below\nmodel_id\n=\n\"\"\nbaseten_api_key\n=\nos.environ[\n\"BASETEN_API_KEY\"\n]\ndata\n=\n{\n\"texts\"\n: [\n\"I want to eat pasta\"\n,\n\"I want to eat pizza\"\n],\n\"task_type\"\n:\n\"search_document\"\n,\n\"dimensionality\"\n:\n512\n}\n# Call model endpoint\nres\n=\nrequests.post(\nf\n\"https://model-\n{\nmodel_id\n}\n.api.baseten.co/production/predict\"\n,\nheaders\n=\n{\n\"Authorization\"\n:\nf\n\"Api-Key\n{\nbaseten_api_key\n}\n\"\n},\njson\n=\ndata\n)\n# Print the output of the model\nprint\n(res.json())\n\u200b\nJSON output\nCopy\nAsk AI\n[\n[\n-0.03811980411410332\n,\n\"...\"\n,\n-0.023593541234731674\n],\n[\n-0.042617011815309525\n,\n\"...\"\n,\n-0.0191882885992527\n]\n]\nWas this page helpful?\nYes\nNo\nPrevious\nWhisper V3\nWhisper V3 is a fast and accurate speech recognition model.\nNext\nOn this page\nExample usage\nJSON output\nAssistant\nResponses are generated using AI and may contain mistakes.\n\n\nContent from https://docs.baseten.co/examples/models/overview:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nExamples\nOverview\nDeploy your first model\nFast LLMs with TensorRT-LLM\nRun any LLM with vLLM\nDeploy LLMs with SGLang\nRAG pipeline with Chains\nTranscribe audio with Chains\nImage generation\nDeploy a ComfyUI project\nEmbeddings with BEI\nDockerized model\nLLM with Streaming\nText to speech\nModel library\nOverview\nDeepseek\nLlama\nQwen\nGemma\nStable Diffusion\nFlux\nKokoro\nMicrosoft\nNomic\nWhisper\nMars\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nModel library\nOverview\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\n\u200b\nFeatured models\nDeepSeek R1\nWhisper V3\nQwen 2.5 32B Coder Instruct\nLlama 3.3 70B Instruct\nflux-schnell\nGemma 3 27B IT\nMARS6\nWas this page helpful?\nYes\nNo\nPrevious\nDeepseek R1\nA state-of-the-art 671B-parameter MoE LLM with o1-style reasoning licensed for commercial use\nNext\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 325906, "end_char_idx": 329284, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "351e2982-4e08-4376-b2d5-07f6b0fe0ea3": {"__data__": {"id_": "351e2982-4e08-4376-b2d5-07f6b0fe0ea3", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2e0951a7-b6ff-44ae-907d-de3eab8cd06e", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "5c8a0057a36848abd87832c06e50225b090dc9d3155cbeb81378d68d28a1153d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3a122c60-bfbf-46c9-b35a-13ec08eac9e3", "node_type": "1", "metadata": {}, "hash": "427168fce4d137649265c37be8753dd3ccffb315a81b5ac9a94284b70a73bd9d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Content from https://docs.baseten.co/examples/models/qwen/qwen-2-5-32b-coder-instruct:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nExamples\nOverview\nDeploy your first model\nFast LLMs with TensorRT-LLM\nRun any LLM with vLLM\nDeploy LLMs with SGLang\nRAG pipeline with Chains\nTranscribe audio with Chains\nImage generation\nDeploy a ComfyUI project\nEmbeddings with BEI\nDockerized model\nLLM with Streaming\nText to speech\nModel library\nOverview\nDeepseek\nLlama\nQwen\nQwen-2-5-32B-Coder-Instruct\nGemma\nStable Diffusion\nFlux\nKokoro\nMicrosoft\nNomic\nWhisper\nMars\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nQwen\nQwen-2-5-32B-Coder-Instruct\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nDeploy Qwen 2.5 32B Coder Instruct\n\u200b\nExample usage\nCopy\nAsk AI\nfrom\nopenai\nimport\nOpenAI\nimport\nos\nmodel_url\n=\n\"\"\n# Copy in from API pane in Baseten model dashboard\nclient\n=\nOpenAI(\napi_key\n=\nos.environ[\n'BASETEN_API_KEY'\n],\nbase_url\n=\nmodel_url\n)\n# Chat completion\nresponse_chat\n=\nclient.chat.completions.create(\nmodel\n=\n\"\"\n,\nmessages\n=\n[\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"Tell me a fun fact about Python.\"\n}\n],\ntemperature\n=\n0.3\n,\nmax_tokens\n=\n100\n,\n)\nprint\n(response_chat)\n\u200b\nJSON output\nCopy\nAsk AI\n{\n\"id\"\n:\n\"143\"\n,\n\"choices\"\n: [\n{\n\"finish_reason\"\n:\n\"stop\"\n,\n\"index\"\n:\n0\n,\n\"logprobs\"\n:\nnull\n,\n\"message\"\n: {\n\"content\"\n:\n\"[Model output here]\"\n,\n\"role\"\n:\n\"assistant\"\n,\n\"audio\"\n:\nnull\n,\n\"function_call\"\n:\nnull\n,\n\"tool_calls\"\n:\nnull\n}\n}\n],\n\"created\"\n:\n1741224586\n,\n\"model\"\n:\n\"\"\n,\n\"object\"\n:\n\"chat.completion\"\n,\n\"service_tier\"\n:\nnull\n,\n\"system_fingerprint\"\n:\nnull\n,\n\"usage\"\n: {\n\"completion_tokens\"\n:\n145\n,\n\"prompt_tokens\"\n:\n38\n,\n\"total_tokens\"\n:\n183\n,\n\"completion_tokens_details\"\n:\nnull\n,\n\"prompt_tokens_details\"\n:\nnull\n}\n}\nWas this page helpful?\nYes\nNo\nPrevious\nGemma 3 27B IT\nInstruct-tuned open model by Google with excellent ELO/size tradeoff and vision capabilities\nNext\nOn this page\nExample usage\nJSON output\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 329287, "end_char_idx": 331303, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3a122c60-bfbf-46c9-b35a-13ec08eac9e3": {"__data__": {"id_": "3a122c60-bfbf-46c9-b35a-13ec08eac9e3", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "351e2982-4e08-4376-b2d5-07f6b0fe0ea3", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "e2165f35cec99adb08e2aab75b0d3f56aaf571fa6114856555e65188e416ae03", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3dabc1d5-12a5-495f-9491-958d353cfa83", "node_type": "1", "metadata": {}, "hash": "c2bfd1137694d948238e3bb1b4e0c021ddbb614cd12b159014490a013909ea6b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Content from https://docs.baseten.co/examples/models/stable-diffusion/sdxl-lightning:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nExamples\nOverview\nDeploy your first model\nFast LLMs with TensorRT-LLM\nRun any LLM with vLLM\nDeploy LLMs with SGLang\nRAG pipeline with Chains\nTranscribe audio with Chains\nImage generation\nDeploy a ComfyUI project\nEmbeddings with BEI\nDockerized model\nLLM with Streaming\nText to speech\nModel library\nOverview\nDeepseek\nLlama\nQwen\nGemma\nStable Diffusion\nSDXL Lightning\nFlux\nKokoro\nMicrosoft\nNomic\nWhisper\nMars\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nStable Diffusion\nSDXL Lightning\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nDeploy SDXL Lightning\n\u200b\nExample usage\nThe model accepts a single input, prompt, and returns a base64 string of the image as the key\nresult\n.\nThis implementation uses the 4-step UNet checkpoint to balance speed and quality. You can\ndeploy your own version\nwith either 2 steps for even faster results or 8 steps for even higher quality.\nCopy\nAsk AI\nimport\nbase64\nimport\nrequests\nimport\nos\n# Replace the empty string with your model id below\nmodel_id\n=\n\"\"\nbaseten_api_key\n=\nos.environ[\n\"BASETEN_API_KEY\"\n]\nBASE64_PREAMBLE\n=\n\"data:image/png;base64,\"\ndata\n=\n{\n\"prompt\"\n:\n\"a picture of a rhino wearing a suit\"\n,\n}\n# Call model endpoint\nres\n=\nrequests.post(\nf\n\"https://model-\n{\nmodel_id\n}\n.api.baseten.co/production/predict\"\n,\nheaders\n=\n{\n\"Authorization\"\n:\nf\n\"Api-Key\n{\nbaseten_api_key\n}\n\"\n},\njson\n=\ndata\n)\n# Get output image\nres\n=\nres.json()\nimg_b64\n=\nres.get(\n\"result\"\n)\nimg\n=\nbase64.b64decode(img_b64)\n# Save the base64 string to a PNG\nimg_file\n=\nopen\n(\n\"sdxl-output-1.png\"\n,\n\"wb\"\n)\nimg_file.write(img)\nimg_file.close()\nos.system(\n\"open sdxl-output-1.png\"\n)\nJSON Output\nCopy\nAsk AI\n{\n\"result\"\n:\n\"iVBORw0KGgoAAAANSUhEUgAABAAAAAQACAIAAA...\"\n}\nWas this page helpful?\nYes\nNo\nPrevious\nFlux-Schnell\nFlux-Schnell is a state-of-the-art image generation model\nNext\nOn this page\nExample usage\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 331306, "end_char_idx": 333375, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3dabc1d5-12a5-495f-9491-958d353cfa83": {"__data__": {"id_": "3dabc1d5-12a5-495f-9491-958d353cfa83", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3a122c60-bfbf-46c9-b35a-13ec08eac9e3", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "55db1b9672d3545571250a0d2586f3a223bac765416d9b462c047e4196415796", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "15b806c2-18b8-4c03-ace2-107df8e142df", "node_type": "1", "metadata": {}, "hash": "0c7ee93e95805c1b9dd52ed982b6fc482df2f9fe44e140e925be8fbadbb19c07", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Content from https://docs.baseten.co/examples/models/whisper/whisper-v3-fastest:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nExamples\nOverview\nDeploy your first model\nFast LLMs with TensorRT-LLM\nRun any LLM with vLLM\nDeploy LLMs with SGLang\nRAG pipeline with Chains\nTranscribe audio with Chains\nImage generation\nDeploy a ComfyUI project\nEmbeddings with BEI\nDockerized model\nLLM with Streaming\nText to speech\nModel library\nOverview\nDeepseek\nLlama\nQwen\nGemma\nStable Diffusion\nFlux\nKokoro\nMicrosoft\nNomic\nWhisper\nWhisper V3\nMars\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nWhisper\nWhisper V3\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nDeploy Whisper V3\n\u200b\nExample usage\nTranscribe audio files at up to a 400x real-time factor \u2014 that\u2019s 1 hour of audio in under 9 seconds. This setup requires meaningful production traffic to be cost-effective, but at scale it\u2019s at least 80% cheaper than OpenAI.\nGet in touch with us\nand we\u2019ll work with you to deploy a transcription pipeline that\u2019s customized to match your needs.\nFor quick deployments of Whisper suitable for shorter audio files and lower traffic volume, you can deploy Whisper V3 and Whisper V3 Turbo directly from the model library.\nCopy\nAsk AI\nimport\nrequests\nimport\nos\n# Model ID for production deployment\nmodel_id\n=\n\"\"\n# Read secrets from environment variables\nbaseten_api_key\n=\nos.environ[\n\"BASETEN_API_KEY\"\n]\n# Call model endpoint\nresp\n=\nrequests.post(\nf\n\"https://model-\n{\nmodel_id\n}\n.api.baseten.co/production/predict\"\n,\nheaders\n=\n{\n\"Authorization\"\n:\nf\n\"Api-Key\n{\nbaseten_api_key\n}\n\"\n},\njson\n=\n{\n\"url\"\n:\n\"https://www2.cs.uic.edu/~i101/SoundFiles/gettysburg10.wav\"\n,\n}\n)\nprint\n(resp.content.decode(\n\"utf-8\"\n))\nJSON Output\nCopy\nAsk AI\n{\n\"segments\"\n: [\n{\n\"start\"\n:\n0\n,\n\"end\"\n:\n9.8\n,\n\"text\"\n:\n\"Four score and seven years ago, our fathers brought forth on this continent a new nation, conceived in liberty and dedicated to the proposition that all men are created equal.\"\n}\n],\n\"language_code\"\n:\n\"en\"\n}\nWas this page helpful?\nYes\nNo\nPrevious\nMARS6\nMARS6 is a frontier text-to-speech model by CAMB.AI with voice/prosody cloning capabilities in 10 languages. MARS6 must be licensed for commercial use, we can help!\nNext\nOn this page\nExample usage\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 333378, "end_char_idx": 335708, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "15b806c2-18b8-4c03-ace2-107df8e142df": {"__data__": {"id_": "15b806c2-18b8-4c03-ace2-107df8e142df", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3dabc1d5-12a5-495f-9491-958d353cfa83", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "a4ad305c67c4e24c8b9d477f611c4da58a5dc41320b8269458a5f3ff554a2b1e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c1ffbf9c-cf78-4ec0-bb21-996eee6485b0", "node_type": "1", "metadata": {}, "hash": "71b44fc70f441a54b0cf660f6ca3e3867b072effc5252654463a5d4060bd3900", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Content from https://docs.baseten.co/examples/overview:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nExamples\nOverview\nDeploy your first model\nFast LLMs with TensorRT-LLM\nRun any LLM with vLLM\nDeploy LLMs with SGLang\nRAG pipeline with Chains\nTranscribe audio with Chains\nImage generation\nDeploy a ComfyUI project\nEmbeddings with BEI\nDockerized model\nLLM with Streaming\nText to speech\nModel library\nOverview\nDeepseek\nLlama\nQwen\nGemma\nStable Diffusion\nFlux\nKokoro\nMicrosoft\nNomic\nWhisper\nMars\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nExamples\nBuilding with Baseten\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nThese examples cover a variety of use cases on Baseten, from\ndeploying your first LLM\nand\nimage generation\nto\ntranscription\n,\nembeddings\n, and\nRAG pipelines\n. Whether you\u2019re optimizing inference with\nTensorRT-LLM\nor deploying a model with\nTruss\n, these guides help you build and scale efficiently.\n\u200b\nFeatured examples\nDeploy your first model\nFast LLMs with TensorRT-LLM\nRun any LLM with vLLM\nDeploy LLMs with SGLang\nTranscribe audio with a Chain\nEmbeddings with BEI\n\u200b\nModel library\nFor a\nquick start\n, explore the\nmodel library\nwith prebuilt, ready to deploy in one click models like DeepSeek, Llama, and Qwen.\nDeepSeek R1\nWhisper V3\nQwen 2.5 32B Coder Instruct\nLlama 3.3 70B Instruct\nflux-schnell\nMARS6\nWas this page helpful?\nYes\nNo\nDeploy your first model\nFrom model weights to API endpoint\nNext\nAssistant\nResponses are generated using AI and may contain mistakes.\n\n\nContent from https://docs.baseten.co/examples/sglang:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nExamples\nOverview\nDeploy your first model\nFast LLMs with TensorRT-LLM\nRun any LLM with vLLM\nDeploy LLMs with SGLang\nRAG pipeline with Chains\nTranscribe audio with Chains\nImage generation\nDeploy a ComfyUI project\nEmbeddings with BEI\nDockerized model\nLLM with Streaming\nText to speech\nModel library\nOverview\nDeepseek\nLlama\nQwen\nGemma\nStable Diffusion\nFlux\nKokoro\nMicrosoft\nNomic\nWhisper\nMars\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nExamples\nDeploy LLMs with SGLang\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nAnother great option for inference is\nSGlang\n, which supports a wide range of models and performance optimizations. Besides TensorRT-LLM it is in many cases the state-of-the-art engine for serving LLMs.\n\u200b\nExample: Deploy Qwen 2.5 3B on an A10G via SGLang\nThis configuration serves\nQwen 2.5 3B\nwith SGLang on an A10G GPU. Running this model is fast and cheap, making it a good example for documentation, but the process of deploying it is very similar to larger models like\nLlama 3.3 70B\n.\n\u200b\nSetup\nBefore you deploy a model, you\u2019ll need three quick setup steps.\n1\nCreate an API key for your Baseten account\nCreate an\nAPI key\nand save it as an environment variable:\nCopy\nAsk AI\nexport\nBASETEN_API_KEY\n=\n\"abcd.123456\"\n2\nAdd an access token for Hugging Face\nSome models require that you accept terms and conditions on Hugging Face before deployment. To prevent issues:\nAccept the license for any gated models you wish to access, like\nLlama 3.3\n.\nCreate a read-only\nuser access token\nfrom your Hugging Face account.\nAdd the\nhf_access_token\nsecret\nto your Baseten workspace\n.", "mimetype": "text/plain", "start_char_idx": 335711, "end_char_idx": 339001, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c1ffbf9c-cf78-4ec0-bb21-996eee6485b0": {"__data__": {"id_": "c1ffbf9c-cf78-4ec0-bb21-996eee6485b0", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "15b806c2-18b8-4c03-ace2-107df8e142df", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "46fbf6d1f2337dd87716aa49d53999320467ea3e7de06d75e9e024a6a9376b05", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7c9c8724-b47f-4f88-8b81-e68a4fe0b324", "node_type": "1", "metadata": {}, "hash": "1398f84e5ef9021273c344bb6cc2e3b0758e1228250266b022bfd67a22b42dc8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Running this model is fast and cheap, making it a good example for documentation, but the process of deploying it is very similar to larger models like\nLlama 3.3 70B\n.\n\u200b\nSetup\nBefore you deploy a model, you\u2019ll need three quick setup steps.\n1\nCreate an API key for your Baseten account\nCreate an\nAPI key\nand save it as an environment variable:\nCopy\nAsk AI\nexport\nBASETEN_API_KEY\n=\n\"abcd.123456\"\n2\nAdd an access token for Hugging Face\nSome models require that you accept terms and conditions on Hugging Face before deployment. To prevent issues:\nAccept the license for any gated models you wish to access, like\nLlama 3.3\n.\nCreate a read-only\nuser access token\nfrom your Hugging Face account.\nAdd the\nhf_access_token\nsecret\nto your Baseten workspace\n.\n3\nInstall Truss in your local development environment\nInstall the latest version of Truss, our open-source model packaging framework, as well as OpenAI\u2019s model inference SDK, with:\nCopy\nAsk AI\npip\ninstall\n--upgrade\ntruss\nopenai\n\u200b\nConfiguration\nStart with an empty configuration file.\nCopy\nAsk AI\nmkdir\nqwen-2-5-3b-engine\ntouch\nqwen-2-5-3b-engine/config.yaml\nBelow is an example for Qwen 2.5 3B. You can copy-paste it into the empty\nconfig.yaml\nwe created above.\nconfig.yaml\nCopy\nAsk AI\nmodel_metadata\n:\nexample_model_input\n:\n# Loads sample request into Baseten playground\nmessages\n:\n-\nrole\n:\nsystem\ncontent\n:\n\"You are a helpful assistant.\"\n-\nrole\n:\nuser\ncontent\n:\n\"What does Tongyi Qianwen mean?\"\nstream\n:\ntrue\nmodel\n:\n\"baseten-sglang\"\nmax_tokens\n:\n512\ntemperature\n:\n0.6\ntags\n:\n-\nopenai-compatible\nmodel_name\n:\nQwen 2.5 3B SGLang\nenvironment_variables\n:\nhf_access_token\n:\nnull\nbase_image\n:\nimage\n:\nlmsysorg/sglang:v0.4.4.post1-cu125\ndocker_server\n:\nstart_command\n:\nsh -c \"HF_TOKEN=$(cat /secrets/hf_access_token) python3 -m sglang.launch_server --model-path Qwen/Qwen2.5-3B-Instruct --host 0.0.0.0 --port 8000\"\nreadiness_endpoint\n:\n/health\nliveness_endpoint\n:\n/health\npredict_endpoint\n:\n/v1/chat/completions\nserver_port\n:\n8000\nresources\n:\naccelerator\n:\nA10G\nuse_gpu\n:\ntrue\nruntime\n:\npredict_concurrency\n:\n32\n\u200b\nDeployment\nPushing the model to Baseten kicks off a multi-stage deployment process.\nCopy\nAsk AI\ntruss\npush\nqwen-2-5-3b-engine\n--publish\nUpon deployment, check your terminal logs or Baseten account to find the URL for the model server.\n\u200b\nInference\nThis model is OpenAI compatible and can be called using the OpenAI client.\ncall_model.py\nCopy\nAsk AI\nimport\nos\nfrom\nopenai\nimport\nOpenAI\n# https://model-XXXXXXX.api.baseten.co/environments/production/sync/v1\nmodel_url\n=\n\"\"\nclient\n=\nOpenAI(\nbase_url\n=\nmodel_url,\napi_key\n=\nos.environ.get(\n\"BASETEN_API_KEY\"\n),\n)\nstream\n=\nclient.chat.completions.create(\nmodel\n=\n\"baseten\"\n,\nmessages\n=\n[\n{\n\"role\"\n:\n\"system\"\n,\n\"content\"\n:\n\"You are a helpful assistant.\"\n},\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"What does Tongyi Qianwen mean?\"\n}\n],\nstream\n=\nTrue\n,\n)\nfor\nchunk\nin\nstream:\nif\nchunk.choices[\n0\n].delta.content\nis\nnot\nNone\n:\nprint\n(chunk.choices[\n0\n].delta.content,\nend\n=\n\"\"\n)\nThat\u2019s it! You have successfully deployed and called a model using SGLang.\nWas this page helpful?\nYes\nNo\nPrevious\nRAG pipeline with Chains\nBuild a RAG (retrieval-augmented generation) pipeline with  Chains\nNext\nOn this page\nExample: Deploy Qwen 2.5 3B on an A10G via SGLang\nSetup\nConfiguration\nDeployment\nInference\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 338253, "end_char_idx": 341606, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7c9c8724-b47f-4f88-8b81-e68a4fe0b324": {"__data__": {"id_": "7c9c8724-b47f-4f88-8b81-e68a4fe0b324", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c1ffbf9c-cf78-4ec0-bb21-996eee6485b0", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "9160a7c37e58853be6b1f1eef265a86fea0536d54c258cc6dceabee9ec2f9fb1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "24fd3e82-2856-4c41-8dda-bbbd673588fe", "node_type": "1", "metadata": {}, "hash": "4dd18e1fec66098f8eef4926d3b918a48c5cf6eb424535803cd856f490e08007", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "},\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"What does Tongyi Qianwen mean?\"\n}\n],\nstream\n=\nTrue\n,\n)\nfor\nchunk\nin\nstream:\nif\nchunk.choices[\n0\n].delta.content\nis\nnot\nNone\n:\nprint\n(chunk.choices[\n0\n].delta.content,\nend\n=\n\"\"\n)\nThat\u2019s it! You have successfully deployed and called a model using SGLang.\nWas this page helpful?\nYes\nNo\nPrevious\nRAG pipeline with Chains\nBuild a RAG (retrieval-augmented generation) pipeline with  Chains\nNext\nOn this page\nExample: Deploy Qwen 2.5 3B on an A10G via SGLang\nSetup\nConfiguration\nDeployment\nInference\nAssistant\nResponses are generated using AI and may contain mistakes.\n\n\nContent from https://docs.baseten.co/examples/streaming:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nExamples\nOverview\nDeploy your first model\nFast LLMs with TensorRT-LLM\nRun any LLM with vLLM\nDeploy LLMs with SGLang\nRAG pipeline with Chains\nTranscribe audio with Chains\nImage generation\nDeploy a ComfyUI project\nEmbeddings with BEI\nDockerized model\nLLM with Streaming\nText to speech\nModel library\nOverview\nDeepseek\nLlama\nQwen\nGemma\nStable Diffusion\nFlux\nKokoro\nMicrosoft\nNomic\nWhisper\nMars\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nExamples\nLLM with Streaming\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nView on Github\nIn this example, we go through a Truss that serves the Qwen 7B Chat LLM, and streams the output to the client.\n\u200b\nWhy Streaming?\nFor certain ML models, generations can take a long time. Especially with LLMs, a long output could take\n10-20 seconds to generate. However, because LLMs generate tokens in sequence, useful output can be\nmade available to users sooner. To support this, in Truss, we support streaming output.\n\u200b\nSet up the imports\nIn this example, we use the HuggingFace transformers library to build a text generation model.\nmodel/model.py\nCopy\nAsk AI\nfrom\nthreading\nimport\nThread\nfrom\ntyping\nimport\nDict\nimport\ntorch\nfrom\ntransformers\nimport\nAutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer\nfrom\ntransformers.generation\nimport\nGenerationConfig\n\u200b\nDefine the load function\nIn the\nload\nfunction of the Truss, we implement logic\ninvolved in downloading the chat version of the Qwen 7B model and loading it into memory.\nmodel/model.py\nCopy\nAsk AI\nclass\nModel\n:\ndef\n__init__\n(\nself\n,\n**\nkwargs\n):\nself\n.model\n=\nNone\nself\n.tokenizer\n=\nNone\ndef\nload\n(\nself\n):\nself\n.tokenizer\n=\nAutoTokenizer.from_pretrained(\n\"Qwen/Qwen-7B-Chat\"\n,\ntrust_remote_code\n=\nTrue\n)\nself\n.model\n=\nAutoModelForCausalLM.from_pretrained(\n\"Qwen/Qwen-7B-Chat\"\n,\ndevice_map\n=\n\"auto\"\n,\ntrust_remote_code\n=\nTrue\n).eval()\n\u200b\nDefine the preprocess function\nIn the\npreprocess\nfunction of the Truss, we set up a\ngenerate_args\ndictionary with some generation arguments from the inference request to be used in the\npredict\nfunction.\nmodel/model.py\nCopy\nAsk AI\ndef\npreprocess\n(\nself\n,\nrequest\n:\ndict\n) ->\ndict\n:\ngenerate_args\n=\n{\n\"max_new_tokens\"\n: request.get(\n\"max_new_tokens\"\n,\n512\n),\n\"temperature\"\n: request.get(\n\"temperature\"\n,\n0.5\n),\n\"top_p\"\n: request.get(\n\"top_p\"\n,\n0.95\n),\n\"top_k\"\n: request.get(\n\"top_k\"\n,\n40\n),\n\"repetition_penalty\"\n:\n1.0\n,\n\"no_repeat_ngram_size\"\n:\n0\n,\n\"use_cache\"\n:\nTrue\n,\n\"do_sample\"\n:\nTrue\n,\n\"eos_token_id\"\n:\nself\n.tokenizer.eos_token_id,\n\"pad_token_id\"\n:\nself\n.tokenizer.pad_token_id,\n}\nrequest[\n\"generate_args\"\n]\n=\ngenerate_args\nreturn\nrequest\n\u200b\nDefine the predict function\nIn the\npredict\nfunction of the Truss, we implement the actual\ninference logic.", "mimetype": "text/plain", "start_char_idx": 341009, "end_char_idx": 344451, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "24fd3e82-2856-4c41-8dda-bbbd673588fe": {"__data__": {"id_": "24fd3e82-2856-4c41-8dda-bbbd673588fe", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7c9c8724-b47f-4f88-8b81-e68a4fe0b324", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "b555a9dcfae0131de7b0121acfb138869c4f4ebbd8e77945a841b8c164c75e8f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "446825a5-5453-46bd-9a98-2ab522e790c1", "node_type": "1", "metadata": {}, "hash": "a2adcbdb9f401cb4d1e39a61506c739884bbf16878b9eb47fd6ab3881b398f68", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The two main steps are:\nTokenize the input\nCall the model\u2019s\ngenerate\nfunction if we\u2019re not streaming the output, otherwise call the\nstream\nhelper function\nmodel/model.py\nCopy\nAsk AI\ndef\npredict\n(\nself\n,\nrequest\n: Dict):\nstream\n=\nrequest.pop(\n\"stream\"\n,\nFalse\n)\nprompt\n=\nrequest.pop(\n\"prompt\"\n)\ngeneration_args\n=\nrequest.pop(\n\"generate_args\"\n)\ninput_ids\n=\nself\n.tokenizer(prompt,\nreturn_tensors\n=\n\"pt\"\n).input_ids.cuda()\nif\nstream:\nreturn\nself\n.stream(input_ids, generation_args)\nwith\ntorch.no_grad():\noutput\n=\nself\n.model.generate(\ninputs\n=\ninput_ids,\n**\ngeneration_args)\nreturn\nself\n.tokenizer.decode(output[\n0\n])\n\u200b\nDefine the\nstream\nhelper function\nIn this helper function, we\u2019ll instantiate the\nTextIteratorStreamer\nobject, which we\u2019ll later use for\nreturning the LLM output to users.\nmodel/model.py\nCopy\nAsk AI\ndef\nstream\n(\nself\n,\ninput_ids\n:\nlist\n,\ngeneration_args\n:\ndict\n):\nstreamer\n=\nTextIteratorStreamer(\nself\n.tokenizer)\nWhen creating the generation parameters, ensure to pass the\nstreamer\nobject\nthat we created previously.\nmodel/model.py\nCopy\nAsk AI\ngeneration_config\n=\nGenerationConfig(\n**\ngeneration_args)\ngeneration_kwargs\n=\n{\n\"input_ids\"\n: input_ids,\n\"generation_config\"\n: generation_config,\n\"return_dict_in_generate\"\n:\nTrue\n,\n\"output_scores\"\n:\nTrue\n,\n\"max_new_tokens\"\n: generation_args[\n\"max_new_tokens\"\n],\n\"streamer\"\n: streamer,\n}\nSpawn a thread to run the generation, so that it does not block the main\nthread.\nmodel/model.py\nCopy\nAsk AI\nwith\ntorch.no_grad():\n# Begin generation in a separate thread\nthread\n=\nThread(\ntarget\n=\nself\n.model.generate,\nkwargs\n=\ngeneration_kwargs)\nthread.start()\nIn Truss, the way to achieve streaming output is to return a generator\nthat yields content. In this example, we yield the output of the\nstreamer\n,\nwhich produces output and yields it until the generation is complete.\nWe define this\ninner\nfunction to create our generator.\nmodel/model.py\nCopy\nAsk AI\n# Yield generated text as it becomes available\ndef\ninner\n():\nfor\ntext\nin\nstreamer:\nyield\ntext\nthread.join()\nreturn\ninner()\n\u200b\nSetting up the\nconfig.yaml\nRunning Qwen 7B requires torch, transformers,\nand a few other related libraries.\nconfig.yaml\nCopy\nAsk AI\nmodel_name\n:\nqwen-7b-chat\nmodel_metadata\n:\nexample_model_input\n:\nprompt\n:\nWhat is the meaning of life?\nrequirements\n:\n-\naccelerate==0.23.0\n-\ntiktoken==0.5.1\n-\neinops==0.6.1\n-\nscipy==1.11.3\n-\ntransformers_stream_generator==0.0.4\n-\npeft==0.5.0\n-\ndeepspeed==0.11.1\n-\ntorch==2.0.1\n-\ntransformers==4.32.0\n\u200b\nConfigure resources for Qwen\nNote that we need an A10G to run this model.\nconfig.yaml\nCopy\nAsk AI\nresources\n:\naccelerator\n:\nA10G\ncpu\n:\n\"3\"\nmemory\n:\n14Gi\nuse_gpu\n:\ntrue\n\u200b\nDeploy Qwen 7B Chat\nDeploy the model like you would other Trusses, with:\nCopy\nAsk AI\ntruss\npush\nqwen-7b-chat\n--publish\nWas this page helpful?\nYes\nNo\nPrevious\nText to speech\nBuilding a text-to-speech model with Kokoro\nNext\nOn this page\nWhy Streaming?\nSet up the imports\nDefine the load function\nDefine the preprocess function\nDefine the predict function\nDefine the stream helper function\nSetting up the config.yaml\nConfigure resources for Qwen\nDeploy Qwen 7B Chat\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 344452, "end_char_idx": 347620, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "446825a5-5453-46bd-9a98-2ab522e790c1": {"__data__": {"id_": "446825a5-5453-46bd-9a98-2ab522e790c1", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "24fd3e82-2856-4c41-8dda-bbbd673588fe", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "0fb7a0047c99250ad5b79575556957910a1cc448a52d0d15fbd19237b895c56b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "056cd542-3046-419a-ac7b-506e850052c2", "node_type": "1", "metadata": {}, "hash": "0804e4b62c158f0d95fdba4c6a4671179c99866eab6970f84015f67076eff86e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "config.yaml\nCopy\nAsk AI\nresources\n:\naccelerator\n:\nA10G\ncpu\n:\n\"3\"\nmemory\n:\n14Gi\nuse_gpu\n:\ntrue\n\u200b\nDeploy Qwen 7B Chat\nDeploy the model like you would other Trusses, with:\nCopy\nAsk AI\ntruss\npush\nqwen-7b-chat\n--publish\nWas this page helpful?\nYes\nNo\nPrevious\nText to speech\nBuilding a text-to-speech model with Kokoro\nNext\nOn this page\nWhy Streaming?\nSet up the imports\nDefine the load function\nDefine the preprocess function\nDefine the predict function\nDefine the stream helper function\nSetting up the config.yaml\nConfigure resources for Qwen\nDeploy Qwen 7B Chat\nAssistant\nResponses are generated using AI and may contain mistakes.\n\n\nContent from https://docs.baseten.co/examples/tensorrt-llm:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nExamples\nOverview\nDeploy your first model\nFast LLMs with TensorRT-LLM\nRun any LLM with vLLM\nDeploy LLMs with SGLang\nRAG pipeline with Chains\nTranscribe audio with Chains\nImage generation\nDeploy a ComfyUI project\nEmbeddings with BEI\nDockerized model\nLLM with Streaming\nText to speech\nModel library\nOverview\nDeepseek\nLlama\nQwen\nGemma\nStable Diffusion\nFlux\nKokoro\nMicrosoft\nNomic\nWhisper\nMars\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nExamples\nFast LLMs with TensorRT-LLM\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nTo get the best performance, we recommend using our\nTensorRT-LLM Engine Builder\nwhen deploying LLMs. Models deployed with the Engine Builder are\nOpenAI compatible\n, support\nstructured output\nand\nfunction calling\n, and offer deploy-time post-training quantization to FP8 with Hopper GPUs.\nThe Engine Builder supports LLMs from the following families, both foundation models and fine-tunes:\nLlama 3.0 and later (including DeepSeek-R1 distills)\nQwen 2.5 and later (including Math, Coder, and DeepSeek-R1 distills)\nMistral (all LLMs)\nYou can download preset Engine Builder configs for common models from the\nmodel library\n.\nThe Engine Builder does not support vision-language models like\nLlama 3.2 11B\nor\nPixtral\n. For these models, we recommend\nvLLM\n.\n\u200b\nExample: Deploy Qwen 2.5 3B on an H100\nThis configuration builds an inference engine to serve\nQwen 2.5 3B\non an H100 GPU. Running this model is fast and cheap, making it a good example for documentation, but the process of deploying it is very similar to larger models like\nLlama 3.3 70B\n.\n\u200b\nSetup\nBefore you deploy a model, you\u2019ll need three quick setup steps.\n1\nCreate an API key for your Baseten account\nCreate an\nAPI key\nand save it as an environment variable:\nCopy\nAsk AI\nexport\nBASETEN_API_KEY\n=\n\"abcd.123456\"\n2\nAdd an access token for Hugging Face\nSome models require that you accept terms and conditions on Hugging Face before deployment. To prevent issues:\nAccept the license for any gated models you wish to access, like\nLlama 3.3\n.\nCreate a read-only\nuser access token\nfrom your Hugging Face account.\nAdd the\nhf_access_token\nsecret\nto your Baseten workspace\n.\n3\nInstall Truss in your local development environment\nInstall the latest version of Truss, our open-source model packaging framework, as well as OpenAI\u2019s model inference SDK, with:\nCopy\nAsk AI\npip\ninstall\n--upgrade\ntruss\nopenai\n\u200b\nConfiguration\nStart with an empty configuration file.\nCopy\nAsk AI\nmkdir\nqwen-2-5-3b-engine\ntouch\nqwen-2-5-3b-engine/config.yaml\nThis configuration file specifies model information and Engine Builder arguments. You can find dozens of examples in the\nmodel library\nas well as details on each config option in the\nengine builder reference\n.\nBelow is an example for Qwen 2.5 3B.", "mimetype": "text/plain", "start_char_idx": 346993, "end_char_idx": 350535, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "056cd542-3046-419a-ac7b-506e850052c2": {"__data__": {"id_": "056cd542-3046-419a-ac7b-506e850052c2", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "446825a5-5453-46bd-9a98-2ab522e790c1", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "c4d9be4691baa43a2f418cb6a94299d977e7a7541dd76a80a32cd44462695d1d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aa9dfbca-bd79-4ba4-8441-b2395a35d76d", "node_type": "1", "metadata": {}, "hash": "42890f5f73bc88a6d8ef675ab3129386585f972322ebfb70602e8494baa8de8f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Create a read-only\nuser access token\nfrom your Hugging Face account.\nAdd the\nhf_access_token\nsecret\nto your Baseten workspace\n.\n3\nInstall Truss in your local development environment\nInstall the latest version of Truss, our open-source model packaging framework, as well as OpenAI\u2019s model inference SDK, with:\nCopy\nAsk AI\npip\ninstall\n--upgrade\ntruss\nopenai\n\u200b\nConfiguration\nStart with an empty configuration file.\nCopy\nAsk AI\nmkdir\nqwen-2-5-3b-engine\ntouch\nqwen-2-5-3b-engine/config.yaml\nThis configuration file specifies model information and Engine Builder arguments. You can find dozens of examples in the\nmodel library\nas well as details on each config option in the\nengine builder reference\n.\nBelow is an example for Qwen 2.5 3B.\nconfig.yaml\nCopy\nAsk AI\nmodel_metadata\n:\nexample_model_input\n:\n# Loads sample request into Baseten playground\nmessages\n:\n-\nrole\n:\nsystem\ncontent\n:\n\"You are a helpful assistant.\"\n-\nrole\n:\nuser\ncontent\n:\n\"What does Tongyi Qianwen mean?\"\nstream\n:\ntrue\nmax_tokens\n:\n512\ntemperature\n:\n0.6\n# Check recommended temperature per model\nrepo_id\n:\nQwen/Qwen2.5-3B-Instruct\nmodel_name\n:\nQwen 2.5 3B Instruct\npython_version\n:\npy39\nresources\n:\n# Engine Builder GPU cannot be changed post-deployment\naccelerator\n:\nH100\nuse_gpu\n:\ntrue\nsecrets\n: {}\ntrt_llm\n:\nbuild\n:\nbase_model\n:\ndecoder\ncheckpoint_repository\n:\nrepo\n:\nQwen/Qwen2.5-3B-Instruct\nsource\n:\nHF\nnum_builder_gpus\n:\n1\nquantization_type\n:\nno_quant\n# `fp8_kv` often recommended for large models\nmax_seq_len\n:\n32768\n# option to very the max sequence length, e.g. 131072 for Llama models\ntensor_parallel_count\n:\n1\n# Set equal to number of GPUs\nplugin_configuration\n:\nuse_paged_context_fmha\n:\ntrue\nuse_fp8_context_fmha\n:\nfalse\n# Set to true when using `fp8_kv`\npaged_kv_cache\n:\ntrue\nruntime\n:\nbatch_scheduler_policy\n:\nmax_utilization\nenable_chunked_context\n:\ntrue\nrequest_default_max_tokens\n:\n32768\n# 131072 for Llama models\n\u200b\nDeployment\nPushing the model to Baseten kicks off a multi-stage build and deployment process.\nCopy\nAsk AI\ntruss\npush\nqwen-2-5-3b-engine\n--publish\nUpon deployment, check your terminal logs or Baseten account to find the URL for the model server.\n\u200b\nInference\nThis model is OpenAI compatible and can be called using the OpenAI client.\nCopy\nAsk AI\nimport\nos\nfrom\nopenai\nimport\nOpenAI\n# https://model-XXXXXXX.api.baseten.co/environments/production/sync/v1\nmodel_url\n=\n\"\"\nclient\n=\nOpenAI(\nbase_url\n=\nmodel_url,\napi_key\n=\nos.environ.get(\n\"BASETEN_API_KEY\"\n),\n)\nstream\n=\nclient.chat.completions.create(\nmodel\n=\n\"baseten\"\n,\nmessages\n=\n[\n{\n\"role\"\n:\n\"system\"\n,\n\"content\"\n:\n\"You are a helpful assistant.\"\n},\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"What does Tongyi Qianwen mean?\"\n}\n],\nstream\n=\nTrue\n,\n)\nfor\nchunk\nin\nstream:\nif\nchunk.choices[\n0\n].delta.content\nis\nnot\nNone\n:\nprint\n(chunk.choices[\n0\n].delta.content,\nend\n=\n\"\"\n)\nThat\u2019s it! You have successfully deployed and called an LLM optimized with the TensorRT-LLM Engine Builder. Check the\nmodel library\nfor more examples and the\nengine builder reference\nfor details on each config option.\nWas this page helpful?\nYes\nNo\nPrevious\nRun any LLM with vLLM\nServe a wide range of models\nNext\nOn this page\nExample: Deploy Qwen 2.5 3B on an H100\nSetup\nConfiguration\nDeployment\nInference\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 349803, "end_char_idx": 353080, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "aa9dfbca-bd79-4ba4-8441-b2395a35d76d": {"__data__": {"id_": "aa9dfbca-bd79-4ba4-8441-b2395a35d76d", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "056cd542-3046-419a-ac7b-506e850052c2", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "96a89af2c7046dc69fe1737609dabf50ca599ef84d5689a2c09b5c397b25da31", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1ee20a64-de36-4f10-bd0a-0423682edd98", "node_type": "1", "metadata": {}, "hash": "61a2463a298f69d10746f0d1d3bc362b47ab52394c1c52aae3a7ae8094a553d9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "},\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"What does Tongyi Qianwen mean?\"\n}\n],\nstream\n=\nTrue\n,\n)\nfor\nchunk\nin\nstream:\nif\nchunk.choices[\n0\n].delta.content\nis\nnot\nNone\n:\nprint\n(chunk.choices[\n0\n].delta.content,\nend\n=\n\"\"\n)\nThat\u2019s it! You have successfully deployed and called an LLM optimized with the TensorRT-LLM Engine Builder. Check the\nmodel library\nfor more examples and the\nengine builder reference\nfor details on each config option.\nWas this page helpful?\nYes\nNo\nPrevious\nRun any LLM with vLLM\nServe a wide range of models\nNext\nOn this page\nExample: Deploy Qwen 2.5 3B on an H100\nSetup\nConfiguration\nDeployment\nInference\nAssistant\nResponses are generated using AI and may contain mistakes.\n\n\nContent from https://docs.baseten.co/examples/text-to-speech:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nExamples\nOverview\nDeploy your first model\nFast LLMs with TensorRT-LLM\nRun any LLM with vLLM\nDeploy LLMs with SGLang\nRAG pipeline with Chains\nTranscribe audio with Chains\nImage generation\nDeploy a ComfyUI project\nEmbeddings with BEI\nDockerized model\nLLM with Streaming\nText to speech\nModel library\nOverview\nDeepseek\nLlama\nQwen\nGemma\nStable Diffusion\nFlux\nKokoro\nMicrosoft\nNomic\nWhisper\nMars\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nExamples\nText to speech\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nView example on GitHub\nIn this example, we go through a Truss that serves Kokoro, a frontier text-to-speech model.\n\u200b\nSet up imports\nWe import necessary libraries and enable Hugging Face file transfers. We also download the NLTK tokenizer data.\nmodel/model.py\nCopy\nAsk AI\nimport\nlogging\nimport\nos\nos.environ[\n\"HF_HUB_ENABLE_HF_TRANSFER\"\n]\n=\n\"1\"\nimport\nbase64\nimport\nio\nimport\nsys\nimport\ntime\nimport\nnltk\nimport\nnumpy\nas\nnp\nimport\nscipy.io.wavfile\nas\nwav\nimport\ntorch\nfrom\nhuggingface_hub\nimport\nsnapshot_download\nfrom\nnltk.tokenize\nimport\nsent_tokenize\nfrom\nmodels\nimport\nbuild_model\nfrom\nkokoro\nimport\ngenerate\nlogger\n=\nlogging.getLogger(\n__name__\n)\nnltk.download(\n\"punkt\"\n)\n\u200b\nDownloading model weights\nWe need to prepare model weights by doing the following:\nCreate a directory for the model data\nDownload the Kokoro model from Hugging Face into the created model data directory\nAdd the model data directory to the system path\nmodel/model.py\nCopy\nAsk AI\n# Ensure data directory exists\nos.makedirs(\n\"/app/data/Kokoro-82M\"\n,\nexist_ok\n=\nTrue\n)\n# Download model\nsnapshot_download(\nrepo_id\n=\n\"hexgrad/Kokoro-82M\"\n,\nrepo_type\n=\n\"model\"\n,\nrevision\n=\n\"c97b7bbc3e60f447383c79b2f94fee861ff156ac\"\n,\nlocal_dir\n=\n\"/app/data/Kokoro-82M\"\n,\nignore_patterns\n=\n[\n\"*.onnx\"\n,\n\"kokoro-v0_19.pth\"\n,\n\"demo/\"\n],\nmax_workers\n=\n8\n,\n)\n# Add data_dir to the system path\nsys.path.append(\n\"/app/data/Kokoro-82M\"\n)\n\u200b\nDefine the\nModel\nclass and\nload\nfunction\nIn the\nload\nfunction of the Truss, we download and set up the model. This\nload\nfunction handles setting up the device, loading the model weights, and loading the default voice. We also define the available voices.", "mimetype": "text/plain", "start_char_idx": 352392, "end_char_idx": 355399, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1ee20a64-de36-4f10-bd0a-0423682edd98": {"__data__": {"id_": "1ee20a64-de36-4f10-bd0a-0423682edd98", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "aa9dfbca-bd79-4ba4-8441-b2395a35d76d", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "2e3206a05ca0658760b2ce1821f973bc7046b12fbea96f112006f9b16e19ba17", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b55fc3a0-341e-4295-9f27-625ab38c9103", "node_type": "1", "metadata": {}, "hash": "a93389a8fcc1cc82c6416caa2932804ab0051ecdb418ca4a980ae61b8796f28a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This\nload\nfunction handles setting up the device, loading the model weights, and loading the default voice. We also define the available voices.\nmodel/model.py\nCopy\nAsk AI\nclass\nModel\n:\ndef\n__init__\n(\nself\n,\n**\nkwargs\n):\nself\n._data_dir\n=\nkwargs[\n\"data_dir\"\n]\nself\n.model\n=\nNone\nself\n.device\n=\nNone\nself\n.default_voice\n=\nNone\nself\n.voices\n=\nNone\nreturn\ndef\nload\n(\nself\n):\nlogger.info(\n\"Starting setup...\"\n)\nself\n.device\n=\n\"cuda\"\nif\ntorch.cuda.is_available()\nelse\n\"cpu\"\nlogger.info(\nf\n\"Using device:\n{\nself\n.device\n}\n\"\n)\n# Load model\nlogger.info(\n\"Loading model...\"\n)\nmodel_path\n=\n\"/app/data/Kokoro-82M/fp16/kokoro-v0_19-half.pth\"\nlogger.info(\nf\n\"Model path:\n{\nmodel_path\n}\n\"\n)\nif\nnot\nos.path.exists(model_path):\nlogger.info(\nf\n\"Error: Model file not found at\n{\nmodel_path\n}\n\"\n)\nraise\nFileNotFoundError\n(\nf\n\"Model file not found at\n{\nmodel_path\n}\n\"\n)\ntry\n:\nself\n.model\n=\nbuild_model(model_path,\nself\n.device)\nlogger.info(\n\"Model loaded successfully\"\n)\nexcept\nException\nas\ne:\nlogger.info(\nf\n\"Error loading model:\n{\nstr\n(e)\n}\n\"\n)\nraise\n# Load default voice\nlogger.info(\n\"Loading default voice...\"\n)\nvoice_path\n=\n\"/app/data/Kokoro-82M/voices/af.pt\"\nif\nnot\nos.path.exists(voice_path):\nlogger.info(\nf\n\"Error: Voice file not found at\n{\nvoice_path\n}\n\"\n)\nraise\nFileNotFoundError\n(\nf\n\"Voice file not found at\n{\nvoice_path\n}\n\"\n)\ntry\n:\nself\n.default_voice\n=\ntorch.load(voice_path).to(\nself\n.device)\nlogger.info(\n\"Default voice loaded successfully\"\n)\nexcept\nException\nas\ne:\nlogger.info(\nf\n\"Error loading default voice:\n{\nstr\n(e)\n}\n\"\n)\nraise\n# Dictionary of available voices\nself\n.voices\n=\n{\n\"default\"\n:\n\"af\"\n,\n\"bella\"\n:\n\"af_bella\"\n,\n\"sarah\"\n:\n\"af_sarah\"\n,\n\"adam\"\n:\n\"am_adam\"\n,\n\"michael\"\n:\n\"am_michael\"\n,\n\"emma\"\n:\n\"bf_emma\"\n,\n\"isabella\"\n:\n\"bf_isabella\"\n,\n\"george\"\n:\n\"bm_george\"\n,\n\"lewis\"\n:\n\"bm_lewis\"\n,\n\"nicole\"\n:\n\"af_nicole\"\n,\n\"sky\"\n:\n\"af_sky\"\n,\n}\nreturn\n\u200b\nDefine the\npredict\nfunction\nThe\npredict\nfunction contains the actual inference logic. The steps here are:\nProcess input text and handle voice selection\nChunk text for long inputs\nGenerate audio\nConvert resulting audio to base64 and return it\nmodel/model.py\nCopy\nAsk AI\ndef\npredict\n(\nself\n,\nmodel_input\n):\n# Run model inference here\nstart\n=\ntime.time()\ntext\n=\nstr\n(model_input.get(\n\"text\"\n,\n\"Hi, I'm kokoro\"\n))\nvoice\n=\nstr\n(model_input.get(\n\"voice\"\n,\n\"af\"\n))\nspeed\n=\nfloat\n(model_input.get(\n\"speed\"\n,\n1.0\n))\nlogger.info(\nf\n\"Text has\n{\nlen\n(text)\n}\ncharacters. Using voice\n{\nvoice\n}\nand speed\n{\nspeed\n}\n.\"\n)\nif\nvoice\n!=\n\"af\"\n:\nvoicepack\n=\ntorch.load(\nf\n\"/app/data/Kokoro-82M/voices/\n{\nvoice\n}\n.pt\"\n).to(\nself\n.device\n)\nelse\n:\nvoicepack\n=\nself\n.default_voice\nif\nlen\n(text)\n>=\n400\n:\nlogger.info(\n\"Text is longer than 400 characters, splitting into sentences.\"\n)\nwavs\n=\n[]\ndef\ngroup_sentences\n(\ntext\n,\nmax_length\n=\n400\n):\nsentences\n=\nsent_tokenize(text)\n# Split long sentences\nwhile\nmax\n([\nlen\n(sent)\nfor\nsent\nin\nsentences])\n>\nmax_length:\nmax_sent\n=\nmax\n(sentences,\nkey\n=\nlen\n)\nsentences_before\n=\nsentences[: sentences.index(max_sent)]\nsentences_after\n=\nsentences[sentences.index(max_sent)\n+\n1\n:]\nnew_sentences\n=\n[\ns.strip()\n+\n\".\"\nfor\ns\nin\nmax_sent.split(\n\".\"", "mimetype": "text/plain", "start_char_idx": 355255, "end_char_idx": 358353, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b55fc3a0-341e-4295-9f27-625ab38c9103": {"__data__": {"id_": "b55fc3a0-341e-4295-9f27-625ab38c9103", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1ee20a64-de36-4f10-bd0a-0423682edd98", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "b212d935c009f73e13b0a95da1cf18921aa1d28395c948cb480530b49b5d25d0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ad29cabf-3f1d-4e78-a57d-dba69871acda", "node_type": "1", "metadata": {}, "hash": "43b93b0e74fe96f0c3428ea93e85b12816b9377683aeb0035322aa8b85218341", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": ")\nwavs\n=\n[]\ndef\ngroup_sentences\n(\ntext\n,\nmax_length\n=\n400\n):\nsentences\n=\nsent_tokenize(text)\n# Split long sentences\nwhile\nmax\n([\nlen\n(sent)\nfor\nsent\nin\nsentences])\n>\nmax_length:\nmax_sent\n=\nmax\n(sentences,\nkey\n=\nlen\n)\nsentences_before\n=\nsentences[: sentences.index(max_sent)]\nsentences_after\n=\nsentences[sentences.index(max_sent)\n+\n1\n:]\nnew_sentences\n=\n[\ns.strip()\n+\n\".\"\nfor\ns\nin\nmax_sent.split(\n\".\"\n)\nif\ns.strip()\n]\nsentences\n=\nsentences_before\n+\nnew_sentences\n+\nsentences_after\nreturn\nsentences\nsentences\n=\ngroup_sentences(text)\nlogger.info(\nf\n\"Processing\n{\nlen\n(sentences)\n}\nchunks. Starting generation...\"\n)\nfor\nsent\nin\nsentences:\nif\nsent.strip():\naudio, _\n=\ngenerate(\nself\n.model, sent.strip(), voicepack,\nlang\n=\nvoice[\n0\n],\nspeed\n=\nspeed\n)\n# Remove potential artifacts at the end\naudio\n=\naudio[:\n-\n2000\n]\nif\nlen\n(audio)\n>\n2000\nelse\naudio\nwavs.append(audio)\n# Concatenate all audio chunks\naudio\n=\nnp.concatenate(wavs)\nelse\n:\nlogger.info(\n\"No splitting needed. Generating audio...\"\n)\naudio, _\n=\ngenerate(\nself\n.model, text, voicepack,\nlang\n=\nvoice[\n0\n],\nspeed\n=\nspeed)\n# Write audio to in-memory buffer\nbuffer\n=\nio.BytesIO()\nwav.write(buffer,\n24000\n, audio)\nwav_bytes\n=\nbuffer.getvalue()\nduration_seconds\n=\nlen\n(audio)\n/\n24000\nlogger.info(\nf\n\"Generation took\n{\ntime.time()\n-\nstart\n}\nseconds to generate\n{\nduration_seconds\n:.2f}\nseconds of audio\"\n)\nreturn\n{\n\"base64\"\n: base64.b64encode(wav_bytes).decode(\n\"utf-8\"\n)}\n\u200b\nSetting up the\nconfig.yaml\nRunning Kokoro requires a handful of Python libraries, including\ntorch\n,\ntransformers\n, and others.\nconfig.yaml\nCopy\nAsk AI\nbuild_commands\n:\n-\npython3 -c \"import nltk; nltk.download('punkt'); nltk.download('punkt_tab')\"\nenvironment_variables\n: {}\nmodel_metadata\n:\nexample_model_input\n: {\n\"text\"\n:\n\"Kokoro is a frontier TTS model for its size of 82 million parameters (text in/audio out). On 25 Dec 2024, Kokoro v0.19 weights were permissively released in full fp32 precision under an Apache 2.0 license. As of 2 Jan 2025, 10 unique Voicepacks have been released, and a .onnx version of v0.19 is available.In the weeks leading up to its release, Kokoro v0.19 was the #1\ud83e\udd47 ranked model in TTS Spaces Arena. Kokoro had achieved higher Elo in this single-voice Arena setting over other models, using fewer parameters and less data. Kokoro's ability to top this Elo ladder suggests that the scaling law (Elo vs compute/data/params) for traditional TTS models might have a steeper slope than previously expected.\"\n,\n\"voice\"\n:\n\"af\"\n,\n\"speed\"\n:\n1.0\n}\nmodel_name\n:\nkokoro\npython_version\n:\npy311\nrequirements\n:\n-\ntorch==2.5.1\n-\ntransformers==4.48.0\n-\nscipy==1.15.1\n-\nphonemizer==3.3.0\n-\nnltk==3.9.1\n-\nnumpy\n-\nhuggingface_hub[hf_transfer]\n-\nhf_transfer==0.1.9\n-\nmunch==4.0.0\nresources\n:\naccelerator\n:\nT4\nuse_gpu\n:\ntrue\nruntime\n:\npredict_concurrency\n:\n1\nsecrets\n: {}\nsystem_packages\n:\n-\nespeak-ng\n\u200b\nConfiguring resources for Kokoro\nNote that we need an T4 GPU to run this model.\nconfig.yaml\nCopy\nAsk AI\nresources\n:\naccelerator\n:\nT4\nuse_gpu\n:\ntrue\n\u200b\nSystem Packages\nRunning Kokoro requires\nespeak-ng\nto synthesize speech output.\nconfig.yaml\nCopy\nAsk AI\nsystem_packages\n:\n-\nespeak-ng\n\u200b\nDeploy the model\nDeploy the model like you would other Trusses by running the following command:\nCopy\nAsk AI\ntruss\npush\nkokoro\n--publish\n\u200b\nRun an inference\nUse a Python script to call the deployed model and parse its response.", "mimetype": "text/plain", "start_char_idx": 357955, "end_char_idx": 361299, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ad29cabf-3f1d-4e78-a57d-dba69871acda": {"__data__": {"id_": "ad29cabf-3f1d-4e78-a57d-dba69871acda", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b55fc3a0-341e-4295-9f27-625ab38c9103", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "6cbd650e0c7e0002b51d99e890124843856c207013d892a917f9453cac02c9e4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e501f501-0fdc-4723-ad98-08dbe22853c0", "node_type": "1", "metadata": {}, "hash": "c72efeae3a3ec785054e06d4cb45a05bbabc208a3889a68c530061acb366798b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "config.yaml\nCopy\nAsk AI\nresources\n:\naccelerator\n:\nT4\nuse_gpu\n:\ntrue\n\u200b\nSystem Packages\nRunning Kokoro requires\nespeak-ng\nto synthesize speech output.\nconfig.yaml\nCopy\nAsk AI\nsystem_packages\n:\n-\nespeak-ng\n\u200b\nDeploy the model\nDeploy the model like you would other Trusses by running the following command:\nCopy\nAsk AI\ntruss\npush\nkokoro\n--publish\n\u200b\nRun an inference\nUse a Python script to call the deployed model and parse its response. In this example, the script sends text input to the model and saves the returned audio (decoded from base64) as a WAV file:\noutput.wav\n.\ninfer.py\nCopy\nAsk AI\nimport\nhttpx\nimport\nbase64\n# Replace the empty string with your model id below\nmodel_id\n=\n\"\"\nbaseten_api_key\n=\nos.environ[\n\"BASETEN_API_KEY\"\n]\nwith\nhttpx.Client()\nas\nclient:\n# Make the API request\nresp\n=\nclient.post(\nf\n\"https://model-\n{\nmodel_id\n}\n.api.baseten.co/production/predict\"\n,\nheaders\n=\n{\n\"Authorization\"\n:\nf\n\"Api-Key\n{\nbaseten_api_key\n}\n\"\n},\njson\n=\n{\n\"text\"\n:\n\"Hello world\"\n,\n\"voice\"\n:\n\"af\"\n,\n\"speed\"\n:\n1.0\n},\ntimeout\n=\nNone\n,\n)\n# Get the base64 encoded audio\nresponse_data\n=\nresp.json()\naudio_base64\n=\nresponse_data[\n\"base64\"\n]\n# Decode the base64 string\naudio_bytes\n=\nbase64.b64decode(audio_base64)\n# Write to a WAV file\nwith\nopen\n(\n\"output.wav\"\n,\n\"wb\"\n)\nas\nf:\nf.write(audio_bytes)\nprint\n(\n\"Audio saved to output.wav\"\n)\nWas this page helpful?\nYes\nNo\nPrevious\nOverview\nBrowse our library of open source models that are ready to deploy behind an API endpoint in seconds.\nNext\nOn this page\nSet up imports\nDownloading model weights\nDefine the Model class and load function\nDefine the predict function\nSetting up the config.yaml\nConfiguring resources for Kokoro\nSystem Packages\nDeploy the model\nRun an inference\nAssistant\nResponses are generated using AI and may contain mistakes.\n\n\nContent from https://docs.baseten.co/examples/vllm:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nExamples\nOverview\nDeploy your first model\nFast LLMs with TensorRT-LLM\nRun any LLM with vLLM\nDeploy LLMs with SGLang\nRAG pipeline with Chains\nTranscribe audio with Chains\nImage generation\nDeploy a ComfyUI project\nEmbeddings with BEI\nDockerized model\nLLM with Streaming\nText to speech\nModel library\nOverview\nDeepseek\nLlama\nQwen\nGemma\nStable Diffusion\nFlux\nKokoro\nMicrosoft\nNomic\nWhisper\nMars\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nExamples\nRun any LLM with vLLM\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nAnother great option for inference is\nvLLM\n, which supports a wide range of models and performance optimizations.\n\u200b\nExample: Deploy Qwen 2.5 3B on an A10G\nThis configuration serves\nQwen 2.5 3B\nwith vLLM on an A10G GPU. Running this model is fast and cheap, making it a good example for documentation, but the process of deploying it is very similar to larger models like\nLlama 3.3 70B\n.\n\u200b\nSetup\nBefore you deploy a model, you\u2019ll need three quick setup steps.\n1\nCreate an API key for your Baseten account\nCreate an\nAPI key\nand save it as an environment variable:\nCopy\nAsk AI\nexport\nBASETEN_API_KEY\n=\n\"abcd.123456\"\n2\nAdd an access token for Hugging Face\nSome models require that you accept terms and conditions on HuggingFace before deployment. To prevent issues:\nAccept the license for any gated models you wish to access, like\nLlama 3.3\n.\nCreate a read-only\nuser access token\nfrom your Hugging Face account.\nAdd the\nhf_access_token\nsecret\nto your Baseten workspace\n.", "mimetype": "text/plain", "start_char_idx": 360868, "end_char_idx": 364286, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e501f501-0fdc-4723-ad98-08dbe22853c0": {"__data__": {"id_": "e501f501-0fdc-4723-ad98-08dbe22853c0", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ad29cabf-3f1d-4e78-a57d-dba69871acda", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "307741ef7f8ae4c27cfd1c3b37015baa13c597d1ef31a2ef1b4d176d217baa9f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "de2145d2-f6f3-43a7-bb3c-49a286ba46ba", "node_type": "1", "metadata": {}, "hash": "f7640927a897d183d76ac85fba0e07508e447c4e75455c7cc85bf73148b15755", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Running this model is fast and cheap, making it a good example for documentation, but the process of deploying it is very similar to larger models like\nLlama 3.3 70B\n.\n\u200b\nSetup\nBefore you deploy a model, you\u2019ll need three quick setup steps.\n1\nCreate an API key for your Baseten account\nCreate an\nAPI key\nand save it as an environment variable:\nCopy\nAsk AI\nexport\nBASETEN_API_KEY\n=\n\"abcd.123456\"\n2\nAdd an access token for Hugging Face\nSome models require that you accept terms and conditions on HuggingFace before deployment. To prevent issues:\nAccept the license for any gated models you wish to access, like\nLlama 3.3\n.\nCreate a read-only\nuser access token\nfrom your Hugging Face account.\nAdd the\nhf_access_token\nsecret\nto your Baseten workspace\n.\n3\nInstall Truss in your local development environment\nInstall the latest version of Truss, our open-source model packaging framework, as well as OpenAI\u2019s model inference SDK, with:\nCopy\nAsk AI\npip\ninstall\n--upgrade\ntruss\nopenai\n\u200b\nConfiguration\nStart with an empty configuration file.\nCopy\nAsk AI\nmkdir\nqwen-2-5-3b-engine\ntouch\nqwen-2-5-3b-engine/config.yaml\nBelow is an example for Qwen 2.5 3B. You can copy-paste it into the empty\nconfig.yaml\nwe created above.\nconfig.yaml\nCopy\nAsk AI\nmodel_metadata\n:\nengine_args\n:\nmodel\n:\nQwen/Qwen2.5-3B-Instruct\nexample_model_input\n:\n# Loads sample request into Baseten playground\nmessages\n:\n-\nrole\n:\nsystem\ncontent\n:\n\"You are a helpful assistant.\"\n-\nrole\n:\nuser\ncontent\n:\n\"What does Tongyi Qianwen mean?\"\nstream\n:\ntrue\nmax_tokens\n:\n512\ntemperature\n:\n0.6\nbase_image\n:\nimage\n:\nvllm/vllm-openai:v0.7.3\ndocker_server\n:\nstart_command\n:\nsh -c \"HF_TOKEN=$(cat /secrets/hf_access_token) vllm serve Qwen/Qwen2.5-3B-Instruct --enable-prefix-caching --enable-chunked-prefill\"\nreadiness_endpoint\n:\n/health\nliveness_endpoint\n:\n/health\npredict_endpoint\n:\n/v1/completions\nserver_port\n:\n8000\nruntime\n:\npredict_concurrency\n:\n256\nresources\n:\naccelerator\n:\nA10G\nuse_gpu\n:\ntrue\nenvironment_variables\n:\nhf_access_token\n:\nnull\n\u200b\nDeployment\nPushing the model to Baseten kicks off a multi-stage deployment process.\nCopy\nAsk AI\ntruss\npush\nqwen-2-5-3b-engine\n--publish\nUpon deployment, check your terminal logs or Baseten account to find the URL for the model server.\n\u200b\nInference\nThis model is OpenAI compatible and can be called using the OpenAI client.\ncall_model.py\nCopy\nAsk AI\nimport\nos\nfrom\nopenai\nimport\nOpenAI\n# https://model-XXXXXXX.api.baseten.co/environments/production/sync/v1\nmodel_url\n=\n\"\"\nclient\n=\nOpenAI(\nbase_url\n=\nmodel_url,\napi_key\n=\nos.environ.get(\n\"BASETEN_API_KEY\"\n),\n)\nstream\n=\nclient.chat.completions.create(\nmodel\n=\n\"Qwen/Qwen2.5-3B-Instruct\"\n,\nmessages\n=\n[\n{\n\"role\"\n:\n\"system\"\n,\n\"content\"\n:\n\"You are a helpful assistant.\"\n},\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"What does Tongyi Qianwen mean?\"\n}\n],\nstream\n=\nTrue\n,\n)\nfor\nchunk\nin\nstream:\nif\nchunk.choices[\n0\n].delta.content\nis\nnot\nNone\n:\nprint\n(chunk.choices[\n0\n].delta.content,\nend\n=\n\"\"\n)\nThat\u2019s it! You have successfully deployed and called a model using vLLM.\nWas this page helpful?\nYes\nNo\nPrevious\nDeploy LLMs with SGLang\nOptimized inference for LLMs with SGLang\nNext\nOn this page\nExample: Deploy Qwen 2.5 3B on an A10G\nSetup\nConfiguration\nDeployment\nInference\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 363539, "end_char_idx": 366804, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "de2145d2-f6f3-43a7-bb3c-49a286ba46ba": {"__data__": {"id_": "de2145d2-f6f3-43a7-bb3c-49a286ba46ba", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e501f501-0fdc-4723-ad98-08dbe22853c0", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "689e5288cab6a6c8c71e4351fe509b1fc240799fe527abcf7b1f00cc8d4d3e0f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "434a100d-0c39-450f-88c1-699ead75ede8", "node_type": "1", "metadata": {}, "hash": "70116d7ae321af20bbc783182995bd43477c28b3ccbb8451d9b8faf9990180dc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "},\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"What does Tongyi Qianwen mean?\"\n}\n],\nstream\n=\nTrue\n,\n)\nfor\nchunk\nin\nstream:\nif\nchunk.choices[\n0\n].delta.content\nis\nnot\nNone\n:\nprint\n(chunk.choices[\n0\n].delta.content,\nend\n=\n\"\"\n)\nThat\u2019s it! You have successfully deployed and called a model using vLLM.\nWas this page helpful?\nYes\nNo\nPrevious\nDeploy LLMs with SGLang\nOptimized inference for LLMs with SGLang\nNext\nOn this page\nExample: Deploy Qwen 2.5 3B on an A10G\nSetup\nConfiguration\nDeployment\nInference\nAssistant\nResponses are generated using AI and may contain mistakes.\n\n\nContent from https://docs.baseten.co/inference/async:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nDeveloping a Chain\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nExporting metrics\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nInference\nAsync inference\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nAsync requests are a \u201cfire and forget\u201d way of executing model inference requests. Instead of waiting for a response from a model, making an async request queues the request, and immediately returns with a request identifier. Optionally, async request results are sent via a\nPOST\nrequest to a user-defined webhook upon completion.\nUse async requests for:\nLong-running inference tasks that may otherwise hit request timeouts.\nBatched inference jobs.\nPrioritizing certain inference requests.\nAsync fast facts:\nAsync requests can be made to any model\u2014\nno model code changes necessary\n.\nAsync requests can remain queued for up to 72 hours and run for up to 1 hour.\nAsync requests are\nnot\ncompatible with streaming model output.\nAsync request inputs and model outputs are\nnot\nstored after an async request has been completed. Instead, model outputs will be sent to your webhook via a\nPOST\nrequest.\n\u200b\nQuick start\nThere are two ways to use async inference:\nProvide a webhook endpoint where model outputs will be sent via a\nPOST\nrequest. If providing a webhook, you can\nuse async inference on any model, without making any changes to your model code\n.\nInside your Truss\u2019\nmodel.py\n, save prediction results to cloud storage. If a webhook endpoint is provided, your model outputs will also be sent to your webhook.\nNote that Baseten\ndoes not\nstore model outputs. If you do not wish to use a webhook, your\nmodel.py\nmust write model outputs to a cloud storage bucket or database as part of its implementation.\nQuick start with webhook\nQuick start with saving model outputs\n1\nSetup webhook endpoint\nSet up a webhook endpoint for handling completed async requests. Since Baseten doesn\u2019t store model outputs, model outputs from async requests will be sent to your webhook endpoint.\nBefore creating your first async request, try running a sample request against your webhook endpoint to ensure that it can consume async predict results properly. Check out\nthis example webhook test\n.\nWe recommend using\nthis Repl\nas a starting point.\n2\nSchedule an async predict request\nCall\n/async_predict\non your model. The body of an\n/async_predict\nrequest includes the model input in\nmodel_input\nfield, with the addition of a webhook endpoint (from the previous step) in the\nwebhook_endpoint\nfield.", "mimetype": "text/plain", "start_char_idx": 366247, "end_char_idx": 369881, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "434a100d-0c39-450f-88c1-699ead75ede8": {"__data__": {"id_": "434a100d-0c39-450f-88c1-699ead75ede8", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "de2145d2-f6f3-43a7-bb3c-49a286ba46ba", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "16a42004dc84f8b67ae05e3831052ca5a3b0206590a3c4f32a58513a3a8a5e33", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "37b3def5-bfc5-4a0b-9a34-183622b9fd5b", "node_type": "1", "metadata": {}, "hash": "ded8574d7e3b099213b53e78d17fed17af86c781e6e66f592155bac603989cb9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "If you do not wish to use a webhook, your\nmodel.py\nmust write model outputs to a cloud storage bucket or database as part of its implementation.\nQuick start with webhook\nQuick start with saving model outputs\n1\nSetup webhook endpoint\nSet up a webhook endpoint for handling completed async requests. Since Baseten doesn\u2019t store model outputs, model outputs from async requests will be sent to your webhook endpoint.\nBefore creating your first async request, try running a sample request against your webhook endpoint to ensure that it can consume async predict results properly. Check out\nthis example webhook test\n.\nWe recommend using\nthis Repl\nas a starting point.\n2\nSchedule an async predict request\nCall\n/async_predict\non your model. The body of an\n/async_predict\nrequest includes the model input in\nmodel_input\nfield, with the addition of a webhook endpoint (from the previous step) in the\nwebhook_endpoint\nfield.\nPython\nCopy\nAsk AI\nimport\nrequests\nimport\nos\nmodel_id\n=\n\"\"\n# Replace this with your model ID\nwebhook_endpoint\n=\n\"\"\n# Replace this with your webhook endpoint URL\n# Read secrets from environment variables\nbaseten_api_key\n=\nos.environ[\n\"BASETEN_API_KEY\"\n]\n# Call the async_predict endpoint of the production deployment\nresp\n=\nrequests.post(\nf\n\"https://model-\n{\nmodel_id\n}\n.api.baseten.co/production/async_predict\"\n,\nheaders\n=\n{\n\"Authorization\"\n:\nf\n\"Api-Key\n{\nbaseten_api_key\n}\n\"\n},\njson\n=\n{\n\"model_input\"\n: {\n\"prompt\"\n:\n\"hello world!\"\n},\n\"webhook_endpoint\"\n: webhook_endpoint\n# Optional fields for priority, max_time_in_queue_seconds, etc\n},\n)\nprint\n(resp.json())\nSave the\nrequest_id\nfrom the\n/async_predict\nresponse to check its status or cancel it.\n201\nCopy\nAsk AI\n{\n\"request_id\"\n:\n\"9876543210abcdef1234567890fedcba\"\n}\nSee the\nasync inference API reference\nfor more endpoint details.\n3\nCheck async predict results\nUsing the\nrequest_id\nsaved from the previous step, check the status of your async predict request:\nPython\nCopy\nAsk AI\nimport\nrequests\nimport\nos\nmodel_id\n=\n\"\"\nrequest_id\n=\n\"\"\n# Read secrets from environment variables\nbaseten_api_key\n=\nos.environ[\n\"BASETEN_API_KEY\"\n]\nresp\n=\nrequests.get(\nf\n\"https://model-\n{\nmodel_id\n}\n.api.baseten.co/async_request/\n{\nrequest_id\n}\n\"\n,\nheaders\n=\n{\n\"Authorization\"\n:\nf\n\"Api-Key\n{\nbaseten_api_key\n}\n\"\n}\n)\nprint\n(resp.json())\nOnce your model has finished executing the request, the async predict result will be sent to your webhook in a\nPOST\nrequest.\nCopy\nAsk AI\n{\n\"request_id\"\n:\n\"9876543210abcdef1234567890fedcba\"\n,\n\"model_id\"\n:\n\"my_model_id\"\n,\n\"deployment_id\"\n:\n\"my_deployment_id\"\n,\n\"type\"\n:\n\"async_request_completed\"\n,\n\"time\"\n:\n\"2024-04-30T01:01:08.883423Z\"\n,\n\"data\"\n: {\n\"my_model_output\"\n:\n\"hello world!\"\n},\n\"errors\"\n: []\n}\n4\nSecure your webhook\nWe strongly recommend securing the requests sent to your webhooks to validate that they are from Baseten.\nFor instructions, see our\nguide to securing async requests\n.\n1\nSetup webhook endpoint\nSet up a webhook endpoint for handling completed async requests. Since Baseten doesn\u2019t store model outputs, model outputs from async requests will be sent to your webhook endpoint.\nBefore creating your first async request, try running a sample request against your webhook endpoint to ensure that it can consume async predict results properly. Check out\nthis example webhook test\n.\nWe recommend using\nthis Repl\nas a starting point.\n2\nSchedule an async predict request\nCall\n/async_predict\non your model. The body of an\n/async_predict\nrequest includes the model input in\nmodel_input\nfield, with the addition of a webhook endpoint (from the previous step) in the\nwebhook_endpoint\nfield.", "mimetype": "text/plain", "start_char_idx": 368965, "end_char_idx": 372549, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "37b3def5-bfc5-4a0b-9a34-183622b9fd5b": {"__data__": {"id_": "37b3def5-bfc5-4a0b-9a34-183622b9fd5b", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "434a100d-0c39-450f-88c1-699ead75ede8", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "4006f9a2d7584f4166e7d231404029b9cc94f59432c3d5c612f11a82abc6c996", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4d78577a-cf2e-4d33-b256-4ce9ee80b8ef", "node_type": "1", "metadata": {}, "hash": "05b438840124c45fd02c5880487d33bb8c8e5bfbd154449d4e02edef160b3d12", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "},\n\"errors\"\n: []\n}\n4\nSecure your webhook\nWe strongly recommend securing the requests sent to your webhooks to validate that they are from Baseten.\nFor instructions, see our\nguide to securing async requests\n.\n1\nSetup webhook endpoint\nSet up a webhook endpoint for handling completed async requests. Since Baseten doesn\u2019t store model outputs, model outputs from async requests will be sent to your webhook endpoint.\nBefore creating your first async request, try running a sample request against your webhook endpoint to ensure that it can consume async predict results properly. Check out\nthis example webhook test\n.\nWe recommend using\nthis Repl\nas a starting point.\n2\nSchedule an async predict request\nCall\n/async_predict\non your model. The body of an\n/async_predict\nrequest includes the model input in\nmodel_input\nfield, with the addition of a webhook endpoint (from the previous step) in the\nwebhook_endpoint\nfield.\nPython\nCopy\nAsk AI\nimport\nrequests\nimport\nos\nmodel_id\n=\n\"\"\n# Replace this with your model ID\nwebhook_endpoint\n=\n\"\"\n# Replace this with your webhook endpoint URL\n# Read secrets from environment variables\nbaseten_api_key\n=\nos.environ[\n\"BASETEN_API_KEY\"\n]\n# Call the async_predict endpoint of the production deployment\nresp\n=\nrequests.post(\nf\n\"https://model-\n{\nmodel_id\n}\n.api.baseten.co/production/async_predict\"\n,\nheaders\n=\n{\n\"Authorization\"\n:\nf\n\"Api-Key\n{\nbaseten_api_key\n}\n\"\n},\njson\n=\n{\n\"model_input\"\n: {\n\"prompt\"\n:\n\"hello world!\"\n},\n\"webhook_endpoint\"\n: webhook_endpoint\n# Optional fields for priority, max_time_in_queue_seconds, etc\n},\n)\nprint\n(resp.json())\nSave the\nrequest_id\nfrom the\n/async_predict\nresponse to check its status or cancel it.\n201\nCopy\nAsk AI\n{\n\"request_id\"\n:\n\"9876543210abcdef1234567890fedcba\"\n}\nSee the\nasync inference API reference\nfor more endpoint details.\n3\nCheck async predict results\nUsing the\nrequest_id\nsaved from the previous step, check the status of your async predict request:\nPython\nCopy\nAsk AI\nimport\nrequests\nimport\nos\nmodel_id\n=\n\"\"\nrequest_id\n=\n\"\"\n# Read secrets from environment variables\nbaseten_api_key\n=\nos.environ[\n\"BASETEN_API_KEY\"\n]\nresp\n=\nrequests.get(\nf\n\"https://model-\n{\nmodel_id\n}\n.api.baseten.co/async_request/\n{\nrequest_id\n}\n\"\n,\nheaders\n=\n{\n\"Authorization\"\n:\nf\n\"Api-Key\n{\nbaseten_api_key\n}\n\"\n}\n)\nprint\n(resp.json())\nOnce your model has finished executing the request, the async predict result will be sent to your webhook in a\nPOST\nrequest.\nCopy\nAsk AI\n{\n\"request_id\"\n:\n\"9876543210abcdef1234567890fedcba\"\n,\n\"model_id\"\n:\n\"my_model_id\"\n,\n\"deployment_id\"\n:\n\"my_deployment_id\"\n,\n\"type\"\n:\n\"async_request_completed\"\n,\n\"time\"\n:\n\"2024-04-30T01:01:08.883423Z\"\n,\n\"data\"\n: {\n\"my_model_output\"\n:\n\"hello world!\"\n},\n\"errors\"\n: []\n}\n4\nSecure your webhook\nWe strongly recommend securing the requests sent to your webhooks to validate that they are from Baseten.\nFor instructions, see our\nguide to securing async requests\n.\n1\nUpdate your model to save prediction results\nUpdate your Truss\u2019s\nmodel.py\nto save prediction results to cloud storage, such as S3 or GCS. We recommend implementing this in your model\u2019s\npostprocess()\nmethod, which will run on CPU after the prediction has completed.\n2\nSetup webhook endpoint\nOptionally, set up a webhook endpoint so Baseten can notify you when your async request completes.\nBefore creating your first async request, try running a sample request against your webhook endpoint to ensure that it can consume async predict results properly. Check out\nthis example webhook test\n.\nWe recommend using\nthis Repl\nas a starting point.\n3\nSchedule an async predict request\nCall\n/async_predict\non your model. The body of an\n/async_predict\nrequest includes the model input in\nmodel_input\nfield, with the addition of a webhook endpoint (from the previous step) in the\nwebhook_endpoint\nfield.", "mimetype": "text/plain", "start_char_idx": 371633, "end_char_idx": 375402, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4d78577a-cf2e-4d33-b256-4ce9ee80b8ef": {"__data__": {"id_": "4d78577a-cf2e-4d33-b256-4ce9ee80b8ef", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "37b3def5-bfc5-4a0b-9a34-183622b9fd5b", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "9c13ede85f51cb9463ae94007e2f61246c3ad0f4fc48ad3cc1a146c7ac8f0f0f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eaa04127-3117-4679-bf23-b3936537248f", "node_type": "1", "metadata": {}, "hash": "e080439f9ad44e3c3bfb12192dbc3bbf5c97640b5ff822a2fe8a616af506c8e4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1\nUpdate your model to save prediction results\nUpdate your Truss\u2019s\nmodel.py\nto save prediction results to cloud storage, such as S3 or GCS. We recommend implementing this in your model\u2019s\npostprocess()\nmethod, which will run on CPU after the prediction has completed.\n2\nSetup webhook endpoint\nOptionally, set up a webhook endpoint so Baseten can notify you when your async request completes.\nBefore creating your first async request, try running a sample request against your webhook endpoint to ensure that it can consume async predict results properly. Check out\nthis example webhook test\n.\nWe recommend using\nthis Repl\nas a starting point.\n3\nSchedule an async predict request\nCall\n/async_predict\non your model. The body of an\n/async_predict\nrequest includes the model input in\nmodel_input\nfield, with the addition of a webhook endpoint (from the previous step) in the\nwebhook_endpoint\nfield.\nPython\nCopy\nAsk AI\nimport\nrequests\nimport\nos\nmodel_id\n=\n\"\"\n# Replace this with your model ID\nwebhook_endpoint\n=\n\"\"\n# Replace this with your webhook endpoint URL\n# Read secrets from environment variables\nbaseten_api_key\n=\nos.environ[\n\"BASETEN_API_KEY\"\n]\n# Call the async_predict endpoint of the production deployment\nresp\n=\nrequests.post(\nf\n\"https://model-\n{\nmodel_id\n}\n.api.baseten.co/production/async_predict\"\n,\nheaders\n=\n{\n\"Authorization\"\n:\nf\n\"Api-Key\n{\nbaseten_api_key\n}\n\"\n},\njson\n=\n{\n\"model_input\"\n: {\n\"prompt\"\n:\n\"hello world!\"\n},\n\"webhook_endpoint\"\n: webhook_endpoint\n# Optional fields for priority, max_time_in_queue_seconds, etc\n},\n)\nprint\n(resp.json())\nSave the\nrequest_id\nfrom the\n/async_predict\nresponse to check its status or cancel it.\n201\nCopy\nAsk AI\n{\n\"request_id\"\n:\n\"9876543210abcdef1234567890fedcba\"\n}\nSee the\nasync inference API reference\nfor more endpoint details.\n4\nCheck async predict results\nUsing the\nrequest_id\nsaved from the previous step, check the status of your async predict request:\nPython\nCopy\nAsk AI\nimport\nrequests\nimport\nos\nmodel_id\n=\n\"\"\nrequest_id\n=\n\"\"\n# Read secrets from environment variables\nbaseten_api_key\n=\nos.environ[\n\"BASETEN_API_KEY\"\n]\nresp\n=\nrequests.get(\nf\n\"https://model-\n{\nmodel_id\n}\n.api.baseten.co/async_request/\n{\nrequest_id\n}\n\"\n,\nheaders\n=\n{\n\"Authorization\"\n:\nf\n\"Api-Key\n{\nbaseten_api_key\n}\n\"\n}\n)\nprint\n(resp.json())\nOnce your model has finished executing the request, the async predict result will be sent to your webhook in a\nPOST\nrequest.\nCopy\nAsk AI\n{\n\"request_id\"\n:\n\"9876543210abcdef1234567890fedcba\"\n,\n\"model_id\"\n:\n\"my_model_id\"\n,\n\"deployment_id\"\n:\n\"my_deployment_id\"\n,\n\"type\"\n:\n\"async_request_completed\"\n,\n\"time\"\n:\n\"2024-04-30T01:01:08.883423Z\"\n,\n\"data\"\n: {\n\"my_model_output\"\n:\n\"hello world!\"\n},\n\"errors\"\n: []\n}\n5\nSecure your webhook\nWe strongly recommend securing the requests sent to your webhooks to validate that they are from Baseten.\nFor instructions, see our\nguide to securing async requests\n.\nChains\n: this guide is written for Truss models, but\nChains\nsupport async inference likewise. An\nChain entrypoint can be invoked via its\nasync_run_remote\nendpoint, e.g.\nhttps://chain-{chain_id}.api.baseten.co/production/async_run_remote\n. The\ninternal Chainlet-Chainlet call will still run synchronously.\n\u200b\nUser guide\n\u200b\nConfiguring the webhook endpoint\nConfigure your webhook endpoint to handle\nPOST\nrequests with\nasync predict results\n. We require that webhook endpoints use HTTPS.\nWe recommend running a sample request against your webhook endpoint to ensure that it can consume async predict results properly. Try running\nthis webhook test\n.\nFor local development, we recommend using\nthis Repl\nas a starting point. This code validates the webhook request and logs the payload.", "mimetype": "text/plain", "start_char_idx": 374509, "end_char_idx": 378141, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "eaa04127-3117-4679-bf23-b3936537248f": {"__data__": {"id_": "eaa04127-3117-4679-bf23-b3936537248f", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4d78577a-cf2e-4d33-b256-4ce9ee80b8ef", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "97c8ec98c45fb053da9437c34164590b3126151ce400371d7627ec0b96c31cc1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ce9b13d1-3415-4af5-9ecc-835e56370019", "node_type": "1", "metadata": {}, "hash": "5d375059211ebcc3a6adef4be1367930f10e69a6e48af6e470397a78458ce29b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For instructions, see our\nguide to securing async requests\n.\nChains\n: this guide is written for Truss models, but\nChains\nsupport async inference likewise. An\nChain entrypoint can be invoked via its\nasync_run_remote\nendpoint, e.g.\nhttps://chain-{chain_id}.api.baseten.co/production/async_run_remote\n. The\ninternal Chainlet-Chainlet call will still run synchronously.\n\u200b\nUser guide\n\u200b\nConfiguring the webhook endpoint\nConfigure your webhook endpoint to handle\nPOST\nrequests with\nasync predict results\n. We require that webhook endpoints use HTTPS.\nWe recommend running a sample request against your webhook endpoint to ensure that it can consume async predict results properly. Try running\nthis webhook test\n.\nFor local development, we recommend using\nthis Repl\nas a starting point. This code validates the webhook request and logs the payload.\n\u200b\nMaking async requests\nProduction deployment\nDevelopment deployment\nOther published deployments\nPython\nCopy\nAsk AI\nimport\nrequests\nimport\nos\nmodel_id\n=\n\"\"\n# Replace this with your model ID\nwebhook_endpoint\n=\n\"\"\n# Replace this with your webhook endpoint URL\n# Read secrets from environment variables\nbaseten_api_key\n=\nos.environ[\n\"BASETEN_API_KEY\"\n]\n# Call the async_predict endpoint of the production deployment\nresp\n=\nrequests.post(\nf\n\"https://model-\n{\nmodel_id\n}\n.api.baseten.co/production/async_predict\"\n,\nheaders\n=\n{\n\"Authorization\"\n:\nf\n\"Api-Key\n{\nbaseten_api_key\n}\n\"\n},\njson\n=\n{\n\"model_input\"\n: {\n\"prompt\"\n:\n\"hello world!\"\n},\n\"webhook_endpoint\"\n: webhook_endpoint\n# Optional fields for priority, max_time_in_queue_seconds, etc\n},\n)\nprint\n(resp.json())\nPython\nCopy\nAsk AI\nimport\nrequests\nimport\nos\nmodel_id\n=\n\"\"\n# Replace this with your model ID\nwebhook_endpoint\n=\n\"\"\n# Replace this with your webhook endpoint URL\n# Read secrets from environment variables\nbaseten_api_key\n=\nos.environ[\n\"BASETEN_API_KEY\"\n]\n# Call the async_predict endpoint of the production deployment\nresp\n=\nrequests.post(\nf\n\"https://model-\n{\nmodel_id\n}\n.api.baseten.co/production/async_predict\"\n,\nheaders\n=\n{\n\"Authorization\"\n:\nf\n\"Api-Key\n{\nbaseten_api_key\n}\n\"\n},\njson\n=\n{\n\"model_input\"\n: {\n\"prompt\"\n:\n\"hello world!\"\n},\n\"webhook_endpoint\"\n: webhook_endpoint\n# Optional fields for priority, max_time_in_queue_seconds, etc\n},\n)\nprint\n(resp.json())\nPython\nCopy\nAsk AI\nimport\nrequests\nimport\nos\nmodel_id\n=\n\"\"\n# Replace this with your model ID\nwebhook_endpoint\n=\n\"\"\n# Replace this with your webhook endpoint URL\n# Read secrets from environment variables\nbaseten_api_key\n=\nos.environ[\n\"BASETEN_API_KEY\"\n]\n# Call the async_predict endpoint of the development deployment\nresp\n=\nrequests.post(\nf\n\"https://model-\n{\nmodel_id\n}\n.api.baseten.co/development/async_predict\"\n,\nheaders\n=\n{\n\"Authorization\"\n:\nf\n\"Api-Key\n{\nbaseten_api_key\n}\n\"\n},\njson\n=\n{\n\"model_input\"\n: {\n\"prompt\"\n:\n\"hello world!\"\n},\n\"webhook_endpoint\"\n: webhook_endpoint\n# Optional fields for priority, max_time_in_queue_seconds, etc\n},\n)\nprint\n(resp.json())\nPython\nCopy\nAsk AI\nimport\nrequests\nimport\nos\nmodel_id\n=\n\"\"\n# Replace this with your model ID\ndeployment_id\n=\n\"\"\n# Replace this with your deployment ID\nwebhook_endpoint\n=\n\"\"\n# Replace this with your webhook endpoint URL\n# Read secrets from environment variables\nbaseten_api_key\n=\nos.environ[\n\"BASETEN_API_KEY\"\n]\n# Call the async_predict endpoint of the given deployment\nresp\n=\nrequests.post(\nf\n\"https://model-\n{\nmodel_id\n}\n.api.baseten.co/deployment/\n{\ndeployment_id\n}\n/async_predict\"\n,\nheaders\n=\n{\n\"Authorization\"\n:\nf\n\"Api-Key\n{\nbaseten_api_key\n}\n\"\n},\njson\n=\n{\n\"model_input\"\n: {\n\"prompt\"\n:\n\"hello world!\"\n},\n\"webhook_endpoint\"\n: webhook_endpoint\n# Optional fields for priority, max_time_in_queue_seconds, etc\n},\n)\nprint\n(resp.json())\nCreate an async request by calling a model\u2019s\n/async_predict\nendpoint. See the\nasync inference API reference\nfor more endpoint details.", "mimetype": "text/plain", "start_char_idx": 377301, "end_char_idx": 381092, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ce9b13d1-3415-4af5-9ecc-835e56370019": {"__data__": {"id_": "ce9b13d1-3415-4af5-9ecc-835e56370019", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eaa04127-3117-4679-bf23-b3936537248f", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "331a40a27680ec5f73fbc80873e30ebf2eac0e0a2092401d8a4ca92a9ff2426c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fed2fb97-023e-4292-824d-9fd90a12ccff", "node_type": "1", "metadata": {}, "hash": "a07a8861fd79b72c23f26311aa82133f551c96b078138b3920328c1518712090", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "},\n\"webhook_endpoint\"\n: webhook_endpoint\n# Optional fields for priority, max_time_in_queue_seconds, etc\n},\n)\nprint\n(resp.json())\nCreate an async request by calling a model\u2019s\n/async_predict\nendpoint. See the\nasync inference API reference\nfor more endpoint details.\n\u200b\nGetting and canceling async requests\nGet async request details\nCancel async request\nYou may get the status of an async request for up to 1 hour after the request has been completed.\nPython\nCopy\nAsk AI\nimport\nrequests\nimport\nos\nmodel_id\n=\n\"\"\nrequest_id\n=\n\"\"\n# Read secrets from environment variables\nbaseten_api_key\n=\nos.environ[\n\"BASETEN_API_KEY\"\n]\nresp\n=\nrequests.get(\nf\n\"https://model-\n{\nmodel_id\n}\n.api.baseten.co/async_request/\n{\nrequest_id\n}\n\"\n,\nheaders\n=\n{\n\"Authorization\"\n:\nf\n\"Api-Key\n{\nbaseten_api_key\n}\n\"\n}\n)\nprint\n(resp.json())\nYou may get the status of an async request for up to 1 hour after the request has been completed.\nPython\nCopy\nAsk AI\nimport\nrequests\nimport\nos\nmodel_id\n=\n\"\"\nrequest_id\n=\n\"\"\n# Read secrets from environment variables\nbaseten_api_key\n=\nos.environ[\n\"BASETEN_API_KEY\"\n]\nresp\n=\nrequests.get(\nf\n\"https://model-\n{\nmodel_id\n}\n.api.baseten.co/async_request/\n{\nrequest_id\n}\n\"\n,\nheaders\n=\n{\n\"Authorization\"\n:\nf\n\"Api-Key\n{\nbaseten_api_key\n}\n\"\n}\n)\nprint\n(resp.json())\nPython\nCopy\nAsk AI\nimport\nrequests\nimport\nos\nmodel_id\n=\n\"\"\nrequest_id\n=\n\"\"\n# Read secrets from environment variables\nbaseten_api_key\n=\nos.environ[\n\"BASETEN_API_KEY\"\n]\nresp\n=\nrequests.delete(\nf\n\"https://model-\n{\nmodel_id\n}\n.api.baseten.co/async_request/\n{\nrequest_id\n}\n\"\n,\nheaders\n=\n{\n\"Authorization\"\n:\nf\n\"Api-Key\n{\nbaseten_api_key\n}\n\"\n}\n)\nprint\n(resp.json())\nManage async requests using the\nget async request API endpoint\nand the\ncancel async request API endpoint\n.\n\u200b\nProcessing async predict results\nBaseten does not store async predict results. Ensure that prediction outputs are either processed by your webhook, or saved to cloud storage in your model code (for example, in your model\u2019s\npostprocess\nmethod).\nIf a webhook endpoint was provided in the\n/async_predict\nrequest, the async predict results will be sent in a\nPOST\nrequest to the webhook endpoint. Errors in executing the async prediction will be included in the\nerrors\nfield of the async predict result.\nAsync predict result schema:\nrequest_id\n(string): the ID of the completed async request. This matches the\nrequest_id\nfield of the\n/async_predict\nresponse.\nmodel_id\n(string): the ID of the model that executed the request\ndeployment_id\n(string): the ID of the deployment that executed the request\ntype\n(string): the type of the async predict result. This will always be\n\"async_request_completed\"\n, even in error cases.\ntime\n(datetime): the time in UTC at which the request was sent to the webhook\ndata\n(dict or string): the prediction output\nerrors\n(list): any errors that occurred in processing the async request\nExample async predict result:\nCopy\nAsk AI\n{\n\"request_id\"\n:\n\"9876543210abcdef1234567890fedcba\"\n,\n\"model_id\"\n:\n\"my_model_id\"\n,\n\"deployment_id\"\n:\n\"my_deployment_id\"\n,\n\"type\"\n:\n\"async_request_completed\"\n,\n\"time\"\n:\n\"2024-04-30T01:01:08.883423Z\"\n,\n\"data\"\n: {\n\"my_model_output\"\n:\n\"hello world!\"\n},\n\"errors\"\n: []\n}\n\u200b\nObservability\nMetrics for async request execution are available on the\nMetrics tab\nof your model dashboard.\nAsync requests are included in inference latency and volume metrics.\nA time in async queue chart displays the time an async predict request spent in the\nQUEUED\nstate before getting processed by the model.\nA async queue size chart displays the current number of queued async predict requests.\nThe time in async queue chart.\n\u200b\nSecuring async inference\nSince async predict results are sent to a webhook available to anyone over the internet with the endpoint, you\u2019ll want to have some verification that these results sent to the webhook are actually coming from Baseten.\nWe recommend leveraging webhook signatures to secure webhook payloads and ensure they are from Baseten.\nThis is a two-step process:\nCreate a webhook secret.", "mimetype": "text/plain", "start_char_idx": 380829, "end_char_idx": 384807, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fed2fb97-023e-4292-824d-9fd90a12ccff": {"__data__": {"id_": "fed2fb97-023e-4292-824d-9fd90a12ccff", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ce9b13d1-3415-4af5-9ecc-835e56370019", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "0f8d68ab7838290cda75b4bf200a0752bcd1c625c477af954f97141ad4dc730c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4640ee57-adcc-428f-b26f-98898c03b923", "node_type": "1", "metadata": {}, "hash": "4db1078362508d58e7809370467f1002278c26de80df39e884eff199bec6120c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "},\n\"errors\"\n: []\n}\n\u200b\nObservability\nMetrics for async request execution are available on the\nMetrics tab\nof your model dashboard.\nAsync requests are included in inference latency and volume metrics.\nA time in async queue chart displays the time an async predict request spent in the\nQUEUED\nstate before getting processed by the model.\nA async queue size chart displays the current number of queued async predict requests.\nThe time in async queue chart.\n\u200b\nSecuring async inference\nSince async predict results are sent to a webhook available to anyone over the internet with the endpoint, you\u2019ll want to have some verification that these results sent to the webhook are actually coming from Baseten.\nWe recommend leveraging webhook signatures to secure webhook payloads and ensure they are from Baseten.\nThis is a two-step process:\nCreate a webhook secret.\nValidate a webhook signature sent as a header along with the webhook request payload.\n\u200b\nCreating webhook secrets\nWebhook secrets can be generated via the\nSecrets tab\n.\nGenerate a webhook secret with the \"Add webhook secret\" button.\nA webhook secret looks like:\nCopy\nAsk AI\nwhsec_AbCdEf123456GhIjKlMnOpQrStUvWxYz12345678\nEnsure this webhook secret is saved securely. It can be viewed at any time and\nrotated if necessary\nin the Secrets tab.\n\u200b\nValidating webhook signatures\nIf a webhook secret exists, Baseten will include a webhook signature in the\n\"X-BASETEN-SIGNATURE\"\nheader of the webhook request so you can verify that it is coming from Baseten.\nA Baseten signature header looks like:\n\"X-BASETEN-SIGNATURE\": \"v1=signature\"\nWhere\nsignature\nis an\nHMAC\ngenerated using a\nSHA-256\nhash function calculated over the whole async predict result and signed using a webhook secret.\nIf multiple webhook secrets are active, a signature will be generated using each webhook secret. In the example below, the newer webhook secret was used to create\nnewsignature\nand the older (soon to expire) webhook secret was used to create\noldsignature\n.\n\"X-BASETEN-SIGNATURE\": \"v1=newsignature,v1=oldsignature\"\nTo validate a Baseten signature, we recommend the following. A full Baseten signature validation example can be found in\nthis Repl\n.\n1\nCompare timestamps\nCompare the async predict result timestamp with the current time and decide if it was received within an acceptable tolerance window.\nCopy\nAsk AI\nTIMESTAMP_TOLERANCE_SECONDS\n=\n300\n# Check timestamp in async predict result against current time to ensure its within our tolerance\nif\n(datetime.now(timezone.utc)\n-\nasync_predict_result.time).total_seconds()\n>\nTIMESTAMP_TOLERANCE_SECONDS\n:\nlogging.error(\nf\n\"Async predict result was received after\n{\nTIMESTAMP_TOLERANCE_SECONDS\n}\nseconds and is considered stale, Baseten signature was not validated.\"\n)\n2\nRecompute Baseten signature\nRecreate the Baseten signature using webhook secret(s) and the async predict result.\nCopy\nAsk AI\nWEBHOOK_SECRETS\n=\n[]\n# Add your webhook secrets here\nasync_predict_result_json\n=\nasync_predict_result.model_dump_json()\n# We recompute expected Baseten signatures with each webhook secret\nfor\nwebhook_secret\nin\nWEBHOOK_SECRETS\n:\nfor\nactual_signature\nin\nbaseten_signature.replace(\n\"v1=\"\n,\n\"\"\n).split(\n\",\"\n):\nexpected_signature\n=\nhmac.digest(\nwebhook_secret.encode(\n\"utf-8\"\n),\nasync_predict_result_json.encode(\n\"utf-8\"\n),\nhashlib.sha256,\n).hex()\n3\nCompare signatures\nCompare the expected Baseten signature with the actual computed signature using\ncompare_digest\n, which will return a boolean representing whether the signatures are indeed the same.\nCopy\nAsk AI\nhmac.compare_digest(expected_signature, actual_signature)\n\u200b\nKeeping webhook secrets secure\nWe recommend periodically rotating webhook secrets.\nIn the event that a webhook secret is exposed, you\u2019re able to rotate or remove it.\nRotating a secret in the UI will set the existing webhook secret to expire in 24 hours, and generate a new webhook secret. During this period, Baseten will include multiple signatures in the signature headers.\nRemoving webhook secrets could cause your signature validation to fail. Recreate a webhook secret after deleting and ensure your signature validation code is up to date with the new webhook secret.\n\u200b\nFAQs\n\u200b\nCan I run sync and async requests on the same model?\nYes, you can run both sync and async requests on the same model. Sync requests always take priority over async requests.", "mimetype": "text/plain", "start_char_idx": 383954, "end_char_idx": 388301, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4640ee57-adcc-428f-b26f-98898c03b923": {"__data__": {"id_": "4640ee57-adcc-428f-b26f-98898c03b923", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fed2fb97-023e-4292-824d-9fd90a12ccff", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "c1c8122cb3d700febe192e27e570b9330878e206304801b38408c40f856c8649", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a87d2cde-9e07-4bd8-9bab-4f2b7b3515e9", "node_type": "1", "metadata": {}, "hash": "8ce2ad7e835c4fc75eadaddf1434167a4142fb36916e243335bc256448d8d93a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Copy\nAsk AI\nhmac.compare_digest(expected_signature, actual_signature)\n\u200b\nKeeping webhook secrets secure\nWe recommend periodically rotating webhook secrets.\nIn the event that a webhook secret is exposed, you\u2019re able to rotate or remove it.\nRotating a secret in the UI will set the existing webhook secret to expire in 24 hours, and generate a new webhook secret. During this period, Baseten will include multiple signatures in the signature headers.\nRemoving webhook secrets could cause your signature validation to fail. Recreate a webhook secret after deleting and ensure your signature validation code is up to date with the new webhook secret.\n\u200b\nFAQs\n\u200b\nCan I run sync and async requests on the same model?\nYes, you can run both sync and async requests on the same model. Sync requests always take priority over async requests. Keep the following in mind:\nRate Limits\n: Ensure you adhere to rate limits, as they apply to async requests.\nLearn more\nConcurrency\n: Both sync and async requests count toward the total number of concurrent requests.\nLearn more\nWas this page helpful?\nYes\nNo\nPrevious\nStructured output (JSON mode)\nEnforce an output schema on LLM inference\nNext\nOn this page\nQuick start\nUser guide\nConfiguring the webhook endpoint\nMaking async requests\nGetting and canceling async requests\nProcessing async predict results\nObservability\nSecuring async inference\nCreating webhook secrets\nValidating webhook signatures\nKeeping webhook secrets secure\nFAQs\nCan I run sync and async requests on the same model?\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 387473, "end_char_idx": 389058, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a87d2cde-9e07-4bd8-9bab-4f2b7b3515e9": {"__data__": {"id_": "a87d2cde-9e07-4bd8-9bab-4f2b7b3515e9", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4640ee57-adcc-428f-b26f-98898c03b923", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "a147044186f2e92a54bd85ecd01c1895ab556fe0d5ca2fd98e2a0af80e5a9662", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "df7326bb-8486-447e-a459-21a7db120eee", "node_type": "1", "metadata": {}, "hash": "f1c2152e6e990e8241131435c36c5a4df4726529c748078f0f9549a4a0f7f305", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Yes, you can run both sync and async requests on the same model. Sync requests always take priority over async requests. Keep the following in mind:\nRate Limits\n: Ensure you adhere to rate limits, as they apply to async requests.\nLearn more\nConcurrency\n: Both sync and async requests count toward the total number of concurrent requests.\nLearn more\nWas this page helpful?\nYes\nNo\nPrevious\nStructured output (JSON mode)\nEnforce an output schema on LLM inference\nNext\nOn this page\nQuick start\nUser guide\nConfiguring the webhook endpoint\nMaking async requests\nGetting and canceling async requests\nProcessing async predict results\nObservability\nSecuring async inference\nCreating webhook secrets\nValidating webhook signatures\nKeeping webhook secrets secure\nFAQs\nCan I run sync and async requests on the same model?\nAssistant\nResponses are generated using AI and may contain mistakes.\n\n\nContent from https://docs.baseten.co/inference/calling-your-model:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nDeveloping a Chain\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nExporting metrics\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nInference\nCall your model\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nOnce deployed, your model is accessible via an\nAPI endpoint\n. To make an inference request, you\u2019ll need:\nModel ID\nAn\nAPI key\nfor your Baseten account.\nJSON-serializable model input\n\u200b\nPredict API endpoints\nBaseten provides multiple endpoints for different inference modes:\n/predict\n\u2013 Standard synchronous inference.\n/async_predict\n\u2013 Asynchronous inference for long-running tasks.\nEndpoints are available for environments and all deployments. See the\nAPI reference\nfor details.\n\u200b\nSync API endpoints\nCustom servers support both\npredict\nendpoints as well as a special\nsync\nendpoint. By using the\nsync\nendpoint you are able to call different routes in your custom server.\nCopy\nAsk AI\nhttps://model-{model-id}.api.baseten.co/environments/{production}/sync/{route}\nHere are a few example for the given example that show how the sync endpoint maps to the custom server\u2019s routes.\nhttps://model-{model_id}.../sync/health\n->\n/health\nhttps://model-{model_id}.../sync/items\n->\n/items\nhttps://model-{model_id}.../sync/items/123\n->\n/items/123\n\u200b\nOpenAI SDK\nWhen deploying a model with Engine Builder, you will get an OpenAI compatible server. If you are already using one of the OpenAI SDKs, you will simply need to update the base url to your Baseten model URL and include your Baseten API Key.\nCopy\nAsk AI\nimport\nos\nfrom\nopenai\nimport\nOpenAI\nmodel_id\n=\n\"abcdef\"\n#\nTODO\n: replace with your model id\napi_key\n=\nos.environ.get(\n\"BASETEN_API_KEY\"\n)\nmodel_url\n=\nf\n\"https://model-\n{\nmodel_id\n}\n.api.baseten.co/environments/production/sync/v1\"\nclient\n=\nOpenAI(\nbase_url\n=\nmodel_url,\napi_key\n=\napi_key,\n)\nstream\n=\nclient.chat.completions.create(\nmodel\n=\n\"baseten\"\n,\nmessages\n=\n[\n{\n\"role\"\n:\n\"system\"\n,\n\"content\"\n:\n\"You are a helpful assistant.\"\n},\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"What is the capital of France?\"\n}\n],\nstream\n=\nTrue\n,\n)\nfor\nchunk\nin\nstream:\nif\nchunk.choices[\n0\n].delta.content\nis\nnot\nNone\n:\nprint\n(chunk.choices[\n0\n].delta.content,\nend\n=\n\"\"\n)\n\u200b\nAlternative invocation methods\nTruss CLI\n:\ntruss predict\nModel Dashboard\n: \u201cPlayground\u201d button in the Baseten UI\nWas this page helpful?\nYes\nNo\nPrevious\nStreaming\nHow to call a model that has a streaming-capable endpoint.\nNext\nOn this page\nPredict API endpoints\nSync API endpoints\nOpenAI SDK\nAlternative invocation methods\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 388181, "end_char_idx": 392207, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "df7326bb-8486-447e-a459-21a7db120eee": {"__data__": {"id_": "df7326bb-8486-447e-a459-21a7db120eee", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a87d2cde-9e07-4bd8-9bab-4f2b7b3515e9", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "e163f87d5fea848d75e47171d57358c3bcbead3eecc6c451477905bdb4a6d3bf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d0be6438-6762-4c4c-b539-179149aea4ff", "node_type": "1", "metadata": {}, "hash": "4c69b48034b6c01ed0c902331c9339ed220b32c4a23883a9035089ab3e3fc89c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Content from https://docs.baseten.co/inference/concepts:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nDeveloping a Chain\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nExporting metrics\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nInference\nConcepts\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nInference on Baseten is designed for flexibility, efficiency, and scalability. Models can be served\nsynchronously\n,\nasynchronously\n, or via\nstreaming\nto meet different performance and latency needs.\nSynchronously\ninference is ideal for low-latency, real-time responses.\nAsynchronously\ninference handles long-running tasks efficiently without blocking resources.\nStreaming\ninference delivers partial results as they become available for faster response times.\nBaseten supports various input and output formats, including structured data, binary files, and function calls, making it adaptable to different workloads.\nWas this page helpful?\nYes\nNo\nPrevious\nCall your model\nRun inference on deployed models\nNext\nAssistant\nResponses are generated using AI and may contain mistakes.\n\n\nContent from https://docs.baseten.co/inference/function-calling:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nDeveloping a Chain\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nStructured output (JSON mode)\nFunction calling (tool use)\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nExporting metrics\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nStructured LLM output\nFunction calling (tool use)\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nFunction calling requires an LLM deployed using the\nTensorRT-LLM Engine\nBuilder\n.\nTo use function calling:\nDefine a set of functions/tools in Python.\nPass the function set to the LLM with the\ntools\nargument.\nReceive selected function(s) as output.\nWith function calling, it\u2019s essential to understand that\nthe LLM itself is not capable of executing the code in the function\n. Instead, the LLM is used to suggest appropriate function(s), if they exist, based on the prompt. Any code execution must be handled outside of the LLM call \u2013 a great use for\nchains\n.\n\u200b\nDefine functions in Python\nFunctions can be anything: API calls, ORM access, SQL queries, or just a script. It\u2019s essential that functions are well-documented; the LLM relies on the docstrings to select the correct function.", "mimetype": "text/plain", "start_char_idx": 392210, "end_char_idx": 395473, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d0be6438-6762-4c4c-b539-179149aea4ff": {"__data__": {"id_": "d0be6438-6762-4c4c-b539-179149aea4ff", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "df7326bb-8486-447e-a459-21a7db120eee", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "f827c78fe302eb5d2c0b5b9498cfd3f8b61610c4f252c20aa5c97cd16dd74fa5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f28ec727-26f4-462c-9052-b05097fcb0b2", "node_type": "1", "metadata": {}, "hash": "eef7949907177ea5bee091d9aee73e2fc172da9596cd9407861e0b7fb20793e1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To use function calling:\nDefine a set of functions/tools in Python.\nPass the function set to the LLM with the\ntools\nargument.\nReceive selected function(s) as output.\nWith function calling, it\u2019s essential to understand that\nthe LLM itself is not capable of executing the code in the function\n. Instead, the LLM is used to suggest appropriate function(s), if they exist, based on the prompt. Any code execution must be handled outside of the LLM call \u2013 a great use for\nchains\n.\n\u200b\nDefine functions in Python\nFunctions can be anything: API calls, ORM access, SQL queries, or just a script. It\u2019s essential that functions are well-documented; the LLM relies on the docstrings to select the correct function.\nAs a simple example, consider the four basic functions of a calculator:\nCopy\nAsk AI\ndef\nmultiply\n(\na\n:\nfloat\n,\nb\n:\nfloat\n):\n\"\"\"\nA function that multiplies two numbers\nArgs:\na: The first number to multiply\nb: The second number to multiply\n\"\"\"\nreturn\na\n*\nb\ndef\ndivide\n(\na\n:\nfloat\n,\nb\n:\nfloat\n):\n\"\"\"\nA function that divides two numbers\nArgs:\na: The dividend\nb: The divisor\n\"\"\"\nreturn\na\n/\nb\ndef\nadd\n(\na\n:\nfloat\n,\nb\n:\nfloat\n):\n\"\"\"\nA function that adds two numbers\nArgs:\na: The first number\nb: The second number\n\"\"\"\nreturn\na\n+\nb\ndef\nsubtract\n(\na\n:\nfloat\n,\nb\n:\nfloat\n):\n\"\"\"\nA function that subtracts two numbers\nArgs:\na: The number to subtract from\nb: The number to subtract\n\"\"\"\nreturn\na\n-\nb\nThese functions must be serialized into LLM-accessible tools:\nCopy\nAsk AI\nfrom\ntransformers.utils\nimport\nget_json_schema\ncalculator_functions\n=\n{\n'multiply'\n: multiply,\n'divide'\n: divide,\n'add'\n: add,\n'subtract'\n: subtract\n}\ntools\n=\n[get_json_schema(f)\nfor\nf\nin\ncalculator_functions.values()]\n\u200b\nPass functions to the LLM\nThe input spec for models like Llama 3.1 includes a\ntools\nkey that we use to pass the functions:\nCopy\nAsk AI\nimport\njson\nimport\nrequests\npayload\n=\n{\n\"messages\"\n: [\n{\n\"role\"\n:\n\"system\"\n,\n\"content\"\n:\n\"You are a helpful assistant\"\n},\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"What is 3.14+3.14?\"\n},\n],\n\"tools\"\n: tools,\n# tools are provided in the same format as OpenAI's API\n\"tool_choice\"\n:\n\"auto\"\n,\n# auto is default - the model will choose whether or not to make a function call\n}\nMODEL_ID\n=\n\"\"\nBASETEN_API_KEY\n=\n\"\"\nresp\n=\nrequests.post(\nf\n\"https://model-\n{\nMODEL_ID\n}\n.api.baseten.co/production/predict\"\n,\nheaders\n=\n{\n\"Authorization\"\n:\nf\n\"Api-Key\n{\nBASETEN_API_KEY\n}\n\"\n},\njson\n=\npayload,\n)\n\u200b\ntool_choice: auto (default) \u2013 may return a function\nThe default\ntool_choice\noption,\nauto\n, leaves it up to the LLM whether to return one function, multiple functions, or no functions at all, depending on what the model feels is most appropriate based on the prompt.\n\u200b\ntool_choice: required \u2013 will always return a function\nThe\nrequired\noption for\ntool_choice\nmeans that the LLM is guaranteed to chose at least one function, no matter what.\n\u200b\ntool_choice: none \u2013 will always return a function\nThe\nnone\noption for\ntool_choice\nmeans that the LLM will\nnot\nreturn a function, and will instead produce ordinary text output. This is useful when you want to provide the full context of a conversation without adding and dropping the\ntools\nparameter call-by-call.\n\u200b\ntool_choice: direct \u2013 will return a specified function\nYou can also pass a specific function directly into the call, which is guaranteed to be returned. This is useful if you want to hardcode specific behavior into your model call for testing or conditional execution.", "mimetype": "text/plain", "start_char_idx": 394772, "end_char_idx": 398187, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f28ec727-26f4-462c-9052-b05097fcb0b2": {"__data__": {"id_": "f28ec727-26f4-462c-9052-b05097fcb0b2", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d0be6438-6762-4c4c-b539-179149aea4ff", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "29966fce64be49f7dc77908a7ba95ea690094c07f2fa2b78d10f3b43b9597f57", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1480a717-409e-436a-9d8f-95ebfcd01aeb", "node_type": "1", "metadata": {}, "hash": "7e837d3c5971468111bbda7a3309e404faaae3d9523d895e0680989bf7412e98", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u200b\ntool_choice: required \u2013 will always return a function\nThe\nrequired\noption for\ntool_choice\nmeans that the LLM is guaranteed to chose at least one function, no matter what.\n\u200b\ntool_choice: none \u2013 will always return a function\nThe\nnone\noption for\ntool_choice\nmeans that the LLM will\nnot\nreturn a function, and will instead produce ordinary text output. This is useful when you want to provide the full context of a conversation without adding and dropping the\ntools\nparameter call-by-call.\n\u200b\ntool_choice: direct \u2013 will return a specified function\nYou can also pass a specific function directly into the call, which is guaranteed to be returned. This is useful if you want to hardcode specific behavior into your model call for testing or conditional execution.\nCopy\nAsk AI\n\"tool_choice\"\n: {\n\"type\"\n:\n\"function\"\n,\n\"function\"\n: {\n\"name\"\n:\n\"subtract\"\n}}\n\u200b\nReceive function(s) as output\nWhen the model returns functions, they\u2019ll be a list that can be parsed as follows:\nCopy\nAsk AI\nfunc_calls\n=\njson.loads(resp.text)\n# In this example, we execute the first function (one of +-/*) on the provided parameters\nfunc_call\n=\nfunc_calls[\n0\n]\ncalculator_functions[func_call[\n\"name\"\n]](\n**\nfunc_call[\n\"parameters\"\n])\nAfter reading the LLM\u2019s selection, your execution environment can run the necessary functions. For more on combining LLMs with other logic, see the\nchains documentation\n.\nWas this page helpful?\nYes\nNo\nPrevious\nModel I/O in binary\nDecode and save binary model output\nNext\nOn this page\nDefine functions in Python\nPass functions to the LLM\ntool_choice: auto (default) \u2013 may return a function\ntool_choice: required \u2013 will always return a function\ntool_choice: none \u2013 will always return a function\ntool_choice: direct \u2013 will return a specified function\nReceive function(s) as output\nAssistant\nResponses are generated using AI and may contain mistakes.\n\n\nContent from https://docs.baseten.co/inference/integrations:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nDeveloping a Chain\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nExporting metrics\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nInference\nIntegrations\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nChainlit\nBuild your own open-source ChatGPT with Baseten and Chainlit.\nLangChain\nUse your Baseten models in the LangChain ecosystem.\nLiteLLM\nUse your Baseten models in LiteLLM projects.\nLiveKit\nBuild real-time voice agents with TTS models hosted on Baseten.\nBuild your own\nWant to integrate Baseten with your platform or project? Reach out to\nsupport@baseten.co\nand we\u2019ll help with building\nand marketing the integration.\nWas this page helpful?\nYes\nNo\nPrevious\nOverview\nAn introduction to Baseten Training for streamlining and managing the model training lifecycle.\nNext\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 397429, "end_char_idx": 400729, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1480a717-409e-436a-9d8f-95ebfcd01aeb": {"__data__": {"id_": "1480a717-409e-436a-9d8f-95ebfcd01aeb", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f28ec727-26f4-462c-9052-b05097fcb0b2", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "2dbc1b27df0235af849c9611819dbf3d18e9e392300b70fbc6ca346d35d7f78c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "357c81f4-568e-44a0-8265-07314416218e", "node_type": "1", "metadata": {}, "hash": "e9174c9173b82665851a3a7274bec12ae557c1b5d3333d5a8907a54c71a29c5b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Content from https://docs.baseten.co/inference/output-format/binary:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nDeveloping a Chain\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nModel I/O in binary\nModel I/O with files\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nExporting metrics\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nOutput formats\nModel I/O in binary\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nBaseten and Truss natively support model I/O in binary and use msgpack encoding for efficiency.\n\u200b\nDeploy a basic Truss for binary I/O\nIf you need a deployed model to try the invocation examples below, follow these steps to create and deploy a super basic Truss that accepts and returns binary data. The Truss performs no operations and is purely illustrative.\nSteps for deploying an example Truss\n1\nCreate a Truss\nTo create a Truss, run:\nCopy\nAsk AI\ntruss\ninit\nbinary_test\nThis creates a Truss in a new directory\nbinary_test\n. By default, newly created Trusses implement an identity function that returns the exact input they are given.\n2\nAdd logging\nOptionally, modify\nbinary_test/model/model.py\nto log that the data received is of type\nbytes\n:\nbinary_test/model/model.py\nCopy\nAsk AI\ndef\npredict\n(\nself\n,\nmodel_input\n):\n# Run model inference here\nprint\n(\nf\n\"Input type:\n{\ntype\n(model_input[\n'byte_data'\n])\n}\n\"\n)\nreturn\nmodel_input\n3\nDeploy the Truss\nDeploy the Truss to Baseten with:\nCopy\nAsk AI\ntruss\npush\n\u200b\nSend raw bytes as model input\nTo send binary data as model input:\nSet the\ncontent-type\nHTTP header to\napplication/octet-stream\nUse\nmsgpack\nto encode the data or file\nMake a POST request to the model\nThis code sample assumes you have a file\nGettysburg.mp3\nin the current working directory. You can download the\n11-second file from our CDN\nor replace it with your own file.\ncall_model.py\nCopy\nAsk AI\nimport\nos\nimport\nrequests\nimport\nmsgpack\nmodel_id\n=\n\"MODEL_ID\"\n# Replace with your model ID\ndeployment\n=\n\"development\"\n# `development`, `production`, or a deployment ID\nbaseten_api_key\n=\nos.environ[\n\"BASETEN_API_KEY\"\n]\n# Specify the URL to which you want to send the POST request\nurl\n=\nf\n\"https://model-\n{\nmodel_id\n}\n.api.baseten.co/\n{\ndeployment\n}\n/predict\"\nheaders\n=\n{\n\"Authorization\"\n:\nf\n\"Api-Key\n{\nbaseten_api_key\n}\n\"\n,\n\"content-type\"\n:\n\"application/octet-stream\"\n,\n}\nwith\nopen\n(\n'Gettysburg.mp3'\n,\n'rb'\n)\nas\nfile\n:\nresponse\n=\nrequests.post(\nurl,\nheaders\n=\nheaders,\ndata\n=\nmsgpack.packb({\n'byte_data'\n:\nfile\n.read()})\n)\nprint\n(response.status_code)\nprint\n(response.headers)\nTo support certain types like numpy and datetime values, you may need to\nextend client-side\nmsgpack\nencoding with the same\nencoder and decoder used\nby\nTruss\n.\n\u200b\nParse raw bytes from model output\nTo use the output of a non-streaming model response, decode the response content.\ncall_model.py\nCopy\nAsk AI\n# Continues `call_model.py` from above\nbinary_output\n=\nmsgpack.unpackb(response.content)\n# Change extension if not working with mp3 data\nwith\nopen\n(\n'output.mp3'\n,\n'wb'\n)\nas\nfile\n:\nfile\n.write(binary_output[\n\"byte_data\"\n])\n\u200b\nStreaming binary outputs\nYou can also stream output as binary. This is useful for sending large files or reading binary output as it is generated.\nIn the\nmodel.py\n, you must create a streaming output.", "mimetype": "text/plain", "start_char_idx": 400732, "end_char_idx": 404417, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "357c81f4-568e-44a0-8265-07314416218e": {"__data__": {"id_": "357c81f4-568e-44a0-8265-07314416218e", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1480a717-409e-436a-9d8f-95ebfcd01aeb", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "2157a7d1a28e85907355b50ad94c0e6253e3f6c4e54bb19acedea3a752e9403b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bf9024cd-66d8-4658-9e3a-00d897dc74b8", "node_type": "1", "metadata": {}, "hash": "a2ae26ad6d0d830b5ee931faf867047719270ceb8e905b5bef3fa1b80c6086fd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u200b\nParse raw bytes from model output\nTo use the output of a non-streaming model response, decode the response content.\ncall_model.py\nCopy\nAsk AI\n# Continues `call_model.py` from above\nbinary_output\n=\nmsgpack.unpackb(response.content)\n# Change extension if not working with mp3 data\nwith\nopen\n(\n'output.mp3'\n,\n'wb'\n)\nas\nfile\n:\nfile\n.write(binary_output[\n\"byte_data\"\n])\n\u200b\nStreaming binary outputs\nYou can also stream output as binary. This is useful for sending large files or reading binary output as it is generated.\nIn the\nmodel.py\n, you must create a streaming output.\nmodel/model.py\nCopy\nAsk AI\n# Replace the predict function in your Truss\ndef\npredict\n(\nself\n,\nmodel_input\n):\nimport\nos\ncurrent_dir\n=\nos.path.dirname(\n__file__\n)\nfile_path\n=\nos.path.join(current_dir,\n\"tmpfile.txt\"\n)\nwith\nopen\n(file_path,\nmode\n=\n\"wb\"\n)\nas\nfile\n:\nfile\n.write(\nbytes\n(model_input[\n\"text\"\n],\nencoding\n=\n\"utf-8\"\n))\ndef\niterfile\n():\n# Get the directory of the current file\ncurrent_dir\n=\nos.path.dirname(\n__file__\n)\n# Construct the full path to the .wav file\nfile_path\n=\nos.path.join(current_dir,\n\"tmpfile.txt\"\n)\nwith\nopen\n(file_path,\nmode\n=\n\"rb\"\n)\nas\nfile_like:\nyield from\nfile_like\nreturn\niterfile()\nThen, in your client, you can use streaming output directly without decoding.\nstream_model.py\nCopy\nAsk AI\nimport\nos\nimport\nrequests\nimport\njson\nmodel_id\n=\n\"MODEL_ID\"\n# Replace with your model ID\ndeployment\n=\n\"development\"\n# `development`, `production`, or a deployment ID\nbaseten_api_key\n=\nos.environ[\n\"BASETEN_API_KEY\"\n]\n# Specify the URL to which you want to send the POST request\nurl\n=\nf\n\"https://model-\n{\nmodel_id\n}\n.api.baseten.co/\n{\ndeployment\n}\n/predict\"\nheaders\n=\n{\n\"Authorization\"\n:\nf\n\"Api-Key\n{\nbaseten_api_key\n}\n\"\n,\n}\ns\n=\nrequests.Session()\nwith\ns.post(\n# Endpoint for production deployment, see API reference for more\nf\n\"https://model-\n{\nmodel_id\n}\n.api.baseten.co/\n{\ndeployment\n}\n/predict\"\n,\nheaders\n=\n{\n\"Authorization\"\n:\nf\n\"Api-Key\n{\nbaseten_api_key\n}\n\"\n},\ndata\n=\njson.dumps({\n\"text\"\n:\n\"Lorem Ipsum\"\n}),\n# Include stream=True as an argument so the requests libray knows to stream\nstream\n=\nTrue\n,\n)\nas\nresponse:\nfor\ntoken\nin\nresponse.iter_content(\n1\n):\nprint\n(token)\n# Prints bytes\nWas this page helpful?\nYes\nNo\nPrevious\nModel I/O with files\nCall models by passing a file or URL\nNext\nOn this page\nDeploy a basic Truss for binary I/O\nSend raw bytes as model input\nParse raw bytes from model output\nStreaming binary outputs\nAssistant\nResponses are generated using AI and may contain mistakes.\n\n\nContent from https://docs.baseten.co/inference/output-format/files:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nDeveloping a Chain\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nModel I/O in binary\nModel I/O with files\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nExporting metrics\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nOutput formats\nModel I/O with files\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nBaseten supports a wide variety of file-based I/O approaches. These examples show our recommendations for working with files during model inference, whether local or remote, public or private, in the Truss or in your invocation code.\n\u200b\nFiles as input\n\u200b\nExample: Send a file with JSON-serializable content\nThe Truss CLI has a\n-f\nflag to pass file input.", "mimetype": "text/plain", "start_char_idx": 403848, "end_char_idx": 407557, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bf9024cd-66d8-4658-9e3a-00d897dc74b8": {"__data__": {"id_": "bf9024cd-66d8-4658-9e3a-00d897dc74b8", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "357c81f4-568e-44a0-8265-07314416218e", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "6abb5c3b09153802b73987ccc37757f3aabddf2334fd817d91b608e0e23f0af2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5d355b83-d0d9-4440-b837-aff4af36d149", "node_type": "1", "metadata": {}, "hash": "b37ef7f7fac601b3bf37c74e94ae8fada01da97f0dd68aa764d3adb9a5d7f8ae", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "These examples show our recommendations for working with files during model inference, whether local or remote, public or private, in the Truss or in your invocation code.\n\u200b\nFiles as input\n\u200b\nExample: Send a file with JSON-serializable content\nThe Truss CLI has a\n-f\nflag to pass file input. If you\u2019re using the API endpoint via Python, get file contents with the standard\nf.read()\nfunction.\nTruss CLI\nPython script\nCopy\nAsk AI\ntruss\npredict\n-f\ninput.json\n\u200b\nExample: Send a file with non-serializable content\nThe\n-f\nflag for\ntruss predict\nonly applies to JSON-serializable content. For other files, like the audio files required by\nMusicGen Melody\n, the file content needs to be base64 encoded before it is sent.\nCopy\nAsk AI\nimport\nurllib3\nmodel_id\n=\n\"\"\n# Read secrets from environment variables\nbaseten_api_key\n=\nos.environ[\n\"BASETEN_API_KEY\"\n]\n# Open a local file\nwith\nopen\n(\n\"mymelody.wav\"\n,\n\"rb\"\n)\nas\nf:\n# mono wav file, 48khz sample rate\n# Convert file contents into JSON-serializable format\nencoded_data\n=\nbase64.b64encode(f.read())\nencoded_str\n=\nencoded_data.decode(\n\"utf-8\"\n)\n# Define the data payload\ndata\n=\n{\n\"prompts\"\n: [\n\"happy rock\"\n,\n\"energetic EDM\"\n,\n\"sad jazz\"\n],\n\"melody\"\n: encoded_str,\n\"duration\"\n:\n8\n}\n# Make the POST request\nresponse\n=\nrequests.post(url,\nheaders\n=\nheaders,\ndata\n=\ndata)\nresp\n=\nurllib3.request(\n\"POST\"\n,\n# Endpoint for production deployment, see API reference for more\nf\n\"https://model-\n{\nmodel_id\n}\n.api.baseten.co/production/predict\"\n,\nheaders\n=\n{\n\"Authorization\"\n:\nf\n\"Api-Key\n{\nbaseten_api_key\n}\n\"\n},\njson\n=\ndata\n)\ndata\n=\nresp.json()[\n\"data\"\n]\n# Save output to files\nfor\nidx, clip\nin\nenumerate\n(data):\nwith\nopen\n(\nf\n\"clip_\n{\nidx\n}\n.wav\"\n,\n\"wb\"\n)\nas\nf:\nf.write(base64.b64decode(clip))\n\u200b\nExample: Send a URL to a public file\nRather than encoding and serializing a file to send in the HTTP request, you can instead write a Truss that takes a URL as input and loads the content in the\npreprocess()\nfunction.\nHere\u2019s an example from\nWhisper in the model library\n.\nCopy\nAsk AI\nfrom\ntempfile\nimport\nNamedTemporaryFile\nimport\nrequests\n# Get file content without blocking GPU\ndef\npreprocess\n(\nself\n,\nrequest\n):\nresp\n=\nrequests.get(request[\n\"url\"\n])\nreturn\n{\n\"content\"\n: resp.content}\n# Use file content in model inference\ndef\npredict\n(\nself\n,\nmodel_input\n):\nwith\nNamedTemporaryFile()\nas\nfp:\nfp.write(model_input[\n\"content\"\n])\nresult\n=\nwhisper.transcribe(\nself\n._model,\nfp.name,\ntemperature\n=\n0\n,\nbest_of\n=\n5\n,\nbeam_size\n=\n5\n,\n)\nsegments\n=\n[\n{\n\"start\"\n: r[\n\"start\"\n],\n\"end\"\n: r[\n\"end\"\n],\n\"text\"\n: r[\n\"text\"\n]}\nfor\nr\nin\nresult[\n\"segments\"\n]\n]\nreturn\n{\n\"language\"\n: whisper.tokenizer.\nLANGUAGES\n[result[\n\"language\"\n]],\n\"segments\"\n: segments,\n\"text\"\n: result[\n\"text\"\n],\n}\n\u200b\nFiles as output\n\u200b\nExample: Save model output to local file\nWhen saving model output to a local file, there\u2019s nothing Baseten-specific about the code. Just use the standard\n>\noperator in bash or\nfile.write()\nfunction in Python to save the model output.\nTruss CLI\nPython script\nCopy\nAsk AI\ntruss\npredict\n-d\n'\"Model input!\"'\n>\noutput.json\nOutput for some models, like image and audio generation models, may need to be decoded before you save it. See our\nimage generation example\nfor how to parse base64 output.\nWas this page helpful?\nYes\nNo\nPrevious\nIntegrations\nIntegrate your models with tools like LangChain, LiteLLM, and more.", "mimetype": "text/plain", "start_char_idx": 407267, "end_char_idx": 410592, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5d355b83-d0d9-4440-b837-aff4af36d149": {"__data__": {"id_": "5d355b83-d0d9-4440-b837-aff4af36d149", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bf9024cd-66d8-4658-9e3a-00d897dc74b8", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "c3807a02d15450034964309248feca9a82d485b7f77e426af5b9a689ff2356cf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "372c2e82-3fff-426d-8220-64edfbd37215", "node_type": "1", "metadata": {}, "hash": "772230cd956ac8eb370c875e237b25e5de994625e398f5ec9629497092999ee7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "LANGUAGES\n[result[\n\"language\"\n]],\n\"segments\"\n: segments,\n\"text\"\n: result[\n\"text\"\n],\n}\n\u200b\nFiles as output\n\u200b\nExample: Save model output to local file\nWhen saving model output to a local file, there\u2019s nothing Baseten-specific about the code. Just use the standard\n>\noperator in bash or\nfile.write()\nfunction in Python to save the model output.\nTruss CLI\nPython script\nCopy\nAsk AI\ntruss\npredict\n-d\n'\"Model input!\"'\n>\noutput.json\nOutput for some models, like image and audio generation models, may need to be decoded before you save it. See our\nimage generation example\nfor how to parse base64 output.\nWas this page helpful?\nYes\nNo\nPrevious\nIntegrations\nIntegrate your models with tools like LangChain, LiteLLM, and more.\nNext\nOn this page\nFiles as input\nExample: Send a file with JSON-serializable content\nExample: Send a file with non-serializable content\nExample: Send a URL to a public file\nFiles as output\nExample: Save model output to local file\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 409877, "end_char_idx": 410891, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "372c2e82-3fff-426d-8220-64edfbd37215": {"__data__": {"id_": "372c2e82-3fff-426d-8220-64edfbd37215", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5d355b83-d0d9-4440-b837-aff4af36d149", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "eed532f4e2650cad0948b59a245ad22d346012f0402699428b5baac17c994e2b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bcf524a3-cb3c-4227-9d2a-95d14172163b", "node_type": "1", "metadata": {}, "hash": "df859f33fa74e634a86b1ece2870ad26c1b26d233b0a8648b354e6e3f9232944", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Just use the standard\n>\noperator in bash or\nfile.write()\nfunction in Python to save the model output.\nTruss CLI\nPython script\nCopy\nAsk AI\ntruss\npredict\n-d\n'\"Model input!\"'\n>\noutput.json\nOutput for some models, like image and audio generation models, may need to be decoded before you save it. See our\nimage generation example\nfor how to parse base64 output.\nWas this page helpful?\nYes\nNo\nPrevious\nIntegrations\nIntegrate your models with tools like LangChain, LiteLLM, and more.\nNext\nOn this page\nFiles as input\nExample: Send a file with JSON-serializable content\nExample: Send a file with non-serializable content\nExample: Send a URL to a public file\nFiles as output\nExample: Save model output to local file\nAssistant\nResponses are generated using AI and may contain mistakes.\n\n\nContent from https://docs.baseten.co/inference/streaming:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nDeveloping a Chain\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nExporting metrics\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nInference\nStreaming\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nAny model could be packaged with support for streaming output, but it only makes sense to do so for models where:\nGenerating a complete output takes a relatively long time.\nThe first tokens of output are useful without the context of the rest of the output.\nReducing the time to first token improves the user experience.\nLLMs in chat applications are the perfect use case for streaming model output.\n\u200b\nExample: Streaming with Mistral\nMistral 7B Instruct\nfrom Baseten\u2019s model library is a recent LLM with streaming support. Invocation should be the same for any other model library LLM as well as any Truss that follows the same standard.\nDeploy Mistral 7B Instruct\nor a similar LLM to run the following examples.\n\u200b\nTruss CLI\nThe Truss CLI has built-in support for streaming model output.\nCopy\nAsk AI\ntruss\npredict\n-d\n'{\"prompt\": \"What is the Mistral wind?\", \"stream\": true}'\n\u200b\nAPI endpoint\nWhen using a streaming endpoint with cURL, use the\n--no-buffer\nflag to stream output as it is received.\nAs with all cURL invocations, you\u2019ll need a model ID and API key.\nCopy\nAsk AI\ncurl\n-X\nPOST\nhttps://app.baseten.co/models/MODEL_ID/predict\n\\\n-H\n'Authorization: Api-Key YOUR_API_KEY'\n\\\n-d\n'{\"prompt\": \"What is the Mistral wind?\", \"stream\": true}'\n\\\n--no-buffer\n\u200b\nPython application\nLet\u2019s take things a step further and look at how to integrate streaming output with a Python application.\nCopy\nAsk AI\nimport\nrequests\nimport\njson\nimport\nos\n# Model ID for production deployment\nmodel_id\n=\n\"\"\n# Read secrets from environment variables\nbaseten_api_key\n=\nos.environ[\n\"BASETEN_API_KEY\"\n]\n# Open session to enable streaming\ns\n=\nrequests.Session()\nwith\ns.post(\n# Endpoint for production deployment, see API reference for more\nf\n\"https://model-\n{\nmodel_id\n}\n.api.baseten.co/production/predict\"\n,\nheaders\n=\n{\n\"Authorization\"\n:\nf\n\"Api-Key\n{\nbaseten_api_key\n}\n\"\n},\n# Include \"stream\": True in the data dict so the model knows to stream\ndata\n=\njson.dumps({\n\"prompt\"\n:\n\"What even is AGI?\"\n,\n\"stream\"\n:\nTrue\n,\n\"max_new_tokens\"\n:\n4096\n}),\n# Include stream=True as an argument so the requests libray knows to stream\nstream\n=\nTrue\n,\n)\nas\nresp:\n# Print the generated tokens as they get streamed\nfor\ncontent\nin\nresp.iter_content():\nprint\n(content.decode(\n\"utf-8\"\n),\nend\n=\n\"\"\n,\nflush\n=\nTrue\n)\nWas this page helpful?\nYes\nNo\nPrevious\nAsync inference\nRun asynchronous inference on deployed models\nNext\nOn this page\nExample: Streaming with Mistral\nTruss CLI\nAPI endpoint\nPython application\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 410115, "end_char_idx": 414221, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "bcf524a3-cb3c-4227-9d2a-95d14172163b": {"__data__": {"id_": "bcf524a3-cb3c-4227-9d2a-95d14172163b", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "372c2e82-3fff-426d-8220-64edfbd37215", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "fc56d6cdf2579069d2f753d845d46d6870b7b5554a25bee49d3b9edcdbda5d32", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "276f9601-25aa-4c60-a1e2-b5424d8c3169", "node_type": "1", "metadata": {}, "hash": "edcda541f4381e5793a0af842a0c54e7e490a320ba797c498006c11711ca26bd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Content from https://docs.baseten.co/inference/structured-output:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nDeveloping a Chain\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nStructured output (JSON mode)\nFunction calling (tool use)\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nExporting metrics\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nStructured LLM output\nStructured output (JSON mode)\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nStructured outputs requires an LLM deployed using the\nTensorRT-LLM Engine Builder\n.\nIf you want to try this structured output example code for yourself, deploy\nthis implementation of Llama 3.1 8B\n.\nTo generate structured outputs:\nDefine an object schema with\nPydantic\n.\nPass the schema to the LLM with the\nresponse_format\nargument.\nReceive output that is guaranteed to match the provided schema, including types and validations like\nmax_length\n.\nUsing structured output, you should observe approximately equivalent tokens per second output speed to an ordinary call to the model after an initial delay for schema processing. If you\u2019re interested in the mechanisms behind structured output, check out this\nengineering deep dive on our blog\n.\n\u200b\nSchema generation with Pydantic\nPydantic\nis an industry standard Python library for data validation. With Pydantic, we\u2019ll build precise schemas for LLM output to match.\nFor example, here\u2019s a schema for a basic\nPerson\nobject.\nCopy\nAsk AI\nfrom\npydantic\nimport\nBaseModel, Field\nfrom\ntyping\nimport\nOptional\nfrom\ndatetime\nimport\ndate\nclass\nPerson\n(\nBaseModel\n):\nfirst_name:\nstr\n=\nField(\n...\n,\ndescription\n=\n\"The person's first name\"\n,\nmax_length\n=\n50\n)\nlast_name:\nstr\n=\nField(\n...\n,\ndescription\n=\n\"The person's last name\"\n,\nmax_length\n=\n50\n)\nage:\nint\n=\nField(\n...\n,\ndescription\n=\n\"The person's age, must be a non-negative integer\"\n)\nemail:\nstr\n=\nField(\n...\n,\ndescription\n=\n\"The person's email address\"\n)\nStructured output supports multiple data types, required and optional fields, and additional validations like\nmax_length\n.\n\u200b\nAdd response format to LLM call\nThe first time that you pass a given schema for the model, it can take a\nminute for the schema to be processed and cached. Subsequent calls with the\nsame schema will run at normal speeds.\nOnce your object is defined, you can add it as a parameter to your LLM call with the\nresponse_format\nfield:\nCopy\nAsk AI\nimport\njson\nimport\nrequests\npayload\n=\n{\n\"messages\"\n: [\n{\n\"role\"\n:\n\"system\"\n,\n\"content\"\n:\n\"You are a helpful assistant\"\n},\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"Make up a new person!\"\n},\n],\n\"max_tokens\"\n:\n512\n,\n\"response_format\"\n: {\n# Add this parameter to use structured outputs\n\"type\"\n:\n\"json_schema\"\n,\n\"json_schema\"\n: {\n\"schema\"\n: Person.model_json_schema()},\n},\n}\nMODEL_ID\n=\n\"\"\nBASETEN_API_KEY\n=\n\"\"\nresp\n=\nrequests.post(\nf\n\"https://model-\n{\nMODEL_ID\n}\n.api.baseten.co/production/predict\"\n,\nheaders\n=\n{\n\"Authorization\"\n:\nf\n\"Api-Key\n{\nBASETEN_API_KEY\n}\n\"\n},\njson\n=\npayload,\n)\njson.loads(resp.text)\nThe response may have an end of sequence token, which will need to be removed before the JSON can be parsed.\n\u200b\nParsing LLM output\nFrom the LLM, we expect output in the following format:\nCopy\nAsk AI\n{\n\"first_name\"\n:\n\"Astrid\"\n,\n\"last_name\"\n:\n\"Nyxoria\"\n,\n\"age\"\n:\n28\n,\n\"email\"\n:\n\"astrid.nyxoria@starlightmail.com\"\n}\nThis example output is valid, which can be double-checked with:\nCopy\nAsk AI\nPerson.parse_raw(resp.text)\nWas this page helpful?", "mimetype": "text/plain", "start_char_idx": 414224, "end_char_idx": 418065, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "276f9601-25aa-4c60-a1e2-b5424d8c3169": {"__data__": {"id_": "276f9601-25aa-4c60-a1e2-b5424d8c3169", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bcf524a3-cb3c-4227-9d2a-95d14172163b", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "18567c09b471464127821fd44e41681a031b1d0db42d94ad82355ce092fc32c1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "985e16fa-a202-41f6-9339-6dae0eee21c3", "node_type": "1", "metadata": {}, "hash": "e16b05c76f1aba10357dfbd3d7531563a81483e5bf18f5efe42d031080207512", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u200b\nParsing LLM output\nFrom the LLM, we expect output in the following format:\nCopy\nAsk AI\n{\n\"first_name\"\n:\n\"Astrid\"\n,\n\"last_name\"\n:\n\"Nyxoria\"\n,\n\"age\"\n:\n28\n,\n\"email\"\n:\n\"astrid.nyxoria@starlightmail.com\"\n}\nThis example output is valid, which can be double-checked with:\nCopy\nAsk AI\nPerson.parse_raw(resp.text)\nWas this page helpful?\nYes\nNo\nPrevious\nFunction calling (tool use)\nUse an LLM to select amongst provided tools\nNext\nOn this page\nSchema generation with Pydantic\nAdd response format to LLM call\nParsing LLM output\nAssistant\nResponses are generated using AI and may contain mistakes.\n\n\nContent from https://docs.baseten.co/observability/access:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nDeveloping a Chain\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nSecure model inference\nWorkspace access control\nBest practices for API keys\nBest practices for secrets\nExporting metrics\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nSecurity\nWorkspace access control\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nPermission\nAdmin\nCreator\nManage members\n\u2705\n\u274c\nManage billing\n\u2705\n\u274c\nDeploy models\n\u2705\n\u2705\nCall models\n\u2705\n\u2705\nNote:\nStartup plan workspaces are limited to five users.\nContact us\nif you need to increase this limit.\nWas this page helpful?\nYes\nNo\nPrevious\nBest practices for API keys\nSecurely access your Baseten models\nNext\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 417736, "end_char_idx": 419625, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "985e16fa-a202-41f6-9339-6dae0eee21c3": {"__data__": {"id_": "985e16fa-a202-41f6-9339-6dae0eee21c3", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "276f9601-25aa-4c60-a1e2-b5424d8c3169", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ea4b45b86ad505a1958ba53626d82911be5cc657e68cfffb559c02c9ce25bc8c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "41986ff7-696a-401f-a4f4-0c5b368e0f47", "node_type": "1", "metadata": {}, "hash": "f9af426447466a27737eca0d640c22139300983c522fb65b21d13a2426d748cf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Content from https://docs.baseten.co/observability/api-keys:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nDeveloping a Chain\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nSecure model inference\nWorkspace access control\nBest practices for API keys\nBest practices for secrets\nExporting metrics\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nSecurity\nBest practices for API keys\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nAPI keys enable secure access to Baseten models for:\nModel deployment\nvia Truss CLI\nInference API calls\n(\ntruss predict\n,\n/wake\nrequests)\nModel management\nvia the\nmanagement API\nMetrics export\nvia the\n/metrics\nendpoint\nYou can create and revoke API keys from your\nBaseten account\n.\n\u200b\nAPI key scope: Personal vs Workspace\nThere are two types of API keys on Baseten:\nPersonal API Keys:\nTied to a user account.\nInherit full workspace permissions.\nActions are linked to the specific user.\nWorkspace API Keys:\nShared across a workspace.\nCan have full access or be restricted to specific models.\nUse personal keys for testing and workspace keys for automation and production.\n\u200b\nUsing API keys with Truss\nAdd your API key to\n~/.trussrc\nfor authentication:\n~/.trussrc\nCopy\nAsk AI\n[baseten]\nremote_provider\n=\nbaseten\napi_key\n=\nabcdefgh.1234567890ABCDEFGHIJKL1234567890\nremote_url\n=\nhttps://app.baseten.co\nIf rotating keys, update the file with the new key.\n\u200b\nUsing API keys with endpoints\nInclude the API key in request headers:\nCopy\nAsk AI\ncurl\n-X\nPOST\nhttps://app.baseten.co/models/MODEL_ID/predict\n\\\n-H\n'Authorization: Api-Key abcdefgh.1234567890ABCDEFGHIJKL1234567890'\n\\\n-d\n'MODEL_INPUT'\nOr in Python:\nCopy\nAsk AI\nheaders\n=\n{\n\"Authorization\"\n:\n\"Api-Key abcdefgh.1234567890ABCDEFGHIJKL1234567890\"\n}\n\u200b\nTips for managing API keys\nBest practices for API key use apply to your Baseten API keys:\nAlways store API keys securely.\nNever commit API keys to your codebase.\nNever share or leak API keys in notebooks or screenshots.\nName your API keys to keep them organized.\nThe\nAPI key list on your Baseten account\nshows when each key was first created and last used. Rotate API keys regularly and remove any unused API keys to reduce the risk of accidental leaks.\nWas this page helpful?\nYes\nNo\nPrevious\nBest practices for secrets\nSecurely store and access passwords, tokens, keys, and more\nNext\nOn this page\nAPI key scope: Personal vs Workspace\nUsing API keys with Truss\nUsing API keys with endpoints\nTips for managing API keys\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 419628, "end_char_idx": 422636, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "41986ff7-696a-401f-a4f4-0c5b368e0f47": {"__data__": {"id_": "41986ff7-696a-401f-a4f4-0c5b368e0f47", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "985e16fa-a202-41f6-9339-6dae0eee21c3", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "787c6401608aca623989964aac057e2170f138e46106d1fdaf3fd9ab0ee4b0d6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "afc25fb3-ad93-4495-9177-b8f8f1e1a1d1", "node_type": "1", "metadata": {}, "hash": "9b2115aef16ea1922cd4c5e97c2c8bb99370b8849cee916706fa71f23d82d8ed", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Content from https://docs.baseten.co/observability/export-metrics/datadog:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nDeveloping a Chain\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nExporting metrics\nOverview\nExport to Prometheus\nExport to Datadog\nExport to Grafana Cloud\nExport to New Relic\nMetrics support matrix\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nExporting metrics\nExport to Datadog\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nExporting metrics is in beta mode.\nThe Baseten metrics endpoint can be integrated with\nOpenTelemetry Collector\nby configuring a Prometheus receiver that scrapes the endpoint. This allows Baseten metrics to be pushed to a variety of popular exporters\u2014see the\nOpenTelemetry registry\nfor a full list.\nUsing OpenTelemetry Collector to push to Datadog\nconfig.yaml\nCopy\nAsk AI\nreceivers\n:\n# Configure a Prometheus receiver to scrape the Baseten metrics endpoint.\nprometheus\n:\nconfig\n:\nscrape_configs\n:\n-\njob_name\n:\n'baseten'\nscrape_interval\n:\n60s\nmetrics_path\n:\n'/metrics'\nscheme\n:\nhttps\nauthorization\n:\ntype\n:\n\"Api-Key\"\ncredentials\n:\n\"{BASETEN_API_KEY}\"\nstatic_configs\n:\n-\ntargets\n: [\n'app.baseten.co'\n]\nprocessors\n:\nbatch\n:\nexporters\n:\n# Configure a Datadog exporter.\ndatadog\n:\napi\n:\nkey\n:\n\"{DATADOG_API_KEY}\"\nservice\n:\npipelines\n:\nmetrics\n:\nreceivers\n: [\nprometheus\n]\nprocessors\n: [\nbatch\n]\nexporters\n: [\ndatadog\n]\nWas this page helpful?\nYes\nNo\nPrevious\nExport to Grafana Cloud\nExport metrics from Baseten to Grafana Cloud\nNext\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 422639, "end_char_idx": 424684, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "afc25fb3-ad93-4495-9177-b8f8f1e1a1d1": {"__data__": {"id_": "afc25fb3-ad93-4495-9177-b8f8f1e1a1d1", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "41986ff7-696a-401f-a4f4-0c5b368e0f47", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "611ad938bac191d3c34bba7d406e17ce56158e09035770cb02f8ac2366c05ff9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f1bdc863-4a6f-4729-81e8-fb7d7904ce82", "node_type": "1", "metadata": {}, "hash": "1a4ab505e3f979b24179e43a868bfc0c5e2294bcf5e634f2996ce7f92e172f22", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Content from https://docs.baseten.co/observability/export-metrics/grafana:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nDeveloping a Chain\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nExporting metrics\nOverview\nExport to Prometheus\nExport to Datadog\nExport to Grafana Cloud\nExport to New Relic\nMetrics support matrix\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nExporting metrics\nExport to Grafana Cloud\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nThe Baseten + Grafana Cloud integration enables you to get real-time inference metrics within your existing Grafana setup.\n\u200b\nVideo tutorial\nSee below for step-by-step details from the video.\n\u200b\nSet up the integration\nFor a visual guide, please follow along with the video above.\nOpen your Grafana Cloud account:\nNavigate to \u201cHome > Connections > Add new connection\u201d.\nIn the search bar, type\nMetrics Endpoint\nand select it.\nGive your scrape job a name like\nbaseten_metrics_scrape\n.\nSet the scrape job URL to\nhttps://app.baseten.co/metrics\n.\nLeave the scrape interval set to\nEvery minute\n.\nSelect\nBearer\nfor authentication credentials.\nIn your Baseten account, generate a metrics-only workspace API key.\nIn Grafana, enter the Bearer Token as\nApi-Key abcd.1234567890\nwhere the latter value is replaced by your API key.\nUse the \u201cTest Connection\u201d button to ensure everything is entered correctly.\nClick \u201cSave Scrape Job.\u201d\nClick \u201cInstall.\u201d\nIn your integrations list, select your new export and go through the \u201cEnable\u201d flow shown on video.\nNow, you can navigate to your Dashboards tab, where you will see your data! Please note that it can take a couple of minutes for data to arrive and only new data will be scraped, not historical metrics.\n\u200b\nBuild a Grafana dashboard\nImporting the data is a great first step, but you\u2019ll need a dashboard to properly visualize the incoming information.\nWe\u2019ve prepared a basic dashboard to get you started, which you can import by:\nDownloading\nbaseten_grafana_dashboard.json\nfrom\nthis GitHub Gist\n.\nSelecting \u201cNew > Import\u201d from the dropdown in the top-right corner of the Dashboard page.\nDropping in the provided JSON file.\nFor visual reference in navigating the dashboard, please see the video above.\nWas this page helpful?\nYes\nNo\nPrevious\nExport to New Relic\nExport metrics from Baseten to New Relic\nNext\nOn this page\nVideo tutorial\nSet up the integration\nBuild a Grafana dashboard\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 424687, "end_char_idx": 427625, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f1bdc863-4a6f-4729-81e8-fb7d7904ce82": {"__data__": {"id_": "f1bdc863-4a6f-4729-81e8-fb7d7904ce82", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "afc25fb3-ad93-4495-9177-b8f8f1e1a1d1", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "8a9756f76682d6af044340320f36efe2aa116c6674587518c105d986777d6943", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1f029326-89fa-4c2e-a4fe-b3c4a7534b52", "node_type": "1", "metadata": {}, "hash": "f93e744b14ac541d6a5d5f5b508802cfe02a86810693037cf56dd8830fd8f934", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Content from https://docs.baseten.co/observability/export-metrics/new-relic:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nDeveloping a Chain\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nExporting metrics\nOverview\nExport to Prometheus\nExport to Datadog\nExport to Grafana Cloud\nExport to New Relic\nMetrics support matrix\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nExporting metrics\nExport to New Relic\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nExporting metrics is in beta mode.\nExport Baseten metrics to New Relic by integrating with\nOpenTelemetry Collector\n. This involves configuring a Prometheus receiver that scrapes Baseten\u2019s metrics endpoint and configuring a New Relic exporter to send the metrics to your observability backend.\nUsing OpenTelemetry Collector to push to New Relic\nconfig.yaml\nCopy\nAsk AI\nreceivers\n:\n# Configure a Prometheus receiver to scrape the Baseten metrics endpoint.\nprometheus\n:\nconfig\n:\nscrape_configs\n:\n-\njob_name\n:\n'baseten'\nscrape_interval\n:\n60s\nmetrics_path\n:\n'/metrics'\nscheme\n:\nhttps\nauthorization\n:\ntype\n:\n\"Api-Key\"\ncredentials\n:\n\"{BASETEN_API_KEY}\"\nstatic_configs\n:\n-\ntargets\n: [\n'app.baseten.co'\n]\nprocessors\n:\nbatch\n:\nexporters\n:\n# Configure a New Relic exporter. Visit New Relic documentation to get your regional otlp endpoint.\notlphttp/newrelic\n:\nendpoint\n:\nhttps://otlp.nr-data.net\nheaders\n:\napi-key\n:\n\"{NEW_RELIC_KEY}\"\nservice\n:\npipelines\n:\nmetrics\n:\nreceivers\n: [\nprometheus\n]\nprocessors\n: [\nbatch\n]\nexporters\n: [\notlphttp/newrelic\n]\nWas this page helpful?\nYes\nNo\nPrevious\nMetrics support matrix\nWhich metrics can be exported\nNext\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 427628, "end_char_idx": 429788, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1f029326-89fa-4c2e-a4fe-b3c4a7534b52": {"__data__": {"id_": "1f029326-89fa-4c2e-a4fe-b3c4a7534b52", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f1bdc863-4a6f-4729-81e8-fb7d7904ce82", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "1dfbea91fd39ea476c2581ca9199a511a3e9340c217d832d3aa59a9964545bf0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4e53197e-0fa3-469f-9361-8e4cc4a35111", "node_type": "1", "metadata": {}, "hash": "8ad1dc770491befbc2ef50b404b406a7823adefabd3ab4c170603078826e8491", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Content from https://docs.baseten.co/observability/export-metrics/overview:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nDeveloping a Chain\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nExporting metrics\nOverview\nExport to Prometheus\nExport to Datadog\nExport to Grafana Cloud\nExport to New Relic\nMetrics support matrix\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nExporting metrics\nOverview\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nBaseten provides a metrics endpoint in Prometheus format, allowing integration with observability tools like Prometheus, OpenTelemetry Collector, Datadog Agent, and Vector.\n\u200b\nSetting Up Metrics Scraping\n1\nScrape endpoint: https://app.baseten.co/metrics\n2\nAuthentication\nUse the Authorization header with a\nBaseten API key\n:\nCopy\nAsk AI\n{\n\"Authorization\"\n:\n\"Api-Key YOUR_API_KEY\"\n}\n3\nScrape interval\nRecommended 1-minute interval (metrics update every 30 seconds).\n\u200b\nSupported Integrations\nBaseten metrics can be collected via\nOpenTelemetry Collector\nand exported to:\nPrometheus\nDatadog\nGrafana\nNew Relic\nFor available metrics, see the\nsupported metrics reference\n.\n\u200b\nRate Limits\n6 requests per minute per organization\nExceeding this limit results in\nHTTP 429 (Too Many Requests)\nresponses.\nTo stay within limits, use a\n1-minute scrape interval\n.\nWas this page helpful?\nYes\nNo\nPrevious\nExport to Prometheus\nExport metrics from Baseten to Prometheus\nNext\nOn this page\nSetting Up Metrics Scraping\nSupported Integrations\nRate Limits\nAssistant\nResponses are generated using AI and may contain mistakes.\n\n\nContent from https://docs.baseten.co/observability/export-metrics/prometheus:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nDeveloping a Chain\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nExporting metrics\nOverview\nExport to Prometheus\nExport to Datadog\nExport to Grafana Cloud\nExport to New Relic\nMetrics support matrix\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nExporting metrics\nExport to Prometheus\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nExporting metrics is in beta mode.\nTo integrate with Prometheus, specify the Baseten metrics endpoint in a scrape config. For example:\nprometheus.yml\nCopy\nAsk AI\nglobal\n:\nscrape_interval\n:\n60s\nscrape_configs\n:\n-\njob_name\n:\n'baseten'\nmetrics_path\n:\n'/metrics'\nauthorization\n:\ntype\n:\n\"Api-Key\"\ncredentials\n:\n\"{BASETEN_API_KEY}\"\nstatic_configs\n:\n-\ntargets\n: [\n'app.baseten.co'\n]\nscheme\n:\nhttps\nSee the Prometheus docs for more details on\ngetting started\nand\nconfiguration options\n.\nWas this page helpful?\nYes\nNo\nPrevious\nExport to Datadog\nExport metrics from Baseten to Datadog\nNext\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 429791, "end_char_idx": 433441, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4e53197e-0fa3-469f-9361-8e4cc4a35111": {"__data__": {"id_": "4e53197e-0fa3-469f-9361-8e4cc4a35111", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1f029326-89fa-4c2e-a4fe-b3c4a7534b52", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "c24a7522e45ecc5f0fed003843160ae846569e4ec8377e0da32ecca52414fa81", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "52bfe569-5298-4422-85d9-b7787bc6a0f3", "node_type": "1", "metadata": {}, "hash": "f5ad26eb1951099c02054bf0bdd5d530ff90e453c15baed3deca7a26e82fa80b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Content from https://docs.baseten.co/observability/export-metrics/supported-metrics:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nDeveloping a Chain\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nExporting metrics\nOverview\nExport to Prometheus\nExport to Datadog\nExport to Grafana Cloud\nExport to New Relic\nMetrics support matrix\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nExporting metrics\nMetrics support matrix\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nExporting metrics is in beta mode.\n\u200b\nbaseten_inference_requests_total\nCumulative number of requests to the model.\nType:\ncounter\nLabels:\n\u200b\nmodel_id\nlabel\nrequired\nThe ID of the model.\n\u200b\nmodel_name\nlabel\nrequired\nThe name of the model.\n\u200b\ndeployment_id\nlabel\nrequired\nThe ID of the deployment.\n\u200b\nstatus_code\nlabel\nrequired\nThe status code of the response.\n\u200b\nis_async\nlabel\nrequired\nWhether the request was an\nasync inference request\n.\n\u200b\nenvironment\nlabel\nThe environment that the deployment corresponds to. Empty if the deployment is not associated with an environment.\n\u200b\nrollout_phase\nlabel\nThe phase of the deployment in the\npromote to production process\n. Empty if the deployment is not associated with an environment.\nPossible values:\n\"promoting\"\n\"stable\"\n\u200b\nbaseten_end_to_end_response_time_seconds\nEnd-to-end response time in seconds.\nType:\nhistogram\nLabels:\n\u200b\nmodel_id\nlabel\nrequired\nThe ID of the model.\n\u200b\nmodel_name\nlabel\nrequired\nThe name of the model.\n\u200b\ndeployment_id\nlabel\nrequired\nThe ID of the deployment.\n\u200b\nstatus_code\nlabel\nrequired\nThe status code of the response.\n\u200b\nis_async\nlabel\nrequired\nWhether the request was an\nasync inference request\n.\n\u200b\nenvironment\nlabel\nThe environment that the deployment corresponds to. Empty if the deployment is not associated with an environment.\n\u200b\nrollout_phase\nlabel\nThe phase of the deployment in the\npromote to production process\n. Empty if the deployment is not associated with an environment.\nPossible values:\n\"promoting\"\n\"stable\"\n\u200b\nbaseten_container_cpu_usage_seconds_total\nCumulative CPU time consumed by the container in core-seconds.\nType:\ncounter\nLabels:\n\u200b\nmodel_id\nlabel\nrequired\nThe ID of the model.\n\u200b\nmodel_name\nlabel\nrequired\nThe name of the model.\n\u200b\ndeployment_id\nlabel\nrequired\nThe ID of the deployment.\n\u200b\nreplica\nlabel\nrequired\nThe ID of the replica.\n\u200b\nenvironment\nlabel\nThe environment that the deployment corresponds to. Empty if the deployment is\nnot associated with an environment.\n\u200b\nrollout_phase\nlabel\nThe phase of the deployment in the\npromote to production process\n. Empty if the deployment is not associated with an environment.\nPossible values:\n\"promoting\"\n\"stable\"\n\u200b\nbaseten_replicas_active\nNumber of replicas ready to serve model requests.\nType:\ngauge\nLabels:\n\u200b\nmodel_id\nlabel\nrequired\nThe ID of the model.\n\u200b\nmodel_name\nlabel\nrequired\nThe name of the model.\n\u200b\ndeployment_id\nlabel\nrequired\nThe ID of the deployment.\n\u200b\nenvironment\nlabel\nThe environment that the deployment corresponds to. Empty if the deployment is\nnot associated with an environment.\n\u200b\nrollout_phase\nlabel\nThe phase of the deployment in the\npromote to production process\n. Empty if the deployment is not associated with an environment.\nPossible values:\n\"promoting\"\n\"stable\"\n\u200b\nbaseten_replicas_starting\nNumber of replicas starting up\u2014i.e. either waiting for resources to be available or loading the model.\nType:\ngauge\nLabels:\n\u200b\nmodel_id\nlabel\nrequired\nThe ID of the model.\n\u200b\nmodel_name\nlabel\nrequired\nThe name of the model.\n\u200b\ndeployment_id\nlabel\nrequired\nThe ID of the deployment.", "mimetype": "text/plain", "start_char_idx": 433444, "end_char_idx": 437431, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "52bfe569-5298-4422-85d9-b7787bc6a0f3": {"__data__": {"id_": "52bfe569-5298-4422-85d9-b7787bc6a0f3", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4e53197e-0fa3-469f-9361-8e4cc4a35111", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "513b15025a0e1bee15b39930a631fd61edb72ffd3fb08558c5984d971400adaf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b9081599-bf1d-4e52-b896-40c163aeb607", "node_type": "1", "metadata": {}, "hash": "ec48871c1f7bbe76c47eb0eb72ae336f130cf662dc3d1994e3468f50e5965580", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u200b\nmodel_name\nlabel\nrequired\nThe name of the model.\n\u200b\ndeployment_id\nlabel\nrequired\nThe ID of the deployment.\n\u200b\nenvironment\nlabel\nThe environment that the deployment corresponds to. Empty if the deployment is\nnot associated with an environment.\n\u200b\nrollout_phase\nlabel\nThe phase of the deployment in the\npromote to production process\n. Empty if the deployment is not associated with an environment.\nPossible values:\n\"promoting\"\n\"stable\"\n\u200b\nbaseten_replicas_starting\nNumber of replicas starting up\u2014i.e. either waiting for resources to be available or loading the model.\nType:\ngauge\nLabels:\n\u200b\nmodel_id\nlabel\nrequired\nThe ID of the model.\n\u200b\nmodel_name\nlabel\nrequired\nThe name of the model.\n\u200b\ndeployment_id\nlabel\nrequired\nThe ID of the deployment.\n\u200b\nenvironment\nlabel\nThe environment that the deployment corresponds to. Empty if the deployment is\nnot associated with an environment.\n\u200b\nrollout_phase\nlabel\nThe phase of the deployment in the\npromote to production process\n. Empty if the deployment is not associated with an environment.\nPossible values:\n\"promoting\"\n\"stable\"\n\u200b\nbaseten_container_cpu_memory_working_set_bytes\nCumulative CPU time consumed by the container in seconds.\nType:\ngauge\nLabels:\n\u200b\nmodel_id\nlabel\nrequired\nThe ID of the model.\n\u200b\nmodel_name\nlabel\nrequired\nThe name of the model.\n\u200b\ndeployment_id\nlabel\nrequired\nThe ID of the deployment.\n\u200b\nreplica\nlabel\nrequired\nThe ID of the replica.\n\u200b\nenvironment\nlabel\nThe environment that the deployment corresponds to. Empty if the deployment is not associated with an environment.\n\u200b\nrollout_phase\nlabel\nThe phase of the deployment in the\npromote to production process\n. Empty if the deployment is not associated with an environment.\nPossible values:\n\"promoting\"\n\"stable\"\n\u200b\nbaseten_gpu_memory_used\nGPU memory used in MiB.\nType:\ngauge\nLabels:\n\u200b\nmodel_id\nlabel\nrequired\nThe ID of the model.\n\u200b\nmodel_name\nlabel\nrequired\nThe name of the model.\n\u200b\ndeployment_id\nlabel\nrequired\nThe ID of the deployment.\n\u200b\nreplica\nlabel\nrequired\nThe ID of the replica.\n\u200b\ngpu\nlabel\nrequired\nThe ID of the GPU.\n\u200b\nenvironment\nlabel\nThe environment that the deployment corresponds to. Empty if the deployment is not associated with an environment.\n\u200b\nrollout_phase\nlabel\nThe phase of the deployment in the\npromote to production process\n. Empty if the deployment is not associated with an environment.\nPossible values:\n\"promoting\"\n\"stable\"\n\u200b\nbaseten_gpu_utilization\nGPU utilization as a percentage (between 0 and 100).\nType:\ngauge\nLabels:\n\u200b\nmodel_id\nlabel\nrequired\nThe ID of the model.\n\u200b\nmodel_name\nlabel\nrequired\nThe name of the model.\n\u200b\ndeployment_id\nlabel\nrequired\nThe ID of the deployment.\n\u200b\nreplica\nlabel\nrequired\nThe ID of the replica.\n\u200b\ngpu\nlabel\nrequired\nThe ID of the GPU.\n\u200b\nenvironment\nlabel\nThe environment that the deployment corresponds to. Empty if the deployment is not associated with an environment.\n\u200b\nrollout_phase\nlabel\nThe phase of the deployment in the promote to production process. Empty if the deployment is not associated with an environment.\nPossible values:\n\"promoting\"\n\"stable\"\nWas this page helpful?\nYes\nNo\nPrevious\nTracing\nInvestigate the prediction flow in detail\nNext\nOn this page\nbaseten_inference_requests_total\nbaseten_end_to_end_response_time_seconds\nbaseten_container_cpu_usage_seconds_total\nbaseten_replicas_active\nbaseten_replicas_starting\nbaseten_container_cpu_memory_working_set_bytes\nbaseten_gpu_memory_used\nbaseten_gpu_utilization\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 436693, "end_char_idx": 440152, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b9081599-bf1d-4e52-b896-40c163aeb607": {"__data__": {"id_": "b9081599-bf1d-4e52-b896-40c163aeb607", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "52bfe569-5298-4422-85d9-b7787bc6a0f3", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "5c93197cd82e2335df9a9b6f488677e6c1e943ffe1b67d127dbfb00e4cce33b8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c7036f8a-86a7-4b13-ac9b-7fc1e6ea9856", "node_type": "1", "metadata": {}, "hash": "371e0a8a0781f2d2db3508d76fcf8d74341fd535d76d4f7cd55c8217f7d2d060", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u200b\ngpu\nlabel\nrequired\nThe ID of the GPU.\n\u200b\nenvironment\nlabel\nThe environment that the deployment corresponds to. Empty if the deployment is not associated with an environment.\n\u200b\nrollout_phase\nlabel\nThe phase of the deployment in the promote to production process. Empty if the deployment is not associated with an environment.\nPossible values:\n\"promoting\"\n\"stable\"\nWas this page helpful?\nYes\nNo\nPrevious\nTracing\nInvestigate the prediction flow in detail\nNext\nOn this page\nbaseten_inference_requests_total\nbaseten_end_to_end_response_time_seconds\nbaseten_container_cpu_usage_seconds_total\nbaseten_replicas_active\nbaseten_replicas_starting\nbaseten_container_cpu_memory_working_set_bytes\nbaseten_gpu_memory_used\nbaseten_gpu_utilization\nAssistant\nResponses are generated using AI and may contain mistakes.\n\n\nContent from https://docs.baseten.co/observability/health:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nDeveloping a Chain\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nExporting metrics\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nObservability\nStatus and health\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\n\u200b\nModel statuses\nHealthy states:\nActive\n: The deployment is active and available. It can be called with\ntruss predict\nor from its API endpoints.\nScaled to zero\n: The deployment is active but is not consuming resources. It will automatically start up when called, then scale back to zero after traffic ceases.\nStarting up\n: The deployment is starting up from a scaled to zero state after receiving a request.\nInactive\n: The deployment is unavailable and is not consuming resources. It may be manually reactivated.\nError states:\nUnhealthy\n: The deployment is active but is in an unhealthy state due to errors while running, such as an external service it relies on going down or a problem in your Truss that prevents it from responding to requests.\nBuild failed\n: The deployment is not active due to a Docker build failure.\nDeployment failed\n: The deployment is not active due to a model deployment failure.\n\u200b\nFixing unhealthy deployments\nIf you have an unhealthy or failed deployment, check the model logs to see if there\u2019s any indication of what the problem is. You can try deactivating and reactivating your deployment to see if the issue goes away. In the case of an external service outage, you may need to wait for the service to come back up before your deployment works again. For issues inside your Truss, you\u2019ll need to diagnose your code to see what is making it unresponsive.\nWas this page helpful?\nYes\nNo\nPrevious\nSecure model inference\nKeeping your models safe and private\nNext\nOn this page\nModel statuses\nFixing unhealthy deployments\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 439352, "end_char_idx": 442587, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c7036f8a-86a7-4b13-ac9b-7fc1e6ea9856": {"__data__": {"id_": "c7036f8a-86a7-4b13-ac9b-7fc1e6ea9856", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b9081599-bf1d-4e52-b896-40c163aeb607", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "8c1b04916cf7eb537b79729888b5dddc00215808612aae90f05f5a490ff98d2d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e3e11f3b-9f75-4a4a-a7f5-d39eee24b915", "node_type": "1", "metadata": {}, "hash": "309a37dfecbfdc946a2901049eb728db1c9000ba03c5c97bc79c7fecd9066bdc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Content from https://docs.baseten.co/observability/metrics:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nDeveloping a Chain\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nExporting metrics\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nObservability\nMetrics\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nThe Metrics tab in the model dashboard provides deployment-specific insights into model load and performance. Use the dropdowns to filter by environment or deployment and time range.\n\u200b\nInference volume\nTracks the request rate over time, segmented by HTTP status codes:\n2xx\n: \ud83d\udfe2 Successful requests\n4xx\n: \ud83d\udfe1 Client errors\n5xx\n: \ud83d\udd34 Server errors (includes model prediction exceptions)\n\u200b\nResponse time\nMeasured at different percentiles (p50, p90, p95, p99):\nEnd-to-end response time:\nIncludes cold starts, queuing, and inference (excludes client-side latency). Reflects real-world performance.\nInference time:\nCovers only model execution, including pre/post-processing. Useful for optimizing single-replica performance.\nTime to first byte:\nMeasures the time-to-first-byte time distribution, including any queueing and routing time. A proxy for TTFT.\n\u200b\nRequest and response size\nMeasured at different percentiles (p50, p90, p95, p99):\nRequest size:\nTracks the request size distribution. A proxy for input tokens.\nResponse size:\nTracks the response size distribution. A proxy for generated tokens.\n\u200b\nReplicas\nTracks the number of\nactive\nand\nstarting\nreplicas:\nStarting:\nWaiting for resources or loading the model.\nActive:\nReady to serve requests.\nFor development deployments, a replica is considered active while running the live reload server.\n\u200b\nCPU usage and memory\nDisplays resource utilization across replicas. Metrics are averaged and may not capture short spikes.\n\u200b\nConsiderations:\nHigh CPU/memory usage\n: May degrade performance\u2014consider upgrading to a larger instance type.\nLow CPU/memory usage\n: Possible overprovisioning\u2014switch to a smaller instance to reduce costs.\n\u200b\nGPU usage and memory\nShows GPU utilization across replicas.\nGPU usage\n: Percentage of time a kernel function occupies the GPU.\nGPU memory\n: Total memory used.\n\u200b\nConsiderations:\nHigh GPU load\n: Can slow inference\u2014check response time metrics.\nHigh memory usage\n: May cause out-of-memory failures.\nLow utilization\n: May indicate overprovisioning\u2014consider a smaller GPU.\n\u200b\nAsync Queue Metrics\nTime in Async Queue\n: Time spent in the async queue before execution (p50, p90, p95, p99).\nAsync Queue Size\n: Number of queued async requests.\n\u200b\nConsiderations:\nLarge queue size indicates requests are queued faster than they are processed.\nTo improve async throughput, increase the max replicas or adjust autoscaling concurrency.\nWas this page helpful?\nYes\nNo\nPrevious\nStatus and health\nEvery model deployment in your Baseten workspace has a status to represent its activity and health.\nNext\nOn this page\nInference volume\nResponse time\nRequest and response size\nReplicas\nCPU usage and memory\nConsiderations:\nGPU usage and memory\nConsiderations:\nAsync Queue Metrics\nConsiderations:\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 442590, "end_char_idx": 446207, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e3e11f3b-9f75-4a4a-a7f5-d39eee24b915": {"__data__": {"id_": "e3e11f3b-9f75-4a4a-a7f5-d39eee24b915", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c7036f8a-86a7-4b13-ac9b-7fc1e6ea9856", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "f588d8f886a5692715da0b6da98383d05ccee5ce5a2f0072b63fa03fed506081", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e3756bd1-1403-4a9f-9af8-7e467d081214", "node_type": "1", "metadata": {}, "hash": "e3aea58241837129b01f7ee7313dbd2c5d260c913345a6f72a3096758448cc12", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Content from https://docs.baseten.co/observability/secrets:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nDeveloping a Chain\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nSecure model inference\nWorkspace access control\nBest practices for API keys\nBest practices for secrets\nExporting metrics\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nSecurity\nBest practices for secrets\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nStore sensitive credentials like API keys and passwords using the\nsecrets dashboard\n.\nSecrets are stored as\nkey-value pairs\n(name \u2192 token).\nNaming rules\n: Non-alphanumeric characters are treated the same (e.g.,\nhf_access_token\nand\nhf-access-token\nare identical). Creating a new secret with a similar name overwrites the existing one.\nChanges to secrets\nimmediately\naffect all models using them.\nAny model deployed to a workspace will have access to any secrets specified on the workspace.\n\u200b\nUsing secrets in Truss\n1\nAdd the secret name to config.yaml, setting the value to null:\nconfig.yaml\nCopy\nAsk AI\n...\nsecrets\n:\nhf_access_token\n:\nnull\n...\nNever set the actual value of the secret in\nconfig.yaml\nor any other file that gets committed to your codebase.\n2\nAccess secrets in model.py:\nmodel/model.py\nCopy\nAsk AI\ndef\n__init__\n(\nself\n,\n**\nkwargs\n):\nself\n._secrets\n=\nkwargs[\n\"secrets\"\n]\n3\nUse secrets in load or predict:\nmodel/model.py\nCopy\nAsk AI\ndef\nload\n(\nself\n):\nself\n._model\n=\npipeline(\n\"fill-mask\"\n,\nmodel\n=\n\"baseten/docs-example-gated-model\"\n,\nuse_auth_token\n=\nself\n._secrets[\n\"hf_access_token\"\n]\n)\nWas this page helpful?\nYes\nNo\nPrevious\nOverview\nExport metrics from Baseten to your observability stack\nNext\nOn this page\nUsing secrets in Truss\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 446210, "end_char_idx": 448452, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e3756bd1-1403-4a9f-9af8-7e467d081214": {"__data__": {"id_": "e3756bd1-1403-4a9f-9af8-7e467d081214", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e3e11f3b-9f75-4a4a-a7f5-d39eee24b915", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "b110bf96f931fa762a4cfeba51af189fb1ce1213702a14721f5e17b87490e717", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "251fc671-9500-4c7a-831b-39dc46b32bd1", "node_type": "1", "metadata": {}, "hash": "4d1f5f9fb0df32492a4482623affb6df09e2ee47be47a753dbcb6f274abd5b80", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Content from https://docs.baseten.co/observability/security:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nDeveloping a Chain\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nSecure model inference\nWorkspace access control\nBest practices for API keys\nBest practices for secrets\nExporting metrics\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nSecurity\nSecure model inference\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nBaseten maintains\nSOC 2 Type II certification\nand\nHIPAA compliance\n, with robust security measures beyond compliance.\n\u200b\nData privacy\nBaseten does not store model inputs, outputs, or weights by default.\nModel inputs/outputs\n: Inputs for\nasync inference\nare temporarily stored until processed. Outputs are never stored.\nModel weights\n: Loaded dynamically from sources like Hugging Face, GCS, or S3, moving directly to GPU memory.\nUsers can enable caching via Truss. Cached weights can be permanently deleted on request.\nPostgres data tables\n: Existing users may store data in Baseten\u2019s hosted Postgres tables, which can be deleted anytime.\nBaseten\u2019s network accelerator optimizes model downloads.\nContact support\nto disable it.\n\u200b\nWorkload security\nInference workloads are isolated to protect users and Baseten\u2019s infrastructure.\nContainer security\n:\nNo GPUs are shared across users.\nSecurity tooling: Falco (Sysdig), Gatekeeper (Pod Security Policies).\nMinimal privileges for workloads and nodes to limit incident impact.\nNetwork security\n:\nEach customer has a dedicated Kubernetes namespace.\nIsolation enforced via\nCalico\n.\nNodes run in a private subnet with firewall protections.\nPentesting\n:\nExtended pentesting by\nRunSybil\n(ex-OpenAI and CrowdStrike experts).\nMalicious model deployments tested in a dedicated prod-like environment.\n\u200b\nSelf-hosted model inference\nBaseten offers single-tenant environments and self-hosted deployments. The cloud version is recommended for ease of setup, cost efficiency, and elastic GPU access.\nFor self-hosting,\ncontact support\n.\nWas this page helpful?\nYes\nNo\nPrevious\nWorkspace access control\nWorkspaces use role-based access control (RBAC) with two roles:\nNext\nOn this page\nData privacy\nWorkload security\nSelf-hosted model inference\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 448455, "end_char_idx": 451214, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "251fc671-9500-4c7a-831b-39dc46b32bd1": {"__data__": {"id_": "251fc671-9500-4c7a-831b-39dc46b32bd1", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e3756bd1-1403-4a9f-9af8-7e467d081214", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "2ce910fdf8cdaa3ba16b225619eb56f90155b46c8d4cc0c1c46488c1e20ade60", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d7a17ddc-128f-412e-bcc2-a16e0054c96f", "node_type": "1", "metadata": {}, "hash": "592f8ae6755e14e2bd4f2e229cbaadda4fdbfaf78a3dba0de98bb2eed5f2cb75", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Content from https://docs.baseten.co/observability/tracing:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nDeveloping a Chain\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nExporting metrics\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nObservability\nTracing\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nBaseten\u2019s Truss server includes built-in\nOpenTelemetry\n(OTEL) instrumentation, with support for custom tracing.\nTracing helps diagnose performance bottlenecks but introduces minor overhead, so it is\ndisabled by default\n.\n\u200b\nExporting builtin trace data to Honeycomb\nCreate a Honeycomb API\nkey and add it to\nBaseten secrets\n.\nUpdate\nconfig.yaml\nfor the target model:\nconfig.yaml\nCopy\nAsk AI\nenvironment_variables\n:\nHONEYCOMB_DATASET\n:\nyour_dataset_name\nruntime\n:\nenable_tracing_data\n:\ntrue\nsecrets\n:\nHONEYCOMB_API_KEY\n:\n'***'\nSend requests with tracing\nProvide traceparent headers for distributed tracing.\nIf omitted, Baseten generates random trace IDs.\n\u200b\nAdding custom OTEL instrumentation\nTo define custom spans and events, integrate OTEL directly:\nmodel.py\nCopy\nAsk AI\nimport\ntime\nfrom\ntyping\nimport\nAny, Generator\nimport\nopentelemetry.exporter.otlp.proto.http.trace_exporter\nas\noltp_exporter\nimport\nopentelemetry.sdk.resources\nas\nresources\nimport\nopentelemetry.sdk.trace\nas\nsdk_trace\nimport\nopentelemetry.sdk.trace.export\nas\ntrace_export\nfrom\nopentelemetry\nimport\ntrace\nfrom\nopentelemetry.sdk.resources\nimport\nResource\nfrom\nopentelemetry.sdk.trace\nimport\nTracerProvider\ntrace.set_tracer_provider(\nTracerProvider(\nresource\n=\nResource.create({resources.\nSERVICE_NAME\n:\n\"UserModel\"\n}))\n)\ntracer\n=\ntrace.get_tracer(\n__name__\n)\ntrace_provider\n=\ntrace.get_tracer_provider()\nclass\nModel\n:\ndef\n__init__\n(\nself\n,\n**\nkwargs\n) ->\nNone\n:\nhoneycomb_api_key\n=\nkwargs[\n\"secrets\"\n][\n\"HONEYCOMB_API_KEY\"\n]\nhoneycomb_exporter\n=\noltp_exporter.OTLPSpanExporter(\nendpoint\n=\n\"https://api.honeycomb.io/v1/traces\"\n,\nheaders\n=\n{\n\"x-honeycomb-team\"\n: honeycomb_api_key,\n\"x-honeycomb-dataset\"\n:\n\"marius_testing_user\"\n,\n},\n)\nhoneycomb_processor\n=\nsdk_trace.export.BatchSpanProcessor(honeycomb_exporter)\ntrace_provider.add_span_processor(honeycomb_processor)\n@tracer.start_as_current_span\n(\n\"load_model\"\n)\ndef\nload\n(\nself\n):\n...\ndef\npreprocess\n(\nself\n,\nmodel_input\n):\nwith\ntracer.start_as_current_span(\n\"preprocess\"\n):\n...\nreturn\nmodel_input\n@tracer.start_as_current_span\n(\n\"predict\"\n)\ndef\npredict\n(\nself\n,\nmodel_input\n: Any) -> Generator[\nstr\n,\nNone\n,\nNone\n]:\nwith\ntracer.start_as_current_span(\n\"start-predict\"\n)\nas\nspan:\ndef\ninner\n():\ntime.sleep(\n0.01\n)\nfor\ni\nin\nrange\n(\n5\n):\nspan.add_event(\n\"yield\"\n)\nyield\nstr\n(i)\nreturn\ninner()\nBaseten\u2019s built-in tracing\ndoes not interfere\nwith user-defined OTEL implementations.\nWas this page helpful?\nYes\nNo\nPrevious\nBilling and usage\nManage payments and track overall Baseten usage\nNext\nOn this page\nExporting builtin trace data to Honeycomb\nAdding custom OTEL instrumentation\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 451217, "end_char_idx": 454685, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d7a17ddc-128f-412e-bcc2-a16e0054c96f": {"__data__": {"id_": "d7a17ddc-128f-412e-bcc2-a16e0054c96f", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "251fc671-9500-4c7a-831b-39dc46b32bd1", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "04d7fd063b8f32c8aa86c918347d8d4f2418748de85ac29a48f4408ced59bf17", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "507ac74d-1d52-4b71-b156-c9ddd88a48e3", "node_type": "1", "metadata": {}, "hash": "9f703ae268ab8825371274fc6a773eeb74b58cc86c5b66726c93b1d5b28da4b8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Content from https://docs.baseten.co/observability/usage:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nDeveloping a Chain\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nExporting metrics\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nObservability\nBilling and usage\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nThe\nbilling and usage dashboard\nprovides a breakdown of model usage and costs, updated hourly. Usage is tracked per deployment, and any available credits are automatically applied to your bill.\n\u200b\nBilling\n\u200b\nCredits\nNew workspaces receive free credits for testing and deployment.\nIf credits run out and no payment method is set, models will be deactivated until a payment method is added.\n\u200b\nPayment method\nPayment details can be added or updated on the\nbilling page\n.\nPayment information is securely stored with our payment processor, not Baseten.\nOn the\nbilling page\n, you can set and update your payment method. Your payment information, including credit card numbers and bank information, is always stored securely with our payments processor and not by Baseten directly.\n\u200b\nInvoice history\nView past invoices and payments in the billing dashboard.\nFor questions,\ncontact support\n.\n\u200b\nUsage and billing FAQs\nFor full details, see our\npricing page\n, but here are answers to some common questions:\n\u200b\nHow exactly is usage calculated?\nUsage is billed per minute while a model is deploying, scaling, or serving requests.\nCosts are based on the\ninstance type\nused.\n\u200b\nHow often are payments due?\nInitially, charges occur when usage\nexceeds $50\nor at the\nend of the month\n, whichever comes first.\nAfter a history of successful payments, billing occurs\nmonthly\n.\n\u200b\nDo you offer volume discounts?\nVolume discounts are available on the\nPro plan\n.\nContact support\nfor details.\n\u200b\nDo you offer education and non-profit discounts?\nYes, discounts are available for educational and nonprofit ML projects.\nContact support\nto apply.\nWas this page helpful?\nYes\nNo\nPrevious\nDeployments\nTroubleshoot common problems during model deployment\nNext\nOn this page\nBilling\nCredits\nPayment method\nInvoice history\nUsage and billing FAQs\nHow exactly is usage calculated?\nHow often are payments due?\nDo you offer volume discounts?\nDo you offer education and non-profit discounts?\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 454688, "end_char_idx": 457518, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "507ac74d-1d52-4b71-b156-c9ddd88a48e3": {"__data__": {"id_": "507ac74d-1d52-4b71-b156-c9ddd88a48e3", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d7a17ddc-128f-412e-bcc2-a16e0054c96f", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "63911471a471fbce4dbc570919a11b78e7180737540e23ca190468f592ac8f39", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f4879e88-a1e1-461a-8bd8-eb7450a49da6", "node_type": "1", "metadata": {}, "hash": "76b4f9b928ae78e9d8d41094b73c42c1e5a841a1ce27f4e6e3e0608cf7effc55", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Content from https://docs.baseten.co/overview:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nDeveloping a Chain\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nExporting metrics\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nGet started\nDocumentation\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nBuild with Baseten\nBaseten is a platform for deploying and serving AI models performantly,\nscalably, and cost-efficiently.\nQuick start\nChoose from common AI/ML usecases and modalities to get started on Baseten quickly.\nHow Baseten works\nBaseten makes it easy to deploy, serve, and scale AI models so you can focus on building, not infrastructure.\nBaseten is an inference and training platform that lets you:\n\u200b\nDeploy dedicated models with full control\nPackage any model for production\n: Define dependencies, hardware, and custom code without needing to learn Docker. Build with your preferred frameworks (e.g.\nPyTorch\n,\ntransformers\n,\ndiffusers\n),\ninference engines\n(e.g.\nTensorRT-LLM\n,\nSGLang\n,\nvLLM\n), and serving tools (e.g.\nTriton\n) as well as\nany package\ninstallable via\npip\nor\napt\n.\nBuild complex AI systems\n: Orchestrate multi-step workflows with\nChains\n, combining models, business logic, and external APIs.\nDeploy with confidence\n:\nAutoscale\nmodels, manage\nenvironments\n, and roll out updates with zero-downtime deployments.\nRun high-performance inference\n: Serve\nsynchronous\n,\nasynchronous\n, and\nstreaming\npredictions with low-latency execution controls.\nMonitor and optimize in production\n: Monitor performance, debug issues, and\nexport metrics\nwith built-in observability tooling.\n\u200b\nStart fast with model APIs\nTry model APIs\n: Model APIs provide a fast path to production with reliable, high-performance inference. Use OpenAI-compatible endpoints to integrate models like Llama, DeepSeek, and Qwen, with built-in support for structured outputs and tool calling.\n\u200b\nPre-train and fine-tune models\nRun training jobs on scalable infrastructure\n: Launch containerized training jobs with configurable environments, compute (CPU/GPU), and resource scaling. Supports any training framework via a framework-agnostic API.\nManage artifacts and streamline workflows\n: Track experiments, organize training runs, and handle large artifacts like checkpoints and logs. Seamlessly transition from training to deployment within the Baseten ecosystem.\nResources\nExamples\nFrom deploying AI models to optimizing inference and scaling ML models.\nModel library\nPrebuilt, ready to deploy in one click models like DeepSeek, Llama, and\nQwen.\nExplore API reference\nAPI reference for calling deployed models, Chains or managing models and\nyour workspace.\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 457521, "end_char_idx": 460727, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f4879e88-a1e1-461a-8bd8-eb7450a49da6": {"__data__": {"id_": "f4879e88-a1e1-461a-8bd8-eb7450a49da6", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "507ac74d-1d52-4b71-b156-c9ddd88a48e3", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "c9d8ff26f9074d2a74c7369ff4c036bc7a4a8194a9e7afff4574099815e7dc6f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c63fddf7-e277-49e4-82ae-ce320904ac33", "node_type": "1", "metadata": {}, "hash": "ac42bd277446c276813475defa969690f8f657537d68c9bcd69e0f24585c06cc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Content from https://docs.baseten.co/quickstart:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nDeveloping a Chain\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nExporting metrics\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nGet started\nQuick start\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\n1\nWhat modality are you working with?\nChoose from common modalities like LLMs, transcription, and image generation to get started quickly.\nLLMs\nBuild and deploy large language models\nTranscription\nTranscribe audio and video\nImage generation\nRapidly generate images\nText to speech\nBuild humanlike experiences\nCompound AI\nBuild real-time AI-native applications\nEmbeddings\nProcess millions of data points\nCustom models\nDeploy any model\n2\nSelect a model or guide to get started...\nChoose a use case or modality above first\u2026\nWas this page helpful?\nYes\nNo\nPrevious\nWhy Baseten\nBaseten delivers fast, scalable AI/ML inference with enterprise-grade security and reliability\u2014whether in our cloud or yours.\nNext\nAssistant\nResponses are generated using AI and may contain mistakes.\n\n\nContent from https://docs.baseten.co/status/status:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nBaseten platform status\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nBaseten platform status\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nThis page automatically refreshes with real-time data from our status monitoring system.\nAll systems are operational.\nModel Inference\nNormal\nManagement API\nNormal\nWeb Application\nNormal\nLast updated: Loading\u2026\nWas this page helpful?\nYes\nNo\nAssistant\nResponses are generated using AI and may contain mistakes.\n\n\nContent from https://docs.baseten.co/training/concepts:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nDeveloping a Chain\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nExporting metrics\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nTraining\nConcepts\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nBaseten Training is designed to provide a structured yet flexible way to manage your machine learning training workflows. To use it effectively, it helps to understand the main ideas behind its components and how they fit together. This isn\u2019t an API reference, but rather a guide to thinking about how to organize and execute your training tasks.\n\u200b\nOrganizing Your Work with\nTrainingProject\ns\nA\nTrainingProject\nis a lightweight organization tool to help you group different\nTrainingJob\ns together.\nWhile there a few technical details to consider, your team can use\nTrainingProject\ns to facilitate collaboration and organization.\n\u200b\nRunning a\nTrainingJob\nOnce you have a\nTrainingProject\n, the actual work of training a model happens within a\nTrainingJob\n. Each\nTrainingJob\nrepresents a single, complete execution of your training script with a specific configuration.\nWhat it is:\nA\nTrainingJob\nis the fundamental unit of execution. It bundles together:\nYour training code.\nA base\nimage\n.\nThe\ncompute\nresources needed to run the job.\nThe\nruntime\nconfigurations like startup commands and environment variables.\nWhy use it:\nEach job is a self-contained, reproducible experiment.", "mimetype": "text/plain", "start_char_idx": 460730, "end_char_idx": 464884, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c63fddf7-e277-49e4-82ae-ce320904ac33": {"__data__": {"id_": "c63fddf7-e277-49e4-82ae-ce320904ac33", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f4879e88-a1e1-461a-8bd8-eb7450a49da6", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "25103997668ffea71c7cf9a874b38abb13fe0d05b289b99adce632e7ad2374f2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eb731752-5e68-463d-9e0f-a61b820a8cb6", "node_type": "1", "metadata": {}, "hash": "215c09ec10e3cd70a42a8e8113bc5134fc5b68b37c0024d8df6727b8d173a70a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u200b\nOrganizing Your Work with\nTrainingProject\ns\nA\nTrainingProject\nis a lightweight organization tool to help you group different\nTrainingJob\ns together.\nWhile there a few technical details to consider, your team can use\nTrainingProject\ns to facilitate collaboration and organization.\n\u200b\nRunning a\nTrainingJob\nOnce you have a\nTrainingProject\n, the actual work of training a model happens within a\nTrainingJob\n. Each\nTrainingJob\nrepresents a single, complete execution of your training script with a specific configuration.\nWhat it is:\nA\nTrainingJob\nis the fundamental unit of execution. It bundles together:\nYour training code.\nA base\nimage\n.\nThe\ncompute\nresources needed to run the job.\nThe\nruntime\nconfigurations like startup commands and environment variables.\nWhy use it:\nEach job is a self-contained, reproducible experiment. If you want to try training your model with a different learning rate, more GPUs, or a slightly modified script, you can create new\nTrainingJob\ns while knowing that previous ones have been persisted on Baseten.\nLifecycle:\nA job goes through various stages, from being created (\nTRAINING_JOB_CREATED\n), to resources being set up (\nTRAINING_JOB_DEPLOYING\n), to actively running your script (\nTRAINING_JOB_RUNNING\n), and finally to a terminal state like\nTRAINING_JOB_COMPLETED\n. More details on the job lifecycle can be found on the\nLifecycle\npage.\n\u200b\nIterate Faster with the Training Cache\nThe training cache enables you to persist data between training jobs. This can significantly improve iteration speed by skipping expensive downloads and data transformations.\nHow to use it:\nset\nenable_cache=True\nin your\nRuntime\n.\nCache Directories\n: The cache will be mounted at\n/root/.cache/huggingface\nand at\n$BT_RW_CACHE_DIR\n.\nSeeding Your Data\n: For multi-gpu training, you should ensure that your data is seeded before running multi-process training jobs. You can do this by separating your training script into training script and data loading script.\nSpeedup:\nFor a 400 GB HF Dataset, you can expect to save\nnearly an hour\nof compute time for each job - data download and preparation have been done already!\n\u200b\nTaking Advantage of Automated Checkpointing\nTraining machine learning models can be lengthy and resource-intensive. Baseten\u2019s automated\nCheckpointing\nprovides seemless storage for checkpoints and a jumping off point for inference and eval.\nWhat it is:\nAutomated Checkpointing provides a seamless way to save model checkpoints to cloud storage.\nWhy use it:\nFault Tolerance:\nResume from the last saved checkpoint if a job fails, saving time and compute.\nExperimentation:\nUse saved checkpoints as starting points for new training runs with different hyperparameters or for transfer learning.\nModel Evaluation:\nDeploy intermediate model versions to track progress.\nTo enable checkpointing, add a\nCheckpointingConfig\nto the\nRuntime\nand set\nenabled\nto\nTrue\n. Baseten will automatically export the\n$BT_CHECKPOINT_DIR\nenvironment variable in your job\u2019s environment. Ensure your code is writing checkpoints to the\n$BT_CHECKPOINT_DIR\n.\n\u200b\nMultinode Training\nBaseten Training supports multinode training via infiniband. To deploy a multinode training job:\nConfigure the\nCompute\nresource in your\nTrainingJob\nby setting the\nnode_count\nto the number of nodes you\u2019d like to use (e.g. 2).\nMake sure you\u2019ve properly integrated with the\nBaseten provided environment variables\n.\n\u200b\nSecurely Integrate with External Services with\nSecretReference\nSuccessfully training a model often requires many tools and services. Baseten provides\nSecretReference\nfor secure handling of secrets.\nHow to use it:\nStore your secret (e.g., an API key for Weights & Biases) in your Baseten workspace with a specific name. In your job\u2019s configuration (e.g., environment variables), you refer to this secret by its name using\nSecretReference\n. The actual secret value is never exposed in your code.\nHow it works:\nBaseten injects the secret value at runtime under the environment variable name that you specify.\n\u200b\nRunning Inference on Trained Models\nThe journey from training to a usable model in Baseten typically follows this path:\nA\nTrainingJob\nwith checkpointing enabled, produces one or more model artifacts.\nYou run\ntruss train deploy_checkpoint\nto deploy a model from your most recent training job. You can read more about this at\nDeploying Trained Models\n.\nOnce deployed, your model will be avialble for inference via API. See more at\nCalling Your Model\n.\nWas this page helpful?", "mimetype": "text/plain", "start_char_idx": 464058, "end_char_idx": 468530, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "eb731752-5e68-463d-9e0f-a61b820a8cb6": {"__data__": {"id_": "eb731752-5e68-463d-9e0f-a61b820a8cb6", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c63fddf7-e277-49e4-82ae-ce320904ac33", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "b92fe303d05350cf46b20656b2401e9278f84e50ef833be9d98bb0da2b66efdd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3467eb37-e970-47ec-be94-65b56d86a240", "node_type": "1", "metadata": {}, "hash": "07b1dbe0e66b982c3cbcdbbfdd3fbc10240c4c724bc22023fe7bc78029be4cdc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In your job\u2019s configuration (e.g., environment variables), you refer to this secret by its name using\nSecretReference\n. The actual secret value is never exposed in your code.\nHow it works:\nBaseten injects the secret value at runtime under the environment variable name that you specify.\n\u200b\nRunning Inference on Trained Models\nThe journey from training to a usable model in Baseten typically follows this path:\nA\nTrainingJob\nwith checkpointing enabled, produces one or more model artifacts.\nYou run\ntruss train deploy_checkpoint\nto deploy a model from your most recent training job. You can read more about this at\nDeploying Trained Models\n.\nOnce deployed, your model will be avialble for inference via API. See more at\nCalling Your Model\n.\nWas this page helpful?\nYes\nNo\nPrevious\nManagement\nHow to monitor, manage, and interact with your Baseten Training projects and jobs.\nNext\nOn this page\nOrganizing Your Work with TrainingProjects\nRunning a TrainingJob\nIterate Faster with the Training Cache\nTaking Advantage of Automated Checkpointing\nMultinode Training\nSecurely Integrate with External Services with SecretReference\nRunning Inference on Trained Models\nAssistant\nResponses are generated using AI and may contain mistakes.\n\n\nContent from https://docs.baseten.co/training/deployment:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nDeveloping a Chain\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nExporting metrics\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nTraining\nDeploying checkpoints\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nBaseten Training seamlessly integrates with Baseten\u2019s model deployment capabilities. Once your\nTrainingJob\nhas produced model checkpoints, you can deploy them as fully operational model endpoints.\nTo leverage deploying checkpoints, first ensure you have a\nTrainingJob\nthat\u2019s running with a\ncheckpointing_config\nenabled.\nCopy\nAsk AI\nruntime\n=\ndefinitions.Runtime(\nstart_commands\n=\n[\n\"/bin/sh -c './run.sh'\"\n,\n],\ncheckpointing_config\n=\ndefinitions.CheckpointingConfig(\nenabled\n=\nTrue\n,\n),\n)\nIn your training code or configuration, ensure that your checkpoints are being written to the checkpointing directory, which can be referenced in via\n$BT_CHECKPOINT_DIR\n.\nThe contents of this directory are uploaded to Baseten\u2019s storage and made immediately available for deployment.\n(You can optionally specify a\ncheckpoint_path\nin your\ncheckpointing_config\nif you prefer to write to a specific directory).\nTo deploy your checkpoint(s) as a\nDeployment\n, you can:\nrun\ntruss train deploy_checkpoints [--job-id <job_id>]\nand follow the setup wizard.\ndefine an instance of a\nDeployCheckpointsConfig\nclass (this is helpful for small changes that aren\u2019t provided by the wizard) and run\ntruss train deploy_checkpoints --config <path_to_config_file>\n.\nWhen\ndeploy_checkpoints\nis run,\ntruss\nwill construct a deployment\nconfig.yml\nand store it on disk in a temporary directory. If you\u2019d like to preserve or modify the resulting deployment config, you can copy paste it\ninto a permanent directory and customize it as needed.\nThis file defines the source of truth for the deployment and can be deployed independently via\ntruss push\n. See\ndeployments\nfor more details.\nAfter successful deployment, your model will be deployed on Baseten, where you can run inference requests and evaluate performance. See\nCalling Your Model\nfor more details.\nWas this page helpful?\nYes\nNo\nPrevious\nMetrics\nUnderstand the load and performance of your model\nNext\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 467769, "end_char_idx": 471799, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3467eb37-e970-47ec-be94-65b56d86a240": {"__data__": {"id_": "3467eb37-e970-47ec-be94-65b56d86a240", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eb731752-5e68-463d-9e0f-a61b820a8cb6", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "2b297ef91893ac3bfef3d43c6de9f93ea4bfe6314bc4494a08f6c32c4bb5b237", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a9447afe-1f53-44e6-9143-b96e57fcd4f5", "node_type": "1", "metadata": {}, "hash": "73d3c94faea93a7de1fc3bed55caede92be6bd8fcbc3cfd42bb7f6aa0aff635b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Content from https://docs.baseten.co/training/getting-started:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nDeveloping a Chain\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nExporting metrics\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nTraining\nGetting started\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nThis guide will walk you through the initial setup and the process of submitting\nyour first\nTrainingJob\nusing Baseten Training.\n\u200b\nPrerequisites\nBefore you begin, ensure you have the following:\nBaseten Account:\nYou\u2019ll need an active Baseten account. If you don\u2019t\nhave one, please sign up on the\nBaseten web app\n.\nAPI Key:\nObtain an API key for your\nBaseten account\n.\nThis key is\nrequired to authenticate with the Baseten API and SDK.\nTruss SDK and CLI:\nThe\ntruss\npackage provides a python-native way for defining and running your training jobs.\njobs. The CLI provides a convenient way to deploy and manage your training jobs. Install or update it:\nCopy\nAsk AI\npip\ninstall\n-U\ntruss\n\u200b\nStep 1: Define your training configuration\nThe primary way to define your training jobs is through a Python configuration\nfile, typically named\nconfig.py\n. This file uses the\ntruss\npackage to specify all\naspects of your\nTrainingProject\nand\nTrainingJob\n.\nA simple example of a\nconfig.py\nfile is shown below:\nconfig.py\nCopy\nAsk AI\n# Import necessary classes from the Baseten Training SDK\nfrom\ntruss_train\nimport\ndefinitions\nfrom\ntruss.base\nimport\ntruss_config\n# 1. Define a base image for your training job\nBASE_IMAGE\n=\n\"axolotlai/axolotl:main-20250324-py3.11-cu124-2.6.0\"\n# 2. Define the Runtime Environment for the Training Job\n# This includes start commands and environment variables.\n# Secrets from the baseten workspace like API keys are referenced using\n# `SecretReference`.\ntraining_runtime\n=\ndefinitions.Runtime(\nstart_commands\n=\n[\n# Example: list of commands to run your training script\n# \"pip install -r requirements.txt\", # pip install requirements on top of base image\n\"/bin/sh -c './run.sh'\"\n,\n],\nenvironment_variables\n=\n{\n# Secrets (ensure these are configured in your Baseten workspace)\n\"HF_TOKEN\"\n: definitions.SecretReference(\nname\n=\n\"hf_access_token\"\n),\n\"WANDB_API_KEY\"\n: definitions.SecretReference(\nname\n=\n\"wandb_api_key\"\n),\n\"HELLO\"\n:\n\"WORLD\"\n},\n)\n# 3. Define the Compute Resources for the Training Job\ntraining_compute\n=\ndefinitions.Compute(\naccelerator\n=\ntruss_config.AcceleratorSpec(\naccelerator\n=\ntruss_config.Accelerator.H100,\ncount\n=\n4\n,\n),\n)\n# 4. Define the Training Job\n# This brings together the image, compute, and runtime configurations.\nmy_training_job\n=\ndefinitions.TrainingJob(\nimage\n=\ndefinitions.Image(\nbase_image\n=\nBASE_IMAGE\n),\ncompute\n=\ntraining_compute,\nruntime\n=\ntraining_runtime\n)\n# This config will be pushed using the Truss CLI.\n# The association of the job to the project happens at the time of push.\nfirst_project_with_job\n=\ndefinitions.TrainingProject(\nname\n=\nproject_name,\njob\n=\nmy_training_job\n)\n\u200b\nKey considerations for your Baseten training configuration file\nLocal Artifacts:\nIf your training requires local scripts (like\na\ntrain.py\nor a\nrun.sh\n), helper files, or\nconfiguration files (e.g., accelerate config), place them in the same\ndirectory as your\nconfig.py\nor in subdirectories. When you push the training\njob,\ntruss\nwill package these artifacts and upload them. They will be copied\ninto the container at the root of the base image\u2019s working directory.", "mimetype": "text/plain", "start_char_idx": 471802, "end_char_idx": 475706, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a9447afe-1f53-44e6-9143-b96e57fcd4f5": {"__data__": {"id_": "a9447afe-1f53-44e6-9143-b96e57fcd4f5", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3467eb37-e970-47ec-be94-65b56d86a240", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "69e3c3e1763c24491eb14a3cb95ec794ea800b2001a5f1dbd881c4830e609318", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8cd6cfd4-2f6b-4285-8645-5b9b1de2d468", "node_type": "1", "metadata": {}, "hash": "5c8425666fa433034510b0f6ddd32b8379893fbe4b50c4dc553e27ce69303a15", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "my_training_job\n=\ndefinitions.TrainingJob(\nimage\n=\ndefinitions.Image(\nbase_image\n=\nBASE_IMAGE\n),\ncompute\n=\ntraining_compute,\nruntime\n=\ntraining_runtime\n)\n# This config will be pushed using the Truss CLI.\n# The association of the job to the project happens at the time of push.\nfirst_project_with_job\n=\ndefinitions.TrainingProject(\nname\n=\nproject_name,\njob\n=\nmy_training_job\n)\n\u200b\nKey considerations for your Baseten training configuration file\nLocal Artifacts:\nIf your training requires local scripts (like\na\ntrain.py\nor a\nrun.sh\n), helper files, or\nconfiguration files (e.g., accelerate config), place them in the same\ndirectory as your\nconfig.py\nor in subdirectories. When you push the training\njob,\ntruss\nwill package these artifacts and upload them. They will be copied\ninto the container at the root of the base image\u2019s working directory.\nIgnore Folders and Files\n: You can exclude specific files from being pushed by creating a\n.truss_ignore\nfile in root directory of your project.\nIn this file, you can add entries in a style similar to\n.gitignore\n. Refer to the\nCLI reference\nfor more details.\nSecrets:\nEnsure any secrets referenced via\nSecretReference\n(e.g.,\nhf_access_token\n,\nwandb_api_key\n) are defined in your Baseten\nworkspace settings\n.\nFor a complete guide on the\nTrainingJob\ntype, check out our\nSDK-reference\n.\n\u200b\nWhat can I run in the\nstart_commands\n?\nIn short, anything! Baseten Training is a framework-agnostic training platform. Any training framework and training methodology\nis supported. Typically, a\nrun.sh\nscript is used. An example might look like this:\nrun.sh\nCopy\nAsk AI\n#!/bin/bash\n# Exit immediately if a command exits with a non-zero status\nset\n-e\n# Install dependencies\npip\ninstall\n-r\nrequirements.txt\n# authenticate with wandb\nwandb\nlogin\n$WANDB_API_KEY\n# defined via Runtime.EnvironmentVariables\n# download models and datasets\nhuggingface-cli\ndownload\ngoogle/gemma-3-27b-it\nhuggingface-cli\ndownload\nAbirate/english_quotes\n--repo-type\ndataset\n# Run training\naccelerate\nlaunch\n--config_file\nconfig.yml\n--num_processes\n$BT_NUM_GPUS\ntrain.py\n\u200b\nAdditional features\nWe\u2019ve kept the above config simple to help you get off the ground - but there\u2019s a lot more you can do Baseten Training:\nCheckpointing\n- automatically save and deploy your model checkpoints.\nTraining Cache\n- speed up training by caching data and models between jobs.\nMultinode\n- train on multiple GPU nodes to make the most out of your compute.\n\u200b\nStep 2: Submit Your Training Job\nOnce your\nconfig.py\nand any local artifacts are ready, you submit the training\njob using the\ntruss\nCLI\n:\nCopy\nAsk AI\ntruss\ntrain\npush\nconfig.py\nThis command does the following:\nParses your\nconfig.py\n.\nPackages any local files in the directory (and subdirectories) alongside\nconfig.py\n.\nCreates or updates the\nTrainingProject\nspecified in your config.\nSubmits the defined\nTrainingJob\nunder that project.\nUpon successful submission, the CLI will output helpful information about your job:\nCopy\nAsk AI\n\u2728 Training job successfully created!\n\ud83e\udeb5 View logs for your job via `truss train logs --job-id e3m512w [--tail]`\n\ud83d\udd0d View metrics for your job via `truss train metrics --job-id e3m512w`\nKeep the Job ID handy, as you\u2019ll use it for\nmanaging and monitoring your job\n.\n\u200b\nNext steps\nCore Concepts\n: Deepen your understanding of Baseten\nTraining and explore key features like\nCheckpointingConfig\n, Training Cache, and Multinode.\nManagement\n: Learn how to check status,\nview logs and metrics, and stop jobs.\nWas this page helpful?\nYes\nNo\nPrevious\nConcepts\nUnderstanding the conceptual framework of Baseten Training for effective model development.\nNext\nOn this page\nPrerequisites\nStep 1: Define your training configuration\nKey considerations for your Baseten training configuration file\nWhat can I run in the start_commands?\nAdditional features\nStep 2: Submit Your Training Job\nNext steps\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 474865, "end_char_idx": 478780, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8cd6cfd4-2f6b-4285-8645-5b9b1de2d468": {"__data__": {"id_": "8cd6cfd4-2f6b-4285-8645-5b9b1de2d468", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a9447afe-1f53-44e6-9143-b96e57fcd4f5", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "a9407395604832cbfd28e33d4810ae6af015f00417a69914b5165b11c7a77462", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "16c6dd83-fbd3-4671-8af8-c45ed743b662", "node_type": "1", "metadata": {}, "hash": "cc7f9536514b11e5396d046dbe28f44c87c1afb7f1b16adad76951fead8bc83c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u200b\nNext steps\nCore Concepts\n: Deepen your understanding of Baseten\nTraining and explore key features like\nCheckpointingConfig\n, Training Cache, and Multinode.\nManagement\n: Learn how to check status,\nview logs and metrics, and stop jobs.\nWas this page helpful?\nYes\nNo\nPrevious\nConcepts\nUnderstanding the conceptual framework of Baseten Training for effective model development.\nNext\nOn this page\nPrerequisites\nStep 1: Define your training configuration\nKey considerations for your Baseten training configuration file\nWhat can I run in the start_commands?\nAdditional features\nStep 2: Submit Your Training Job\nNext steps\nAssistant\nResponses are generated using AI and may contain mistakes.\n\n\nContent from https://docs.baseten.co/training/management:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nDeveloping a Chain\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nExporting metrics\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nTraining\nManagement\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nOnce you have submitted training jobs, Baseten provides tools to manage your\nTrainingProject\ns and individual\nTrainingJob\ns. You can use the\nCLI\nor the\nAPI\nto manage your jobs.\n\u200b\nTrainingProject\nmanagement\nListing Projects:\nTo view all your training projects:\nCopy\nAsk AI\ntruss\ntrain\nview\nThis command will list all\nTrainingProject\ns you have access to, typically showing their names and IDs. Additionally, this command will show all active jobs.\nViewing Jobs within a Project:\nTo see all jobs associated with a specific project, use its\nproject-id\n(obtained when creating the project or from\ntruss train view\n):\nCopy\nAsk AI\ntruss\ntrain\nview\n--project-id\n<\nyour_project_i\nd\n>\n\u200b\nTrainingJob\nmanagement\nAfter submitting a job with\ntruss train push config.py\n, you receive a\nproject_id\nand\njob_id\n.\nListing Jobs:\nAs shown above, you can list all jobs within a project using:\nCopy\nAsk AI\ntruss\ntrain\nview\n--project-id\n<\nyour_project_i\nd\n>\nThis will typically show job IDs, statuses, creation times, etc.\nChecking Status and Retrieving Logs:\nTo view the logs for a specific job, you can tail them in real-time or fetch existing logs.\nTo view logs for the most recently submitted job in the current context (e.g., if you just pushed a job from your current terminal directory):\nCopy\nAsk AI\ntruss\ntrain\nlogs\n--tail\nTo view logs for a specific job using its\njob-id\n:\nCopy\nAsk AI\ntruss\ntrain\nlogs\n--job-id\n<\nyour_job_i\nd\n>\n[--tail]\nAdd\n--tail\nto follow the logs live.\nUnderstanding Job Statuses:\nThe\ntruss train view\nand\ntruss train logs\ncommands will help you track which status a job is in. For more on the job lifecycle, see the\nLifecycle\npage.\nStopping a\nTrainingJob\n:\nIf you need to stop a running job, use the\nstop\ncommand with the job\u2019s project ID and job ID:\nCopy\nAsk AI\ntruss\ntrain\nstop\n--job-id\n<\nyour_job_i\nd\n>\ntruss\ntrain\nstop\n--all\n# Stops all active jobs; Will prompt the user for confirmation.\nThis will transition the job to the\nTRAINING_JOB_STOPPED\nstate.\nUnderstanding Job Outputs & Checkpoints:\nThe primary outputs of a successful\nTrainingJob\nare model\ncheckpoints\n(if checkpointing is enabled and configured).\nThese checkpoints are stored by Baseten. Refer to the\nCheckpointing section in Core Concepts\nfor how\nCheckpointingConfig\nworks.\nWhen you are ready to\ndeploy a model\n, you will specify which checkpoints to use. The\nmodel_name\nyou assign during deployment (via\nDeployCheckpointsConfig\n) becomes the identifier for this trained model version derived from your specific job\u2019s checkpoints.\nYou can see the available checkpoints for a job via the\nTraining API\n.\nWas this page helpful?\nYes\nNo\nPrevious\nDeploying checkpoints\nHow to deploy checkpoints from Baseten Training jobs as usable models.\nNext\nOn this page\nTrainingProject management\nTrainingJob management\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 478095, "end_char_idx": 482432, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "16c6dd83-fbd3-4671-8af8-c45ed743b662": {"__data__": {"id_": "16c6dd83-fbd3-4671-8af8-c45ed743b662", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8cd6cfd4-2f6b-4285-8645-5b9b1de2d468", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "5ddc17f5872c938b14219e4f41903014dbefd03ac200d0752eaa94e222f52561", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "16d7b1b0-c92d-497d-9234-a6ee77da68b1", "node_type": "1", "metadata": {}, "hash": "616121866e0437e6575cebd53236a890e955dc7dfef6f5031d6680dc8bdd9c7a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Content from https://docs.baseten.co/training/overview:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nDeveloping a Chain\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nExporting metrics\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nTraining\nOverview\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\nWelcome to Baseten\nTraining\n, a powerful product designed to streamline and manage the entire lifecycle of your machine learning model training.\n\u200b\nUse cases\nBaseten Training provides a robust, scalable, and configurable platform for:\nTraining machine learning models efficiently.\nManaging training artifacts, such as model checkpoints and logs.\nSeamlessly productionize trained models to scalable deployments.\n\u200b\nBenefits of using Baseten training\nLeveraging Baseten Training for your model training workflows offers several key advantages:\nReproducibility:\nEnsure consistent training runs by precisely defining your environment, code, and configurations.\nScalability:\nEasily scale your training jobs from single-node to multi-node distributed training to handle large datasets and complex models.\nSimplified Management:\nOrganize, monitor, and manage your training projects and jobs in a centralized platform.\nResource Flexibility:\nConfigure compute resources (CPU, GPU, memory) tailored to the specific needs of each training job.\nIntegrated Workflow:\nTransition from training to inference and evals seemlessly within the Baseten ecosystem.\nArtifact Management:\nHandle large artifacts like models, checkpoints, and datasets efficiently with Baseten storage.\nFramework Agnostic:\nBring your favorite training framework or roll your own with Baseten\u2019s framework-agnostic training API.\n\u200b\nGet started\nCheck out our\nGetting Started\nguide to get started with training on Baseten.\n\u200b\nGo deeper\nUse the following resources to learn more about training on Baseten:\nCLI Reference\nSDK Reference\nAPI Reference\nWas this page helpful?\nYes\nNo\nPrevious\nGetting started\nYour first steps to creating and running training jobs on Baseten.\nNext\nOn this page\nUse cases\nBenefits of using Baseten training\nGet started\nGo deeper\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 482435, "end_char_idx": 485106, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "16d7b1b0-c92d-497d-9234-a6ee77da68b1": {"__data__": {"id_": "16d7b1b0-c92d-497d-9234-a6ee77da68b1", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "16c6dd83-fbd3-4671-8af8-c45ed743b662", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "4454c3f694ef1e5c48a7ba90f45c91e40ece3903a03a62deaee4b53b6bb77fc7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "634a7b01-dcd2-403f-b543-f87f8420f2a5", "node_type": "1", "metadata": {}, "hash": "09160d6c78cbe7f0a00623e6f644e84f16dee82148c46fceb85b36f03eec451e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Content from https://docs.baseten.co/troubleshooting/deployments:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nDeveloping a Chain\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nExporting metrics\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nTroubleshooting\nDeployments\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\n\u200b\nIssue:\ntruss push\ncan\u2019t find\nconfig.yaml\nCopy\nAsk AI\n[Errno 2] No such file or directory:\n'/Users/philipkiely/Code/demo_docs/config.yaml'\n\u200b\nFix: set correct target directory\nThe directory\ntruss push\nis looking at is not a Truss. Make sure you\u2019re giving\ntruss push\naccess to the correct directory by:\nRunning\ntruss push\nfrom the directory containing the Truss. You should see the file\nconfig.yaml\nwhen you run\nls\nin your working directory.\nOr passing the target directory as an argument, such as\ntruss push /path/to/my-truss\n.\n\u200b\nIssue: unexpected failure during model build\nDuring the model build step, there can be unexpected failures from temporary circumstances. An example is a network error while downloading model weights from Hugging Face or installing a Python package from PyPi.\n\u200b\nFix: restart deploy from Baseten UI\nFirst, check your model logs to determine the exact cause of the error. If it\u2019s an error during model download, package installation, or similar, you can try restarting the deploy from the model dashboard in your workspace.\nWas this page helpful?\nYes\nNo\nPrevious\nInference\nTroubleshoot common problems during model inference\nNext\nOn this page\nIssue: truss push can\u2019t find config.yaml\nFix: set correct target directory\nIssue: unexpected failure during model build\nFix: restart deploy from Baseten UI\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 485109, "end_char_idx": 487323, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "634a7b01-dcd2-403f-b543-f87f8420f2a5": {"__data__": {"id_": "634a7b01-dcd2-403f-b543-f87f8420f2a5", "embedding": null, "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d914f83b-13c7-4aa6-9d99-6c6966cefb76", "node_type": "4", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "ba2d21184296ab6bfe938a5b69c2a9b85bf2c9e75398af5288e9173d882da990", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "16d7b1b0-c92d-497d-9234-a6ee77da68b1", "node_type": "1", "metadata": {"file_path": "/Users/alexker/code/agent-starter-react-baseten/data/raw_data.txt", "file_name": "raw_data.txt", "file_type": "text/plain", "file_size": 493899, "creation_date": "2025-06-25", "last_modified_date": "2025-06-25"}, "hash": "7a9e5d1ee74e68f721a095acff87aab32084301ed0302900d103717de790e2a5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Content from https://docs.baseten.co/troubleshooting/inference:\n\nBaseten\nhome page\nSearch...\n\u2318\nK\nGet started\nOverview\nQuick start\nConcepts\nWhy Baseten\nHow Baseten works\nDevelopment\nConcepts\nModel APIs\nDeveloping a model\nDeveloping a Chain\nDeployment\nConcepts\nDeployments\nEnvironments\nResources\nAutoscaling\nInference\nConcepts\nCall your model\nStreaming\nAsync inference\nStructured LLM output\nOutput formats\nIntegrations\nTraining\nOverview\nGetting started\nConcepts\nManagement\nDeploying checkpoints\nObservability\nMetrics\nStatus and health\nSecurity\nExporting metrics\nTracing\nBilling and usage\nTroubleshooting\nDeployments\nInference\nSupport\nReturn to Baseten\nBaseten\nhome page\nSearch...\n\u2318\nK\nAsk AI\nSearch...\nNavigation\nTroubleshooting\nInference\nDocumentation\nExamples\nReference\nStatus\nDocumentation\nExamples\nReference\nStatus\n\u200b\nModel I/O issues\n\u200b\nError: JSONDecodeError\nCopy\nAsk AI\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\nThis error means you\u2019re attempting to pass a model input that is not JSON-serializable. For example, you might have left out the double quotes required for a valid string:\nCopy\nAsk AI\ntruss\npredict\n-d\n'This is not a string'\n# Wrong\ntruss\npredict\n-d\n'\"This is a string\"'\n# Correct\n\u200b\nModel version issues\n\u200b\nError: No OracleVersion matches the given query\nCopy\nAsk AI\n<Server response: {\n'errors': [{\n'message': 'No OracleVersion matches the given query.',\n'locations': [{'line': 3, 'column': 13}],\n'path': ['model_version']\n}],\n'data': {'model_version': None}\n}>\nMake sure that the model ID or deployment ID you\u2019re passing is correct and that the associated model has not been deleted.\nAdditionally, make sure you\u2019re using the correct endpoint:\nProduction environment endpoint\n.\nDevelopment deployment endpoint\n.\nDeployment endpoint\n.\n\u200b\nAuthentication issues\n\u200b\nError: Service provider not found\nCopy\nAsk AI\nValueError: Service provider example-service-provider not found in ~/.trussrc\nThis error means your\n~/.trussrc\nis incomplete or incorrect. It should be formatted as follows:\nCopy\nAsk AI\n[baseten]\nremote_provider = baseten\napi_key = YOUR.API_KEY\nremote_url = https://app.baseten.co\n\u200b\nError: You have to log in to perform the request\nCopy\nAsk AI\n<Server response: {\n'errors': [{\n'message': 'You have to log in to perform the request',\n'locations': [{'line': 3, 'column': 13}],\n'path': ['model_version'],\n'extensions': {'code': 'UNAUTHENTICATED_ACCESS'}\n}],\n'data': {'model_version': None}\n}>\nThis error occurs on\ntruss predict\nwhen the API key in\n~/.trussrc\nfor a given host is missing or incorrect. To fix it, update your API key in the\n~/.trussrc\nfile.\n\u200b\nError: Please check the API key you provided\nCopy\nAsk AI\n{\n\"error\": \"please check the api-key you provided\"\n}\nThis error occurs when using\ncurl\nor similar to call the model via its API endpoint when the API key passed in the request header is not valid. Make sure you\u2019re using a valid API key then try again.\nWas this page helpful?\nYes\nNo\nPrevious\nDeployments\nTroubleshoot common problems during model deployment\nOn this page\nModel I/O issues\nError: JSONDecodeError\nModel version issues\nError: No OracleVersion matches the given query\nAuthentication issues\nError: Service provider not found\nError: You have to log in to perform the request\nError: Please check the API key you provided\nAssistant\nResponses are generated using AI and may contain mistakes.", "mimetype": "text/plain", "start_char_idx": 487326, "end_char_idx": 490683, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}}}