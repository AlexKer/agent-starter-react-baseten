Content from https://docs.baseten.co/concepts/howbasetenworks:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Developing a Chain
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Exporting metrics
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Concepts
How Baseten works
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
It supports multiple entry points depending on your workflow—whether you’re deploying a dedicated model, calling an open-source LLM via our Model API, or training from scratch.
At the core is the Baseten Inference Stack:
performant model runtimes on top of Inference optimized infrastructure. Instead of managing infrastructure, scaling policies, and performance optimization, you can focus on building and iterating on your AI-powered applications.
​
Dedicated deployments
This is the primary workflow for teams deploying custom, open-source, or fine-tuned models with full control.
Baseten’s deployment stack is structured around four key pillars:
​
Development
Package any model using Truss, our open-source framework for defining dependencies, hardware, and custom logic—no Docker required. For more advanced use cases, build compound inference systems using Chains, orchestrating multiple models, APIs, and processing steps.
Developing a model
Package and deploy any AI/ML model as an API with Truss or a Custom Server.
Developing a Chain
Build multi-model workflows by chaining models, pre/post-processing, and
business logic.
​
Deployment
Deploy models to dedicated, autoscaling infrastructure. Use Environments for controlled versioning, rollouts, and promotion between staging and production. Support includes scale-to-zero, canary deploys, and structured model management.
​
Inference
Serve synchronous, asynchronous, and streaming predictions with configurable execution controls. Optimize for latency, throughput, or cost depending on your application’s needs.
​
Observability
Monitor model health and performance with real-time metrics, logs, and detailed request traces. Export data to observability tools like Datadog or Prometheus. Debug behavior with full visibility into inputs, outputs, and errors.
This full-stack infrastructure, from packaging to observability, is powered by the
Baseten Inference Stack
: performant model runtimes, cross-cloud availability, and seamless developer workflows.
​
Model APIs
Model APIs
offer a fast, reliable path to production for LLM-powered features. Use OpenAI-compatible endpoints to call performant open-source models like Llama 4, DeepSeek, and Qwen, with support for structured outputs and tool calling.
If your code already works with OpenAI’s SDKs, it’ll work with Baseten—no wrappers or rewrites required.
​
Training
Baseten Training
provides scalable infrastructure for running containerized training jobs. Define your code, environment, and compute resources; manage checkpoints and logs; and transition seamlessly from training to deployment.
Organize work with TrainingProjects and track reproducible runs via TrainingJobs. Baseten supports any framework, from PyTorch to custom setups, with centralized artifact and job management.
​
Summary
Use
Dedicated Deployments
to run and scale production-grade models with full control.
Use
Model APIs
to quickly build LLM-powered features without managing infrastructure.
Use
Training
to run reproducible training jobs and productionize your own models.
Each product is built on the same core: reliable infrastructure, strong developer ergonomics, and a focus on operational excellence.
Was this page helpful?
Yes
No
Previous
Concepts
Next
On this page
Dedicated deployments
Development
Deployment
Inference
Observability
Model APIs
Training
Summary
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/concepts/whybaseten:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Developing a Chain
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Exporting metrics
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Concepts
Why Baseten
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
​
Mission-critical inference
Built for high-performance workloads, our platform optimizes inference performance across modalities, from state-of-the-art transcription to blazing-fast LLMs.
Built-in autoscaling, model performance optimizations, and deep observability tools ensure efficiency without complexity.
Trusted by top ML teams serving their products to millions of users, Baseten accelerates time to market for AI-driven products by building on four key pillars of inference: performance, infrastructure, tooling, and expertise.
​
Model performance
Baseten’s model performance engineers apply the latest research and custom engine optimizations in production, so you get low latency and high throughput out of the box.
Production-grade support for critical features, like speculative decoding and LoRA swapping, is baked into our platform.
​
Cloud-native infrastructure
Deploy
and
scale models
across clusters, regions, and clouds with five nines reliability.
We built all the orchestration and optimized the network routing to ensure global scalability without the operational complexity.
​
Model management tooling
Love your development ecosystem, with deep
observability
and easy-to-use tools for deploying, managing, and iterating on models in production.
Quickly serve open-source and custom models, ultra-low-latency compound AI systems, and custom Docker servers in our cloud or yours.
​
Forward deployed engineering
Baseten’s expert engineers work as an extension of your team, customizing deployments for your target performance, quality, and cost-efficiency metrics.
Get hands-on support with deep inference-specific expertise and 24/7 on-call availability.
Was this page helpful?
Yes
No
Previous
How Baseten works
Baseten is a platform for building, serving, and scaling AI models in production.
Next
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/deployment/autoscaling:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Developing a Chain
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Exporting metrics
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Deployment
Autoscaling
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
​
Configuring autoscaling
Autoscaling settings are
per deployment
and are inherited when promoting a model to production unless overridden.
Configure autoscaling through:
UI
→ Manage settings in your Baseten workspace.
API
→ Use the
autoscaling API
.
​
Replica Scaling
Each deployment scales within a configured range of replicas:
Minimum replicas
→ The lowest number of active replicas.
Default:
0
(scale to zero).
Maximum value: Cannot exceed the
maximum replica count
.
Maximum replicas
→ The upper limit of active replicas.
Default:
1
.
Max:
10
by default (contact support to increase).
When first deployed, the model starts with
1
replica (or the
minimum count
, if higher). As traffic increases, additional replicas
scale up
until the
maximum count
is reached. When traffic decreases, replicas
scale down
to match demand.
​
Autoscaler settings
The
autoscaler logic
is controlled by three key parameters:
Autoscaling window
→ Time window for traffic analysis before scaling up/down. Default: 60 seconds.
Scale down delay
→ Time before an unused replica is removed. Default: 900 seconds (15 minutes).
Concurrency target
→ Number of requests a replica should handle before scaling. Default: 1 request.
A
short autoscaling window
with a
longer scale-down delay
is recommended for
fast upscaling
while maintaining capacity during temporary dips.
​
Autoscaling behavior
​
Scaling Up
When the
average requests per active replica
exceed the
concurrency target
within the
autoscaling window
, more replicas are created until:
The
concurrency target is met
, or
The
maximum replica count
is reached.
​
Scaling Down
When traffic drops below the
concurrency target
, excess replicas are flagged for removal. The
scale-down delay
ensures that replicas are not removed prematurely:
If traffic
spikes again before the delay ends
, replicas remain active.
If the
minimum replica count
is reached, no further scaling down occurs.
​
Scale to zero
If you’re just testing your model or anticipate light and inconsistent traffic, scale to zero can save you substantial amounts of money.
Scale to zero means that when a deployed model is not receiving traffic, it scales down to zero replicas. When the model is called, Baseten spins up a new instance to serve model requests.
To turn on scale to zero, just set a deployment’s minimum replica count to zero. Scale to zero is enabled by default in the standard autoscaling config.
Models that have not received any traffic for more than two weeks will be
automatically deactivated. These models will need to be activated manually
before they can serve requests again.
​
Cold starts
A
cold start
is the time required to
initialize a new replica
when scaling up. Cold starts impact:
Scaled-to-zero deployments
→ The first request must wait for a new replica to start.
Scaling events
→ When traffic spikes and a deployment requires more replicas.
​
Cold Start Optimizations
Network accelerator
Baseten speeds up model loading from
Hugging Face, CloudFront, S3, and OpenAI
using parallelized
byte-range downloads
, reducing cold start delays.
Cold start pods
Baseten pre-warms specialized
cold start pods
to accelerate loading times. These pods appear in logs as
[Coldboost]
.
Example coldboost log line
Copy
Ask AI
Oct 09 9:20:25pm [
Coldboost
] Completed model.load() execution in 12650 ms
​
Autoscaling for development deployments
Development deployments have
fixed autoscaling constraints
to optimize for
live reload workflows
:
Min replicas:
0
Max replicas:
1
Autoscaling window:
60 seconds
Scale down delay:
900 seconds (15 min)
Concurrency target:
1 request
To enable full autoscaling,
promote the deployment and environment
like production.
Was this page helpful?
Yes
No
Previous
Concepts
Next
On this page
Configuring autoscaling
Replica Scaling
Autoscaler settings
Autoscaling behavior
Scaling Up
Scaling Down
Scale to zero
Cold starts
Cold Start Optimizations
Autoscaling for development deployments
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/deployment/concepts:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Developing a Chain
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Exporting metrics
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Deployment
Concepts
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
Baseten provides a flexible and scalable infrastructure for deploying and managing machine learning models. This page introduces key concepts -
deployments
,
environments
,
resources
, and
autoscaling
— that shape how models are served, tested, and optimized for performance and cost efficiency.
​
Deployments
Deployments
define how models are served, scaled, and updated. They optimize resource use with autoscaling, scaling to zero, and controlled traffic shifts while ensuring minimal downtime. Deployments can be deactivated to pause resource usage or deleted permanently when no longer needed.
​
Environments
Environments
group deployments, providing stable endpoints and autoscaling to manage model release cycles. They enable structured testing, controlled rollouts, and seamless transitions between staging and production. Each environment maintains its own settings and metrics, ensuring reliable and scalable deployments.
​
Resources
Resources
define the hardware allocated to a model server, balancing performance and cost. Choosing the right instance type ensures efficient inference without unnecessary overhead. Resources can be set before deployment in Truss or adjusted later in the model dashboard to match workload demands.
​
Autoscaling
Autoscaling
dynamically adjusts model resources to handle traffic fluctuations efficiently while minimizing costs. Deployments scale between a defined range of replicas based on demand, with settings for concurrency, scaling speed, and scale-to-zero for low-traffic models. Optimizations like network acceleration and cold start pods ensure fast response times even when scaling up from zero.
Was this page helpful?
Yes
No
Previous
Deployments
Deploy, manage, and scale machine learning models with Baseten
Next
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/deployment/deployments:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Developing a Chain
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Exporting metrics
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Deployment
Deployments
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
A
deployment
in Baseten is a
containerized instance of a model
that serves inference requests via an API endpoint. Deployments exist independently but can be
promoted to an environment
for structured access and scaling.
Every deployment is
automatically wrapped in a REST API
. Once deployed, models can be queried with a simple HTTP request:
Copy
Ask AI
import
requests
resp
=
requests.post(
"https://model-
{modelID}
.api.baseten.co/deployment/[
{deploymentID}
]/predict"
,
headers
=
{
"Authorization"
:
"Api-Key YOUR_API_KEY"
},
json
=
{
'text'
:
'Hello my name is
{MASK}
'
},
)
print
(resp.json())
Learn more about running inference on your deployment
​
Development deployment
A
development deployment
is a mutable instance designed for rapid iteration. It is always in the
development state
and cannot be renamed or detached from it.
Key characteristics:
Live reload
enables direct updates without redeployment.
Single replica, scales to zero
when idle to conserve compute resources.
No autoscaling or zero-downtime updates.
Can be promoted
to create a persistent deployment.
Once promoted, the development deployment transitions to a
deployment
and can optionally be promoted to an environment.
​
Environments & Promotion
Environments provide
logical isolation
for managing deployments but are
not required
for a deployment to function. A deployment can be executed independently or promoted to an environment for controlled traffic allocation and scaling.
The
production environment
exists by default.
Custom environments
(e.g., staging) can be created for specific workflows.
Promoting a deployment does not modify its behavior
, only its routing and lifecycle management.
​
Canary deployments
Canary deployments support
incremental traffic shifting
to a new deployment, mitigating risk during rollouts.
Traffic is routed in
10 evenly distributed stages
over a configurable time window.
Traffic only begins to shift once the new deployment reaches the min replica count of the current production model.
Autoscaling dynamically adjusts to real-time demand.
Canary rollouts can be enabled or canceled via the UI or
REST API
.
​
Managing Deployments
​
Naming deployments
By default, deployments of a model are named
deployment-1
,
deployment-2
, and so forth sequentially. You can instead give deployments custom names via two methods:
While creating the deployment, using a
command line argument in truss push
.
After creating the deployment, in the model management page within your Baseten dashboard.
Renaming deployments is purely aesthetic and does not affect model management API paths, which work via model and deployment IDs.
​
Deactivating a deployment
A deployment can be deactivated to suspend inference execution while preserving configuration.
Remains visible in the dashboard.
Consumes no compute resources
but can be reactivated anytime.
API requests return a 404 error while deactivated.
For demand-driven deployments, consider
autoscaling with scale to zero
.
​
Deleting deployments
Deployments can be
permanently deleted
, but production deployments must be replaced before deletion.
Deleted deployments are purged from the dashboard
but retained in usage logs.
All associated compute resources are released.
API requests return a 404 error post-deletion.
Deletion is irreversible — use deactivation if retention is required.
Was this page helpful?
Yes
No
Previous
Environments
Manage your model’s release cycles with environments.
Next
On this page
Development deployment
Environments & Promotion
Canary deployments
Managing Deployments
Naming deployments
Deactivating a deployment
Deleting deployments
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/deployment/environments:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Developing a Chain
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Exporting metrics
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Deployment
Environments
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
Environments provide structured management for deployments, ensuring controlled rollouts, stable endpoints, and autoscaling. They help teams stage, test, and release models without affecting production traffic.
Deployments can be promoted to an environment (e.g., “staging”) to validate outputs before moving to production, allowing for safer model iteration and evaluation.
​
Using Environments to manage deployments
Environments support
structured validation
before promoting a deployment, including:
Automated tests & evaluations
Manual testing in pre-production
Gradual traffic shifts with canary deployments
Shadow serving for real-world analysis
Promoting a deployment ensures it inherits
environment-specific scaling and monitoring settings
, such as:
Dedicated API endpoint
→
Predict API Reference
Autoscaling controls
→ Scale behavior is managed per environment.
Traffic ramp-up
→ Enable
canary rollouts
.
Monitoring & Metrics
→
Export environment metrics
.
A
production environment
operates like any other environment but has restrictions:
It cannot be deleted
unless the entire model is removed.
You cannot create additional environments named “production.”
​
Creating custom environments
In addition to the standard
production
environment, you can create as many custom environments as needed. There are two ways to create a custom environment:
In the model management page on the Baseten dashboard.
Via the
create environment endpoint
in the model management API.
​
Promoting deployments to environments
When a deployment is promoted, Baseten follows a
three-step process
:
A
new deployment
is created with a unique deployment ID.
The deployment
initializes resources
and becomes active.
The new deployment
replaces the existing deployment
in that environment.
If there was
no previous deployment, default autoscaling settings
are applied.
If a
previous deployment existed
, the new one
inherits autoscaling settings
, and the old deployment is
demoted and scales to zero
.
​
Promoting a Published Deployment
If a
published deployment
(not a development deployment) is promoted:
Its
autoscaling settings are updated
to match the environment.
If
inactive
, it must be
activated
before promotion.
Previous deployments are
demoted but remain in the system
, retaining their
deployment ID and scaling behavior
.
​
Deploying directly to an environment
You can
skip development stage
and deploy directly to an environment by specifying
--environment
in
truss push
:
Copy
Ask AI
cd
my_model/
truss
push
--environment
{environment_name}
Only one active promotion per environment is allowed at a time.
​
Accessing environments in your code
The
environment name
is available in
model.py
via the
environment
keyword argument:
Copy
Ask AI
def
__init__
(
self
,
**
kwargs
):
self
._environment
=
kwargs[
"environment"
]
To ensure the
environment variable remains updated
, enable** “Re-deploy when promoting” **in the UI or via the
REST API
. This guarantees the environment is fully initialized after a promotion.
​
Deleting environments
Environments can be deleted,
except for production
. To remove a
production deployment
, first
promote another deployment to production
or delete the entire model.
Deleted environments are removed from the overview
but remain in billing history.
They do not consume resources
after deletion.
API requests to a deleted environment return a 404 error.
Deletion is permanent - consider deactivation instead.
Was this page helpful?
Yes
No
Previous
Resources
Manage and configure model resources
Next
On this page
Using Environments to manage deployments
Creating custom environments
Promoting deployments to environments
Promoting a Published Deployment
Deploying directly to an environment
Accessing environments in your code
Deleting environments
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/deployment/resources:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Developing a Chain
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Exporting metrics
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Deployment
Resources
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
Every AI/ML model on Baseten runs on an
instance
, a dedicated set of hardware allocated to the model server. Selecting the right instance type ensures
optimal performance
while controlling
compute costs
.
Insufficient resources
→ Slow inference or failures.
Excess resources
→ Higher costs without added benefit.
​
Instance type resource components
Instance
→ The allocated hardware for inference.
vCPU
→ Virtual CPU cores for general computing.
RAM
→ Memory available to the CPU.
GPU
→ Specialized hardware for accelerated ML workloads.
VRAM
→ Dedicated GPU memory for model execution.
​
Configuring model resources
Resources can be defined
before deployment
in Truss or
adjusted later
via the Baseten UI.
​
Defining resources in Truss
Define resource requirements in
config.yaml
before running
truss push
. Any changes after deployment will not impact previous deployments. Running
truss push
again will create a new deployment using the resources specified in the
config.yaml
.
The only exception is the
development
deployment. It will be redeployed with the new specified resources.
Example (Stable Diffusion XL):
config.yaml
Copy
Ask AI
resources
:
accelerator
:
A10G
cpu
:
"4"
memory
:
16Gi
use_gpu
:
true
Baseten provisions the
smallest instance that meets the specified constraints
:
**cpu: “3” or “4” → **Maps to a 4-core instance.
**cpu: “5” to “8” → **Maps to an 8-core instance.
Gi
in
resources.memory
refers to
Gibibytes
, which are slightly larger
than
Gigabytes
.
​
Updating resources in the Baseten UI
Once deployed, resource configurations can only be updated
through the Baseten UI
. Changing the instance type will deploy a new copy of the deployment using the new specified instance type.
Like when running
truss push
, the
development
deployment will be redeployed with the new specified instance type.
For a list of available instance types, see the
instance type reference
.
​
Instance Type Reference
Specs and benchmarks for every Baseten instance type.
Choosing the right instance for model inference means balancing performance and cost. This page lists all available instance types on Baseten to help you deploy and serve models effectively.
​
CPU-only Instances
Cost-effective options for lighter workloads. No GPU.
Starts at
: $0.00058/min
Best for
: Transformers pipelines, small QA models, text embeddings
Instance
$/min
vCPU
RAM
1×2
$0.00058
1
2 GiB
1×4
$0.00086
1
4 GiB
2×8
$0.00173
2
8 GiB
4×16
$0.00346
4
16 GiB
8×32
$0.00691
8
32 GiB
16×64
$0.01382
16
64 GiB
Example workloads:
1x2
: Text classification (e.g., Truss quickstart)
4x16
: LayoutLM Document QA
4x16+
: Sentence Transformers embeddings on larger corpora
​
GPU Instances
Accelerated inference for LLMs, diffusion models, and Whisper.
Instance
$/min
vCPU
RAM
GPU
VRAM
T4x4x16
$0.01052
4
16 GiB
NVIDIA T4
16 GiB
T4x8x32
$0.01504
8
32 GiB
NVIDIA T4
16 GiB
T4x16x64
$0.02408
16
64 GiB
NVIDIA T4
16 GiB
L4x4x16
$0.01414
4
16 GiB
NVIDIA L4
24 GiB
L4:2x4x16
$0.04002
24
96 GiB
2 NVIDIA L4s
48 GiB
L4:4x48x192
$0.08003
48
192 GiB
4 NVIDIA L4s
96 GiB
A10Gx4x16
$0.02012
4
16 GiB
NVIDIA A10G
24 GiB
A10Gx8x32
$0.02424
8
32 GiB
NVIDIA A10G
24 GiB
A10Gx16x64
$0.03248
16
64 GiB
NVIDIA A10G
24 GiB
A10G:2x24x96
$0.05672
24
96 GiB
2 NVIDIA A10Gs
48 GiB
A10G:4x48x192
$0.11344
48
192 GiB
4 NVIDIA A10Gs
96 GiB
A10G:8x192x768
$0.32576
192
768 GiB
8 NVIDIA A10Gs
188 GiB
V100x8x61
$0.06120
16
61 GiB
NVIDIA V100
16 GiB
A100x12x144
$0.10240
12
144 GiB
1 NVIDIA A100
80 GiB
A100:2x24x288
$0.20480
24
288 GiB
2 NVIDIA A100s
160 GiB
A100:3x36x432
$0.30720
36
432 GiB
3 NVIDIA A100s
240 GiB
A100:4x48x576
$0.40960
48
576 GiB
4 NVIDIA A100s
320 GiB
A100:5x60x720
$0.51200
60
720 GiB
5 NVIDIA A100s
400 GiB
A100:6x72x864
$0.61440
72
864 GiB
6 NVIDIA A100s
480 GiB
A100:7x84x1008
$0.71680
84
1008 GiB
7 NVIDIA A100s
560 GiB
A100:8x96x1152
$0.81920
96
1152 GiB
8 NVIDIA A100s
640 GiB
H100x26x234
$0.16640
26
234 GiB
1 NVIDIA H100
80 GiB
H100:2x52x468
$0.33280
52
468 GiB
2 NVIDIA H100s
160 GiB
H100:4x104x936
$0.66560
104
936 GiB
4 NVIDIA H100s
320 GiB
H100:8x208x1872
$1.33120
208
1872 GiB
8 NVIDIA H100s
640 GiB
H100MIG:3gx13x117
$0.08250
13
117 GiB
Fractional NVIDIA H100
40 GiB
​
GPU Details & Workloads
​
T4
Turing-series GPU
2,560 CUDA / 320 Tensor cores
16 GiB VRAM
Best for:
Whisper, small LLMs like StableLM 3B
​
L4
Ada Lovelace-series GPU
7,680 CUDA / 240 Tensor cores
24 GiB VRAM, 300 GiB/s
24 GiB VRAM, 300 GiB/s
121 TFLOPS (fp16)
Best for
: Stable Diffusion XL
Limit
: Not suitable for LLMs due to bandwidth
​
A10G
Ampere-series GPU
9,216 CUDA / 288 Tensor cores
24 GiB VRAM, 600 GiB/s
70 TFLOPS (fp16)
Best for
: Mistral 7B, Whisper, Stable Diffusion/SDXL
​
V100
Volta-series GPU
16 GiB VRAM
Best for
: Legacy workloads needing V100-specific support
​
A100
Ampere-series GPU
6,912 CUDA / 432 Tensor cores
80 GiB VRAM, 1.94 TB/s
312 TFLOPS (fp16)
Best for
: Mixtral, Llama 2 70B (2 A100s), Falcon 180B (5 A100s), SDXL
​
H100
Hopper-series GPU
16,896 CUDA / 640 Tensor cores
80 GiB VRAM, 3.35 TB/s
990 TFLOPS (fp16)
Best for
: Mixtral 8x7B, Llama 2 70B (2×H100), SDXL
​
H100MIG
Fractional H100 (3/7 compute, ½ memory)
7,242 CUDA cores, 40 GiB VRAM
1.675 TB/s bandwidth
Best for
: Efficient LLM inference at lower cost than A100
Was this page helpful?
Yes
No
Previous
Autoscaling
Autoscaling dynamically adjusts the number of active replicas to **handle variable traffic** while minimizing idle compute costs.
Next
On this page
Instance type resource components
Configuring model resources
Defining resources in Truss
Updating resources in the Baseten UI
Instance Type Reference
CPU-only Instances
GPU Instances
GPU Details & Workloads
T4
L4
A10G
V100
A100
H100
H100MIG
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/development/chain/binaryio:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Developing a Chain
Overview
Concepts
Your first Chain
Architecture & Design
Local Development
Deploy
Invocation
Watch
Subclassing
Streaming
Binary IO
Error Handling
Truss Integration
Engine Builder Models
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Exporting metrics
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Developing a Chain
Binary IO
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
Numeric data or audio/video are most efficiently transmitted as bytes.
Other representations such as JSON or base64 encoding loose precision, add
significant parsing overhead and increase message sizes (e.g. ~33% increase
for base64 encoding).
Chains extends the JSON-centred pydantic ecosystem with two ways how you can
include binary data: numpy array support and raw bytes.
​
Numpy
ndarray
support
Once you have your data represented as a numpy array, you can easily (and
often without copying) convert it to
torch
,
tensorflow
or other common
numeric library’s objects.
To include numpy arrays in a pydantic model, chains has a special field type
implementation
NumpyArrayField
. For example:
Copy
Ask AI
import
numpy
as
np
import
pydantic
from
truss_chains
import
pydantic_numpy
class
DataModel
(
pydantic
.
BaseModel
):
some_numbers: pydantic_numpy.NumpyArrayField
other_field:
str
...
numbers
=
np.random.random((
3
,
2
))
data
=
DataModel(
some_numbers
=
numbers,
other_field
=
"Example"
)
print
(data)
# some_numbers=NumpyArrayField(shape=(3, 2), dtype=float64, data=[
#   [0.39595027 0.23837526]
#   [0.56714894 0.61244946]
#   [0.45821942 0.42464844]])
# other_field='Example'
NumpyArrayField
is a wrapper around the actual numpy array. Inside your
python code, you can work with its
array
attribute:
Copy
Ask AI
data.some_numbers.array
+=
10
# some_numbers=NumpyArrayField(shape=(3, 2), dtype=float64, data=[
#   [10.39595027 10.23837526]
#   [10.56714894 10.61244946]
#   [10.45821942 10.42464844]])
# other_field='Example'
The interesting part is, how it serializes when making communicating between
Chainlets or with a client.
It can work in two modes: JSON and binary.
​
Binary
As an JSON alternative that supports byte data, Chains uses
msgpack
(with
msgpack_numpy
) to serialize the dict representation.
For Chainlet-Chainlet RPCs this is done automatically for you by enabling binary
mode of the dependency Chainlets, see
all options
:
Copy
Ask AI
import
truss_chains
as
chains
class
Worker
(
chains
.
ChainletBase
):
async
def
run_remote
(
self
,
data
: DataModel) -> DataModel:
data.some_numbers.array
+=
10
return
data
class
Consumer
(
chains
.
ChainletBase
):
def
__init__
(
self
,
worker
=
chains.depends(Worker,
use_binary
=
True
)):
self
._worker
=
worker
async
def
run_remote
(
self
):
numbers
=
np.random.random((
3
,
2
))
data
=
DataModel(
some_numbers
=
numbers,
other_field
=
"Example"
)
result
=
await
self
._worker.run_remote(data)
Now the data is transmitted in a fast and compact way between Chainlets
which often gives performance increases.
​
Binary client
If you want to send such data as input to a chain or parse binary output
from a chain, you have to add the
msgpack
serialization client-side:
Copy
Ask AI
import
requests
import
msgpack
import
msgpack_numpy
msgpack_numpy.patch()
# Register hook for numpy.
# Dump to "python" dict and then to binary.
data_dict
=
data.model_dump(
mode
=
"python"
)
data_bytes
=
msgpack.dumps(data_dict)
# Set binary content type in request header.
headers
=
{
"Content-Type"
:
"application/octet-stream"
,
"Authorization"
:
...
}
response
=
requests.post(url,
data
=
data_bytes,
headers
=
headers)
response_dict
=
msgpack.loads(response.content)
response_model
=
ResponseModel.model_validate(response_dict)
The steps of dumping from a pydantic model and validating the response dict
into a pydantic model can be skipped, if you prefer working with raw dicts
on the client.
The implementation of
NumpyArrayField
only needs
pydantic
, no other Chains
dependencies. So you can take that implementation code in isolation and
integrated it in your client code.
Some version combinations of
msgpack
and
msgpack_numpy
give errors, we
know that
msgpack = ">=1.0.2"
and
msgpack-numpy = ">=0.4.8"
work.
​
JSON
The JSON-schema to represent the array is a dict of
shape (tuple[int]),  dtype (str), data_b64 (str)
. E.g.
Copy
Ask AI
print
(data.model_dump_json())
'{"some_numbers":{"shape":[3,2],"dtype":"float64", "data_b64":"30d4/rnKJEAsvm...'
The base64 data corresponds to
np.ndarray.tobytes()
.
To get back to the array from the JSON string, use the model’s
model_validate_json
method.
As discussed in the beginning, this schema is not performant for numeric data
and only offered as a compatibility layer (JSON does not allow bytes) -
generally prefer the binary format.
​
Simple
bytes
fields
It is possible to add a
bytes
field to a pydantic model used in a chain,
or as a plain argument to
run_remote
. This can be useful to include
non-numpy data formats such as images or audio/video snippets.
In this case, the “normal” JSON representation does not work and all
involved requests or Chainlet-Chainlet-invocations must use binary mode.
The same steps as for arrays
above
apply: construct dicts
with
bytes
values and keys corresponding to the
run_remote
argument
names or the field names in the pydantic model. Then use
msgpack
to
serialize and deserialize those dicts.
Don’t forget to add
Content-type
headers and that
response.json()
will
not work.
Was this page helpful?
Yes
No
Previous
Error Handling
Understanding and handling Chains errors
Next
On this page
Numpy ndarray support
Binary
Binary client
JSON
Simple bytes fields
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/development/chain/concepts:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Developing a Chain
Overview
Concepts
Your first Chain
Architecture & Design
Local Development
Deploy
Invocation
Watch
Subclassing
Streaming
Binary IO
Error Handling
Truss Integration
Engine Builder Models
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Exporting metrics
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Developing a Chain
Concepts
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
​
Chainlet
A Chainlet is the basic building block of Chains. A Chainlet is a Python class
that specifies:
A set of compute resources.
A Python environment with software dependencies.
A typed interface
run_remote()
for other Chainlets to call.
This is the simplest possible Chainlet — only the
run_remote()
method is
required — and we can layer in other concepts to create a more capable Chainlet.
Copy
Ask AI
import
truss_chains
as
chains
class
SayHello
(
chains
.
ChainletBase
):
async
def
run_remote
(
self
,
name
:
str
) ->
str
:
return
f
"Hello,
{
name
}
"
You can modularize your code by creating your own chainlet sub-classes,
refer to our
subclassing guide
.
​
Remote configuration
Chainlets are meant for deployment as remote services. Each Chainlet specifies
its own requirements for compute hardware (CPU count, GPU type and count, etc)
and software dependencies (Python libraries or system packages). This
configuration is built into a Docker image automatically as part of the
deployment process.
When no configuration is provided, the Chainlet will be deployed on a basic
instance with one vCPU, 2GB of RAM, no GPU, and a standard set of Python and
system packages.
Configuration is set using the
remote_config
class variable
within the Chainlet:
Copy
Ask AI
import
truss_chains
as
chains
class
MyChainlet
(
chains
.
ChainletBase
):
remote_config
=
chains.RemoteConfig(
docker_image
=
chains.DockerImage(
pip_requirements
=
[
"torch==2.3.0"
,
...
]
),
compute
=
chains.Compute(
gpu
=
"H100"
,
...
),
assets
=
chains.Assets(
secret_keys
=
[
"hf_access_token"
],
...
),
)
See the
remote configuration reference
for a complete list of options.
​
Initialization
Chainlets are implemented as classes because we often want to set up expensive
static resources once at startup and then re-use it with each invocation of the
Chainlet. For example, we only want to initialize an AI model and download its
weights once then re-use it every time we run inference.
We do this setup in
__init__()
, which is run exactly once when the Chainlet is
deployed or scaled up.
Copy
Ask AI
import
truss_chains
as
chains
class
PhiLLM
(
chains
.
ChainletBase
):
def
__init__
(
self
) ->
None
:
import
torch
import
transformers
self
._model
=
transformers.AutoModelForCausalLM.from_pretrained(
PHI_HF_MODEL
,
torch_dtype
=
torch.float16,
device_map
=
"auto"
,
)
self
._tokenizer
=
transformers.AutoTokenizer.from_pretrained(
PHI_HF_MODEL
,
)
Chainlet initialization also has two important features: context and dependency
injection of other Chainlets, explained below.
​
Context (access information)
You can add
DeploymentContext
object as an optional argument to the
__init__
-method of a Chainlet.
This allows you to use secrets within your Chainlet, such as using
a
hf_access_token
to access a gated model on Hugging Face (note that when
using secrets, they also need to be added to the
assets
).
Copy
Ask AI
import
truss_chains
as
chains
class
MistralLLM
(
chains
.
ChainletBase
):
remote_config
=
chains.RemoteConfig(
...
assets
=
chains.Assets(
secret_keys
=
[
"hf_access_token"
],
...
),
)
def
__init__
(
self
,
# Adding the `context` argument, allows us to access secrets
context
: chains.DeploymentContext
=
chains.depends_context(),
) ->
None
:
import
transformers
# Using the secret from context to access a gated model on HF
self
._model
=
transformers.AutoModelForCausalLM.from_pretrained(
"mistralai/Mistral-7B-Instruct-v0.2"
,
use_auth_token
=
context.secrets[
"hf_access_token"
],
)
​
Depends (call other Chainlets)
The Chains framework uses the
chains.depends()
function in
Chainlets’
__init__()
method to track the dependency relationship between
different Chainlets within a Chain.
This syntax, inspired by dependency injection, is used to translate local Python
function calls into calls to the remote Chainlets in production.
Once a dependency Chainlet is added with
chains.depends()
, its
run_remote()
method can
call this dependency Chainlet, e.g. below
HelloAll
we can make calls to
SayHello
:
Copy
Ask AI
import
truss_chains
as
chains
class
HelloAll
(
chains
.
ChainletBase
):
def
__init__
(
self
,
say_hello_chainlet
=
chains.depends(SayHello)) ->
None
:
self
._say_hello
=
say_hello_chainlet
async
def
run_remote
(
self
,
names
: list[
str
]) ->
str
:
output
=
[]
for
name
in
names:
output.append(
self
._say_hello.run_remote(name))
return
"
\n
"
.join(output)
​
Run remote (chaining Chainlets)
The
run_remote()
method is run each time the Chainlet is called. It is the
sole public interface for the Chainlet (though you can have as many private
helper functions as you want) and its inputs and outputs must have type
annotations.
In
run_remote()
you implement the actual work of the Chainlet, such as model
inference or data chunking:
Copy
Ask AI
import
truss_chains
as
chains
class
PhiLLM
(
chains
.
ChainletBase
):
async
def
run_remote
(
self
,
messages
: Messages) ->
str
:
import
torch
model_inputs
=
await
self
._tokenizer.apply_chat_template(
messages,
tokenize
=
False
,
add_generation_prompt
=
True
)
inputs
=
await
self
._tokenizer(model_inputs,
return_tensors
=
"pt"
)
input_ids
=
inputs[
"input_ids"
].to(
"cuda"
)
with
torch.no_grad():
outputs
=
await
self
._model.generate(
input_ids
=
input_ids,
**
self
._generate_args)
output_text
=
await
self
._tokenizer.decode(
outputs[
0
],
skip_special_tokens
=
True
)
return
output_text
We recommend implementing this as an
async
method and using async APIs for
doing all the work (e.g. downloads, vLLM or TRT inference).
It is possible to stream results back, see our
streaming guide
.
If
run_remote()
makes calls to other Chainlets, e.g. invoking a dependency
Chainlet for each element in a list, you can benefit from concurrent
execution, by making the
run_remote()
an
async
method and starting the
calls as concurrent tasks
asyncio.ensure_future(self._dep_chainlet.run_remote(...))
.
​
Entrypoint
The entrypoint is called directly from the deployed Chain’s API endpoint and
kicks off the entire chain. The entrypoint is also responsible for returning the
final result back to the client.
Using the
@chains.mark_entrypoint
decorator, one Chainlet within a file is set as the entrypoint to the chain.
Copy
Ask AI
@chains.mark_entrypoint
class
HelloAll
(
chains
.
ChainletBase
):
Optionally you can also set a Chain display name (not to be confused with
Chainlet display name) with this decorator:
Copy
Ask AI
@chains.mark_entrypoint
(
"My Awesome Chain"
)
class
HelloAll
(
chains
.
ChainletBase
):
​
I/O and
pydantic
data types
To make orchestrating multiple remotely deployed services possible, Chains
relies heavily on typed inputs and outputs. Values must be serialized to a safe
exchange format to be sent over the network.
The Chains framework uses the type annotations to infer how data should be
serialized and currently is restricted to types that are JSON compatible. Types
can be:
Direct type annotations for simple types such as
int
,
float
,
or
list[str]
.
Pydantic models to define a schema for nested data structures or multiple
arguments.
An example of pydantic input and output types for a Chainlet is given below:
Copy
Ask AI
import
enum
import
pydantic
class
Modes
(
enum
.
Enum
):
MODE_0
=
"MODE_0"
MODE_1
=
"MODE_1"
class
SplitTextInput
(
pydantic
.
BaseModel
):
data:
str
num_partitions:
int
mode: Modes
class
SplitTextOutput
(
pydantic
.
BaseModel
):
parts: list[
str
]
part_lens: list[
int
]
Refer to the
pydantic docs
for more
details on how
to define custom pydantic data models.
Also refer to the
guide
about efficient integration
of binary and numeric data.
​
Chains compared to Truss
Tips for Truss users
Chains is an alternate SDK for packaging and deploying AI models. It carries over many features and concepts from Truss and gives you access to the benefits of Baseten (resource provisioning, autoscaling, fast cold starts, etc), but it is not a 1-1 replacement for Truss.
Here are some key differences:
Rather than running
truss init
and creating a Truss in a directory, a Chain
is a single file, giving you more flexibility for implementing multi-step
model inference. Create an example with
truss chains init
.
Configuration is done inline in typed Python code rather than in a
config.yaml
file.
While Chainlets are converted to Truss models when run on Baseten,
Chainlet != TrussModel
.
Chains is designed for compatibility and incremental adoption, with a stub
function for wrapping existing deployed models.
Was this page helpful?
Yes
No
Previous
Your first Chain
Build and deploy two example Chains
Next
On this page
Chainlet
Remote configuration
Initialization
Context (access information)
Depends (call other Chainlets)
Run remote (chaining Chainlets)
Entrypoint
I/O and pydantic data types
Chains compared to Truss
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/development/chain/deploy:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Developing a Chain
Overview
Concepts
Your first Chain
Architecture & Design
Local Development
Deploy
Invocation
Watch
Subclassing
Streaming
Binary IO
Error Handling
Truss Integration
Engine Builder Models
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Exporting metrics
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Developing a Chain
Deploy
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
Deploying a Chain is an atomic action that deploys every Chainlet
within the Chain. Each Chainlet specifies its own remote
environment — hardware resources, Python and system dependencies, autoscaling
settings.
​
Development
The default behavior for pushing a chain is to create a development deployment:
Copy
Ask AI
truss
chains
push
./my_chain.py
Where
my_chain.py
contains the entrypoint Chainlet for your Chain.
Development deployments are intended for testing and can’t scale past one
replica. Each time you make a development deployment, it overwrites the existing
development deployment.
Development deployments support rapid iteration with
watch
- see
above
guide
.
​
🆕 Environments
To deploy a Chain to an environment, run:
Copy
Ask AI
truss
chains
push
./my_chain.py
--environment
{env_name}
Environments are intended for live traffic and have access to full
autoscaling settings. Each time you deploy to an environment, a new deployment is
created. Once the new deployment is live, it replaces the previous deployment,
which is relegated to the published deployments list.
Learn more
about environments.
Was this page helpful?
Yes
No
Previous
Invocation
Call your deployed Chain
Next
On this page
Development
🆕 Environments
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/development/chain/design:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Developing a Chain
Overview
Concepts
Your first Chain
Architecture & Design
Local Development
Deploy
Invocation
Watch
Subclassing
Streaming
Binary IO
Error Handling
Truss Integration
Engine Builder Models
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Exporting metrics
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Developing a Chain
Architecture & Design
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
A Chain is composed of multiple connected Chainlets working together to perform
a task.
For example, the Chain in the diagram below takes a large audio file as input.
Then it splits it into smaller chunks, transcribes each chunk in parallel
(reducing the end-to-end latency), and finally aggregates and returns the
results.
To build an efficient Chain, we recommend drafting your high level
structure as a flowchart or diagram. This can help you identifying
parallelizable units of work and steps that need different (model/hardware)
resources.
If one Chainlet creates many “sub-tasks” by calling other dependency
Chainlets (e.g. in a loop over partial work items),
these calls should be done as
aynscio
-tasks that run concurrently.
That way you get the most out of the parallelism that Chains offers. This
design pattern is extensively used in the
audio transcription example
.
While using
asyncio
is essential for performance, it can also be tricky.
Here are a few caveats to look out for:
Executing operations in an async function that block the event loop for
more than a fraction of a second. This hinders the “flow” of processing
requests concurrently and starting RPCs to other Chainlets. Ideally use
native async APIs. Frameworks like vLLM or triton server offer such APIs,
similarly file downloads can be made async and you might find
AsyncBatcher
useful.
If there is no async support, consider running blocking code in a
thread/process pool (as an attributed of a Chainlet).
Creating async tasks (e.g. with
asyncio.ensure_future
) does not start
the task
immediately
. In particular, when starting several tasks in a loop,
ensure_future
must be alternated with operations that yield to the event
loop that, so the task can be started. If the loop is not
async for
or
contains other
await
statements, a “dummy” await can be added, for example
await asyncio.sleep(0)
. This allows the tasks to be started concurrently.
Was this page helpful?
Yes
No
Previous
Local Development
Iterating, Debugging, Testing, Mocking
Next
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/development/chain/engine-builder-models:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Developing a Chain
Overview
Concepts
Your first Chain
Architecture & Design
Local Development
Deploy
Invocation
Watch
Subclassing
Streaming
Binary IO
Error Handling
Truss Integration
Engine Builder Models
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Exporting metrics
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Developing a Chain
Engine Builder Models
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
Baseten’s
Engine Builder
enables the deployment of optimized model inference engines. Currently, it supports TensorRT-LLM. Truss Chains allows seamless integration of these engines into structured workflows. This guide provides a quick entry point for Chains users.
​
LLama 7B Example
Use the
EngineBuilderLLMChainlet
baseclass to configure an LLM engine. The additional
engine_builder_config
field specifies model architecture, repository, and runtime parameters and more, the full options are detailed in the
Engine Builder configuration guide
.
Copy
Ask AI
import
truss_chains
as
chains
from
truss.base
import
trt_llm_config, truss_config
class
Llama7BChainlet
(
chains
.
EngineBuilderLLMChainlet
):
remote_config
=
chains.RemoteConfig(
compute
=
chains.Compute(
gpu
=
truss_config.Accelerator.H100),
assets
=
chains.Assets(
secret_keys
=
[
"hf_access_token"
]),
)
engine_builder_config
=
truss_config.TRTLLMConfiguration(
build
=
trt_llm_config.TrussTRTLLMBuildConfiguration(
base_model
=
trt_llm_config.TrussTRTLLMModel.
LLAMA
,
checkpoint_repository
=
trt_llm_config.CheckpointRepository(
source
=
trt_llm_config.CheckpointSource.
HF
,
repo
=
"meta-llama/Llama-3.1-8B-Instruct"
,
),
max_batch_size
=
8
,
max_seq_len
=
4096
,
tensor_parallel_count
=
1
,
)
)
​
Differences from Standard Chainlets
No
run_remote
implementation: Unlike regular Chainlets,
EngineBuilderLLMChainlet
does not require users to implement
run_remote()
. Instead, it automatically wires into the deployed engine’s API. All LLM Chainlets have the same function signature:
chains.EngineBuilderLLMInput
as input and a stream (
AsyncIterator
) of strings as output. Likewise
EngineBuilderLLMChainlet
s can only be used as dependencies, but not have dependencies themselves.
No
run_local
(
guide
) and
watch
(
guide
) Standard Chains support a local debugging mode and watch. However, when using
EngineBuilderLLMChainlet
, local execution is not available, and testing must be done after deployment.
For a faster dev loop of the rest of your chain (everything except the engine builder chainlet) you can substitute those chainlets with stubs like you can do for an already deployed truss model [
guide
].
​
Integrate the Engine Builder Chainlet
After defining an
EngineBuilderLLMInput
like
Llama7BChainlet
above, you can use it as a dependency in other conventional chainlets:
Copy
Ask AI
from
typing
import
AsyncIterator
import
truss_chains
as
chains
@chains.mark_entrypoint
class
TestController
(
chains
.
ChainletBase
):
"""Example using the Engine Builder Chainlet in another Chainlet."""
def
__init__
(
self
,
llm
=
chains.depends(Llama7BChainlet)) ->
None
:
self
._llm
=
llm
async
def
run_remote
(
self
,
prompt
:
str
) -> AsyncIterator[
str
]:
messages
=
[{
"role"
:
"user"
,
"content"
: prompt}]
llm_input
=
chains.EngineBuilderLLMInput(
messages
=
messages)
async
for
chunk
in
self
._llm.run_remote(llm_input):
yield
chunk
Was this page helpful?
Yes
No
Previous
Concepts
Next
On this page
LLama 7B Example
Differences from Standard Chainlets
Integrate the Engine Builder Chainlet
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/development/chain/errorhandling:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Developing a Chain
Overview
Concepts
Your first Chain
Architecture & Design
Local Development
Deploy
Invocation
Watch
Subclassing
Streaming
Binary IO
Error Handling
Truss Integration
Engine Builder Models
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Exporting metrics
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Developing a Chain
Error Handling
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
Error handling in Chains follows the principle that the root cause “bubbles
up” until the entrypoint - which returns an error response. Similarly to how
python stack traces contain all the layers from where an exception was raised
up until the main function.
Consider the case of a Chain where the entrypoint calls
run_remote
of a
Chainlet named
TextToNum
and this in turn invokes
TextReplicator
. The
respective
run_remote
methods might also use other helper functions that
appear in the call stack.
Below is an example stack trace that shows how the root cause (a
ValueError
) is propagated up to the entrypoint’s
run_remote
method (this
is what you would see as an error log):
Copy
Ask AI
Chainlet-Traceback (most recent call last):
File "/packages/itest_chain.py", line 132, in run_remote
value = self._accumulate_parts(text_parts.parts)
File "/packages/itest_chain.py", line 144, in _accumulate_parts
value += self._text_to_num.run_remote(part)
ValueError: (showing chained remote errors, root error at the bottom)
├─ Error in dependency Chainlet `TextToNum`:
│   Chainlet-Traceback (most recent call last):
│     File "/packages/itest_chain.py", line 87, in run_remote
│       generated_text = self._replicator.run_remote(data)
│   ValueError: (showing chained remote errors, root error at the bottom)
│   ├─ Error in dependency Chainlet `TextReplicator`:
│   │   Chainlet-Traceback (most recent call last):
│   │     File "/packages/itest_chain.py", line 52, in run_remote
│   │       validate_data(data)
│   │     File "/packages/itest_chain.py", line 36, in validate_data
│   │       raise ValueError(f"This input is too long: {len(data)}.")
╰   ╰   ValueError: This input is too long: 100.
​
Exception handling and retries
Above stack trace is what you see if you don’t catch the exception. It is
possible to add error handling around each remote Chainlet invocation.
Chains tries to raise the same exception class on the
caller
Chainlet as was
raised in the
dependency
Chainlet.
Builtin exceptions (e.g.
ValueError
) always work.
Custom or third-party exceptions (e.g. from
torch
) can be only raised
in the caller if they are included in the dependencies of the caller as
well. If the exception class cannot be resolved, a
GenericRemoteException
is raised instead.
Note that the
message
of re-raised exceptions is the concatenation
of the original message and the formatted stack trace of the dependency
Chainlet.
In some cases it might make sense to simply retry a remote invocation (e.g.
if it failed due to some transient problems like networking or any “flaky”
parts).
depends
can be configured with additional
options
for that.
Below example shows how you can add automatic retries and error handling for
the call to
TextReplicator
in
TextToNum
:
Copy
Ask AI
import
truss_chains
as
chains
class
TextToNum
(
chains
.
ChainletBase
):
def
__init__
(
self
,
replicator
: TextReplicator
=
chains.depends(TextReplicator,
retries
=
3
),
) ->
None
:
self
._replicator
=
replicator
async
def
run_remote
(
self
,
data
:
...
):
try
:
generated_text
=
await
self
._replicator.run_remote(data)
except
ValueError
:
...
# Handle error.
​
Stack filtering
The stack trace is intended to show the user implemented code in
run_remote
(and user implemented helper functions). Under the
hood, the calls from one Chainlet to another go through an HTTP
connection, managed by the Chains framework. And each Chainlet itself is
run as a FastAPI server with several layers of request handling code “above”.
In order to provide concise, readable stacks, all of this non-user code is
filtered out.
Was this page helpful?
Yes
No
Previous
Truss Integration
Integrate deployed Truss models with stubs
Next
On this page
Exception handling and retries
Stack filtering
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/development/chain/getting-started:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Developing a Chain
Overview
Concepts
Your first Chain
Architecture & Design
Local Development
Deploy
Invocation
Watch
Subclassing
Streaming
Binary IO
Error Handling
Truss Integration
Engine Builder Models
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Exporting metrics
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Developing a Chain
Your first Chain
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
This quickstart guide contains instructions for creating two Chains:
A simple CPU-only “hello world”-Chain.
A Chain that implements Phi-3 Mini and uses it to write poems.
​
Prerequisites
To use Chains, install a recent Truss version and ensure pydantic is v2:
Copy
Ask AI
pip
install
--upgrade
truss
'pydantic>=2.0.0'
Help for setting up a clean development environment
Truss requires python
>=3.8,<3.13
. To set up a fresh development environment,
you can use the following commands, creating a environment named
chains_env
using
pyenv
:
Copy
Ask AI
curl
https://pyenv.run
|
bash
echo
'export PYENV_ROOT="$HOME/.pyenv"'
>>
~/.bashrc
echo
'[[ -d $PYENV_ROOT/bin ]] && export PATH="$PYENV_ROOT/bin:$PATH"'
>>
~/.bashrc
echo
'eval "$(pyenv init -)"'
>>
~/.bashrc
source
~/.bashrc
pyenv
install
3.11.0
ENV_NAME
=
"chains_env"
pyenv
virtualenv
3.11.0
$ENV_NAME
pyenv
activate
$ENV_NAME
pip
install
--upgrade
truss
'pydantic>=2.0.0'
To deploy Chains remotely, you also need a
Baseten account
.
It is handy to export your API key to the current shell session or permanently in your
.bashrc
:
~/.bashrc
Copy
Ask AI
export
BASETEN_API_KEY
=
"nPh8..."
​
Example: Hello World
Chains are written in Python files. In your working directory,
create
hello_chain/hello.py
:
Copy
Ask AI
mkdir
hello_chain
cd
hello_chain
touch
hello.py
In the file, we’ll specify a basic Chain. It has two Chainlets:
HelloWorld
, the entrypoint, which handles the input and output.
RandInt
, which generates a random integer. It is used a as a dependency
by
HelloWorld
.
Via the entrypoint, the Chain takes a maximum value and returns the string ”
Hello World!” repeated a
variable number of times.
hello.py
Copy
Ask AI
import
random
import
truss_chains
as
chains
class
RandInt
(
chains
.
ChainletBase
):
async
def
run_remote
(
self
,
max_value
:
int
) ->
int
:
return
random.randint(
1
, max_value)
@chains.mark_entrypoint
class
HelloWorld
(
chains
.
ChainletBase
):
def
__init__
(
self
,
rand_int
=
chains.depends(RandInt,
retries
=
3
)) ->
None
:
self
._rand_int
=
rand_int
async
def
run_remote
(
self
,
max_value
:
int
) ->
str
:
num_repetitions
=
await
self
._rand_int.run_remote(max_value)
return
"Hello World! "
*
num_repetitions
​
The Chainlet class-contract
Exactly one Chainlet must be marked as the entrypoint with
the
@chains.mark_entrypoint
decorator. This Chainlet is responsible for
handling public-facing input and output for the whole Chain in response to an
API call.
A Chainlet class has a single public method,
run_remote()
, which is
the API
endpoint for the entrypoint Chainlet and the function that other Chainlets can
use as a dependency. The
run_remote()
method must be fully type-annotated
with
primitive python
types
or
pydantic models
.
Chainlets cannot be
naively
instantiated. The only correct usages are:
Make one Chainlet depend on another one via the
chains.depends()
directive
as an
__init__
-argument as shown above for the
RandInt
Chainlet.
In the
local debugging mode
.
Beyond that, you can structure your code as you like, with private methods,
imports from other files, and so forth.
Keep in mind that Chainlets are intended for distributed, replicated, remote
execution, so using global variables, global state, and certain Python
features like importing modules dynamically at runtime should be avoided as
they may not work as intended.
​
Deploy your Chain to Baseten
To deploy your Chain to Baseten, run:
Copy
Ask AI
truss
chains
push
hello.py
The deploy command results in an output like this:
Copy
Ask AI
⛓️   HelloWorld - Chainlets  ⛓️
╭──────────────────────┬─────────────────────────┬─────────────╮
│ Status               │ Name                    │ Logs URL    │
├──────────────────────┼─────────────────────────┼─────────────┤
│  💚 ACTIVE           │ HelloWorld (entrypoint) │ https://... │
├──────────────────────┼─────────────────────────┼─────────────┤
│  💚 ACTIVE           │ RandInt (dep)           │ https://... │
╰──────────────────────┴─────────────────────────┴─────────────╯
Deployment succeeded.
You can run the chain with:
curl -X POST 'https://chain-.../run_remote' \
-H "Authorization: Api-Key $BASETEN_API_KEY" \
-d '<JSON_INPUT>'
Wait for the status to turn to
ACTIVE
and test invoking your Chain (replace
$INVOCATION_URL
in below command):
Copy
Ask AI
curl
-X
POST
$INVOCATION_URL
\
-H
"Authorization: Api-Key
$BASETEN_API_KEY
"
\
-d
'{"max_value": 10}'
# "Hello World! Hello World! Hello World! "
​
Example: Poetry with LLMs
Our second example also has two Chainlets, but is somewhat more complex and
realistic. The Chainlets are:
PoemGenerator
, the entrypoint, which handles the input and output and
orchestrates calls to the LLM.
PhiLLM
, which runs inference on Phi-3 Mini.
This Chain takes a list of words and returns a poem about each word, written by
Phi-3. Here’s the architecture:
We build this Chain in a new working directory (if you are still inside
hello_chain/
, go up one level with
cd ..
first):
Copy
Ask AI
mkdir
poetry_chain
cd
poetry_chain
touch
poems.py
A similar ent-to-end code example, using Mistral as an LLM, is available in
the
examples
repo
.
​
Building the LLM Chainlet
The main difference between this Chain and the previous one is that we now have
an LLM that needs a GPU and more complex dependencies.
Copy the following code into
poems.py
:
poems.py
Copy
Ask AI
import
asyncio
from
typing
import
List
import
pydantic
import
truss_chains
as
chains
from
truss
import
truss_config
PHI_HF_MODEL
=
"microsoft/Phi-3-mini-4k-instruct"
# This configures to cache model weights from the hunggingface repo
# in the docker image that is used for deploying the Chainlet.
PHI_CACHE
=
truss_config.ModelRepo(
repo_id
=
PHI_HF_MODEL
,
allow_patterns
=
[
"*.json"
,
"*.safetensors"
,
".model"
]
)
class
Messages
(
pydantic
.
BaseModel
):
messages: List[dict[
str
,
str
]]
class
PhiLLM
(
chains
.
ChainletBase
):
# `remote_config` defines the resources required for this chainlet.
remote_config
=
chains.RemoteConfig(
docker_image
=
chains.DockerImage(
# The phi model needs some extra python packages.
pip_requirements
=
[
"accelerate==0.30.1"
,
"einops==0.8.0"
,
"transformers==4.41.2"
,
"torch==2.3.0"
,
]
),
# The phi model needs a GPU and more CPUs.
compute
=
chains.Compute(
cpu_count
=
2
,
gpu
=
"T4"
),
# Cache the model weights in the image
assets
=
chains.Assets(
cached
=
[
PHI_CACHE
]),
)
def
__init__
(
self
) ->
None
:
# Note the imports of the *specific* python requirements are
# pushed down to here. This code will only be executed on the
# remotely deployed Chainlet, not in the local environment,
# so we don't need to install these packages in the local
# dev environment.
import
torch
import
transformers
self
._model
=
transformers.AutoModelForCausalLM.from_pretrained(
PHI_HF_MODEL
,
torch_dtype
=
torch.float16,
device_map
=
"auto"
,
)
self
._tokenizer
=
transformers.AutoTokenizer.from_pretrained(
PHI_HF_MODEL
,
)
self
._generate_args
=
{
"max_new_tokens"
:
512
,
"temperature"
:
1.0
,
"top_p"
:
0.95
,
"top_k"
:
50
,
"repetition_penalty"
:
1.0
,
"no_repeat_ngram_size"
:
0
,
"use_cache"
:
True
,
"do_sample"
:
True
,
"eos_token_id"
:
self
._tokenizer.eos_token_id,
"pad_token_id"
:
self
._tokenizer.pad_token_id,
}
async
def
run_remote
(
self
,
messages
: Messages) ->
str
:
import
torch
model_inputs
=
self
._tokenizer.apply_chat_template(
messages,
tokenize
=
False
,
add_generation_prompt
=
True
)
inputs
=
self
._tokenizer(model_inputs,
return_tensors
=
"pt"
)
input_ids
=
inputs[
"input_ids"
].to(
"cuda"
)
with
torch.no_grad():
outputs
=
self
._model.generate(
input_ids
=
input_ids,
**
self
._generate_args)
output_text
=
self
._tokenizer.decode(
outputs[
0
],
skip_special_tokens
=
True
)
return
output_text
​
Building the entrypoint
Now that we have an LLM, we can use it in a poem generator Chainlet. Add the
following code to
poems.py
:
poems.py
Copy
Ask AI
import
asyncio
@chains.mark_entrypoint
class
PoemGenerator
(
chains
.
ChainletBase
):
def
__init__
(
self
,
phi_llm
: PhiLLM
=
chains.depends(PhiLLM)) ->
None
:
self
._phi_llm
=
phi_llm
async
def
run_remote
(
self
,
words
: list[
str
]) -> list[
str
]:
tasks
=
[]
for
word
in
words:
messages
=
Messages(
messages
=
[
{
"role"
:
"system"
,
"content"
: (
"You are poet who writes short, "
"lighthearted, amusing poetry."
),
},
{
"role"
:
"user"
,
"content"
:
f
"Write a poem about
{
word
}
"
},
]
)
tasks.append(
asyncio.ensure_future(
self
._phi_llm.run_remote(messages)))
await
asyncio.sleep(
0
)
# Yield to event loop, to allow starting tasks.
return
list
(
await
asyncio.gather(
*
tasks))
Note that we use
asyncio.ensure_future
around each RPC to the LLM chainlet.
This makes the current python process start these remote calls concurrently,
i.e. the next call is started before the previous one has finished and we can
minimize our overall runtime. In order to await the results of all calls,
asyncio.gather
is used which gives us back normal python objects.
If the LLM is hit with many concurrent requests, it can auto-scale up (if
autoscaling is configure). More advanced LLM models have batching capabilities,
so for those even a single instance can serve concurrent request.
​
Deploy your Chain to Baseten
To deploy your Chain to Baseten, run:
Copy
Ask AI
truss
chains
push
poems.py
Wait for the status to turn to
ACTIVE
and test invoking your Chain (replace
$INVOCATION_URL
in below command):
Copy
Ask AI
curl
-X
POST
$INVOCATION_URL
\
-H
"Authorization: Api-Key
$BASETEN_API_KEY
"
\
-d
'{"words": ["bird", "plane", "superman"]}'
#[[
#"<s> [INST] Generate a poem about: bird [/INST] In the quiet hush of...</s>",
#"<s> [INST] Generate a poem about: plane [/INST] In the vast, boundless...</s>",
#"<s> [INST] Generate a poem about: superman [/INST] In the realm where...</s>"
#]]
Was this page helpful?
Yes
No
Previous
Architecture & Design
How to structure your Chainlets
Next
On this page
Prerequisites
Example: Hello World
The Chainlet class-contract
Deploy your Chain to Baseten
Example: Poetry with LLMs
Building the LLM Chainlet
Building the entrypoint
Deploy your Chain to Baseten
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/development/chain/invocation:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Developing a Chain
Overview
Concepts
Your first Chain
Architecture & Design
Local Development
Deploy
Invocation
Watch
Subclassing
Streaming
Binary IO
Error Handling
Truss Integration
Engine Builder Models
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Exporting metrics
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Developing a Chain
Invocation
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
Once your Chain is deployed, you can call it via its API endpoint. Chains use
the same inference API as models:
Environment endpoint
Development endpoint
Endpoint by ID
Here’s an example which calls the development deployment:
call_chain.py
Copy
Ask AI
import
requests
import
os
# From the Chain overview page on Baseten
# E.g. "https://chain-<CHAIN_ID>.api.baseten.co/development/run_remote"
CHAIN_URL
=
""
baseten_api_key
=
os.environ[
"BASETEN_API_KEY"
]
# JSON keys and types match the `run_remote` method signature.
data
=
{
...
}
resp
=
requests.post(
CHAIN_URL
,
headers
=
{
"Authorization"
:
f
"Api-Key
{
baseten_api_key
}
"
},
json
=
data,
)
print
(resp.json())
​
How to pass chain input
The data schema of the inference request corresponds to the function
signature of
run_remote()
in your entrypoint Chainlet.
For example, for the Hello Chain,
HelloAll.run_remote()
:
Copy
Ask AI
async
def
run_remote
(
self
,
names
: list[
str
]) ->
str
:
You’d pass the following JSON payload:
Copy
Ask AI
{
"names"
: [
"Marius"
,
"Sid"
,
"Bola"
] }
I.e. the keys in the JSON record, match the argument names and values
match the types of
run_remote.
​
Async chain inference
Like Truss models, Chains support async invocation. The
guide for
models
applies largely - in particular for how to wrap the
input and set up the webhook to process results.
The following additional points are chains specific:
Use chain-based URLS:
https://chain-{chain}.api.baseten.co/production/async_run_remote
https://chain-{chain}.api.baseten.co/development/async_run_remote
https://chain-{chain}.api.baseten.co/deployment/{deployment}/async_run_remote
.
https://chain-{chain}.api.baseten.co/environments/{env_name}/async_run_remote
.
Only the entrypoint is invoked asynchronously. Internal Chainlet-Chainlet
calls run synchronously.
Was this page helpful?
Yes
No
Previous
Watch
Live-patch deployed code
Next
On this page
How to pass chain input
Async chain inference
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/development/chain/localdev:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Developing a Chain
Overview
Concepts
Your first Chain
Architecture & Design
Local Development
Deploy
Invocation
Watch
Subclassing
Streaming
Binary IO
Error Handling
Truss Integration
Engine Builder Models
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Exporting metrics
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Developing a Chain
Local Development
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
Chains are designed for production in replicated remote deployments. But
alongside that production-ready power, we offer great local development and
deployment experiences.
The 6 principles behind Chains
Chains exists to help you build multi-step, multi-model pipelines. The
abstractions that Chains introduces are based on six opinionated principles:
three for architecture and three for developer experience.
Architecture principles
1
Atomic components
Each step in the pipeline can set its own hardware requirements and
software dependencies, separating GPU and CPU workloads.
2
Modular scaling
Each component has independent autoscaling parameters for targeted
resource allocation, removing bottlenecks from your pipelines.
3
Maximum composability
Components specify a single public interface for flexible-but-safe
composition and are reusable between projects
Developer experience principles
4
Type safety and validation
Eliminate entire taxonomies of bugs by writing typed Python code and
validating inputs, outputs, module initializations, function signatures,
and even remote server configurations.
5
Local debugging
Seamless local testing and cloud deployments: test Chains locally with
support for mocking the output of any step and simplify your cloud
deployment loops by separating large model deployments from quick
updates to glue code.
6
Incremental adoption
Use Chains to orchestrate existing model deployments, like pre-packaged
models from Baseten’s model library, alongside new model pipelines built
entirely within Chains.
Locally, a Chain is just Python files in a source tree. While that gives you a
lot of flexibility in how you structure your code, there are some constraints
and rules to follow to ensure successful distributed, remote execution in
production.
The best thing you can do while developing locally with Chains is to run your
code frequently, even if you do not have a
__main__
section: the Chains
framework runs various validations at
module initialization
to help
you catch issues early.
Additionally, running
mypy
and fixing reported type errors can help you
find problems early in a rapid feedback loop, before attempting a (much
slower) deployment.
Complementary to the purely local development Chains also has a “watch” mode,
like Truss, see the
watch guide
.
​
Test a Chain locally
Let’s revisit our “Hello World” Chain:
hello_chain/hello.py
Copy
Ask AI
import
asyncio
import
truss_chains
as
chains
# This Chainlet does the work
class
SayHello
(
chains
.
ChainletBase
):
async
def
run_remote
(
self
,
name
:
str
) ->
str
:
return
f
"Hello,
{
name
}
"
# This Chainlet orchestrates the work
@chains.mark_entrypoint
class
HelloAll
(
chains
.
ChainletBase
):
def
__init__
(
self
,
say_hello_chainlet
=
chains.depends(SayHello)) ->
None
:
self
._say_hello
=
say_hello_chainlet
async
def
run_remote
(
self
,
names
: list[
str
]) ->
str
:
tasks
=
[]
for
name
in
names:
tasks.append(asyncio.ensure_future(
self
._say_hello.run_remote(name)))
return
"
\n
"
.join(
await
asyncio.gather(
*
tasks))
# Test the Chain locally
if
__name__
==
"__main__"
:
with
chains.run_local():
hello_chain
=
HelloAll()
result
=
asyncio.get_event_loop().run_until_complete(
hello_chain.run_remote([
"Marius"
,
"Sid"
,
"Bola"
]))
print
(result)
When the
__main__()
module is run, local instances of the Chainlets are
created, allowing you to test functionality of your chain just by executing the
Python file:
Copy
Ask AI
cd
hello_chain
python
hello.py
# Hello, Marius
# Hello, Sid
# Hello, Bola
​
Mock execution of GPU Chainlets
Using
run_local()
to run your code locally requires that your development
environment have the compute resources and dependencies that each Chainlet
needs. But that often isn’t possible when building with AI models.
Chains offers a workaround, mocking, to let you test the coordination and
business logic of your multi-step inference pipeline without worrying about
running the model locally.
The second example in the
getting started guide
implements a Truss Chain for generating poems with Phi-3.
This Chain has two Chainlets:
The
PhiLLM
Chainlet, which requires an NVIDIA A10G GPU.
The
PoemGenerator
Chainlet, which easily runs on a CPU.
If you have an NVIDIA T4 under your desk, good for you. For the rest of us, we
can mock the
PhiLLM
Chainlet that is infeasible to run locally so that we can
quickly test the
PoemGenerator
Chainlet.
To do this, we define a mock Phi-3 model in our
__main__
module and give it
a
run_remote()
method that
produces a test output that matches the output type we expect from the real
Chainlet. Then, we inject an instance of this mock Chainlet into our Chain:
poems.py
Copy
Ask AI
if
__name__
==
"__main__"
:
class
FakePhiLLM
:
async
def
run_remote
(
self
,
prompt
:
str
) ->
str
:
return
f
"Here's a poem about
{
prompt.split(
" "
)[
-
1
]
}
"
with
chains.run_local():
poem_generator
=
PoemGenerator(
phi_llm
=
FakePhiLLM())
result
=
asyncio.get_event_loop().run_until_complete(
poem_generator.run_remote(
words
=
[
"bird"
,
"plane"
,
"superman"
]))
print
(result)
And run your Python file:
Copy
Ask AI
python
poems.py
# ['Here's a poem about bird', 'Here's a poem about plane', 'Here's a poem about superman']
​
Typing of mocks
You may notice that the argument
phi_llm
expects a type
PhiLLM
, while we
pass an instance of
FakePhiLLM
. These aren’t the same, which is formally a
type error.
However, this works at runtime because we constructed
FakePhiLLM
to
implement the same
protocol
as the real thing. We can make this explicit by
defining a
Protocol
as a type annotation:
Copy
Ask AI
from
typing
import
Protocol
class
PhiProtocol
(
Protocol
):
def
run_remote
(
self
,
data
:
str
) ->
str
:
...
and changing the argument type in
PoemGenerator
:
Copy
Ask AI
@chains.mark_entrypoint
class
PoemGenerator
(
chains
.
ChainletBase
):
def
__init__
(
self
,
phi_llm
: PhiProtocol
=
chains.depends(PhiLLM)) ->
None
:
self
._phi_llm
=
phi_llm
This is a bit more work and not needed to execute the code, but it shows how
typing consistency can be achieved - if desired.
Was this page helpful?
Yes
No
Previous
Deploy
Deploy your Chain on Baseten
Next
On this page
Test a Chain locally
Mock execution of GPU Chainlets
Typing of mocks
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/development/chain/overview:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Developing a Chain
Overview
Concepts
Your first Chain
Architecture & Design
Local Development
Deploy
Invocation
Watch
Subclassing
Streaming
Binary IO
Error Handling
Truss Integration
Engine Builder Models
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Exporting metrics
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Developing a Chain
Overview
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
Chains is a framework for building robust, performant multi-step and multi-model
inference pipelines and deploying them to production. It addresses the common
challenges of managing latency, cost and dependencies for complex workflows,
while leveraging Truss’ existing battle-tested performance, reliability and
developer toolkit.
​
User Guides
Guides focus on specific features and use cases. Also refer to
getting started
and
general concepts
.
Design
How to structure your Chainlets, concurrency, file structure
Local Dev
Iterating, Debugging, Testing, Mocking
Deploy
Deploy your Chain on Baseten
Invocation
Call your deployed Chain
Watch
Live-patch deployed code
Subclassing
Modularize and re-use Chainlet implementations
Streaming
Streaming outputs, reducing latency, SSEs
Binary IO
Performant serialization of numeric data
Error Propagation
Understanding and handling Chains errors
Truss Integration
Integrate deployed Truss models with stubs
​
From model to system
Some models are actually pipelines (e.g. invoking a LLM involves sequentially
tokenizing the input, predicting the next token, and then decoding the predicted
tokens). These pipelines generally make sense to bundle together in a monolithic
deployment because they have the same dependencies, require the same compute
resources, and have a robust ecosystem of tooling to improve efficiency and
performance in a single deployment.
Many other pipelines and systems do not share these properties. Some examples
include:
Running multiple different models in sequence.
Chunking/partitioning a set of files and concatenating/organizing results.
Pulling inputs from or saving outputs to a database or vector store.
Each step in these workflows has different hardware requirements, software
dependencies, and scaling needs so it doesn’t make sense to bundle them in a
monolithic deployment. That’s where Chains comes in!
​
Six principles behind Chains
Chains exists to help you build multi-step, multi-model pipelines. The
abstractions that Chains introduces are based on six opinionated principles:
three for architecture and three for developer experience.
Architecture principles
1
Atomic components
Each step in the pipeline can set its own hardware requirements and
software dependencies, separating GPU and CPU workloads.
2
Modular scaling
Each component has independent autoscaling parameters for targeted
resource allocation, removing bottlenecks from your pipelines.
3
Maximum composability
Components specify a single public interface for flexible-but-safe
composition and are reusable between projects
Developer experience principles
4
Type safety and validation
Eliminate entire taxonomies of bugs by writing typed Python code and
validating inputs, outputs, module initializations, function signatures,
and even remote server configurations.
5
Local debugging
Seamless local testing and cloud deployments: test Chains locally with
support for mocking the output of any step and simplify your cloud
deployment loops by separating large model deployments from quick
updates to glue code.
6
Incremental adoption
Use Chains to orchestrate existing model deployments, like pre-packaged
models from Baseten’s model library, alongside new model pipelines built
entirely within Chains.
​
Hello World with Chains
Here’s a simple Chain that says “hello” to each person in a list of provided
names:
hello_chain/hello.py
Copy
Ask AI
import
asyncio
import
truss_chains
as
chains
# This Chainlet does the work.
class
SayHello
(
chains
.
ChainletBase
):
async
def
run_remote
(
self
,
name
:
str
) ->
str
:
return
f
"Hello,
{
name
}
"
# This Chainlet orchestrates the work.
@chains.mark_entrypoint
class
HelloAll
(
chains
.
ChainletBase
):
def
__init__
(
self
,
say_hello_chainlet
=
chains.depends(SayHello)) ->
None
:
self
._say_hello
=
say_hello_chainlet
async
def
run_remote
(
self
,
names
: list[
str
]) ->
str
:
tasks
=
[]
for
name
in
names:
tasks.append(asyncio.ensure_future(
self
._say_hello.run_remote(name)))
return
"
\n
"
.join(
await
asyncio.gather(
*
tasks))
This is a toy example, but it shows how Chains can be used to separate
preprocessing steps like chunking from workload execution steps. If SayHello
were an LLM instead of a simple string template, we could do a much more complex
action for each person on the list.
​
What to build with Chains
RAG: retrieval-augmented generation
Connect to a vector databases and augment LLM results with additional
context information without introducing overhead to the model inference
step.
Try it yourself:
RAG Chain
.
Chunked Audio Transcription and high-throughput pipelines
Transcribe large audio files by splitting them into smaller chunks and
processing them in parallel — we’ve used this approach to process 10-hour
files in minutes.
Try it yourself:
Audio Transcription Chain
.
Efficient multi-model pipelines
Build powerful experiences wit optimal scaling in each step like:
AI phone calling (transcription + LLM + speech synthesis)
Multi-step image generation (SDXL + LoRAs + ControlNets)
Multimodal chat (LLM + vision + document parsing + audio)
Since each stage runs on its hardware with independent auto-scaling,
you chan achieve better hardware utilization and save costs.
Get started by
building and deploying your first chain
.
Was this page helpful?
Yes
No
Previous
Concepts
Glossary of Chains concepts and terminology
Next
On this page
User Guides
From model to system
Six principles behind Chains
Hello World with Chains
What to build with Chains
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/development/chain/streaming:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Developing a Chain
Overview
Concepts
Your first Chain
Architecture & Design
Local Development
Deploy
Invocation
Watch
Subclassing
Streaming
Binary IO
Error Handling
Truss Integration
Engine Builder Models
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Exporting metrics
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Developing a Chain
Streaming
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
Streaming outputs is useful for returning partial results to the client, before
all data has been processed.
For example LLM text generation happens in incremental text chunks, so the
beginning of the reply can already be sent to the client before the whole
prediction is complete.
Similarly, transcribing audio to text happens in ~30 second chunks and the
first ones can be returned before all completed.
In general, this does not reduce the overall processing time (still the same
amount of work must be done), but the initial latency to get some response
can be reduced significantly.
In some cases it might even reduce overall time, when streaming results
internally in a Chain, allows to start subsequent processing steps sooner -
i.e. pipelining the operations in a more efficient way.
​
Low-level streaming
Low-level, streaming works by sending byte chunks (unicode strings will be
implicitly encoded) via HTTP. The most primitive way of doing this in Chains
is by implementing
run_remote
as a bytes- or string-iterator, e.g.:
Copy
Ask AI
from
typing
import
AsyncIterator
import
truss_chains
as
chains
class
Streamlet
(
chains
.
ChainletBase
):
async
def
run_remote
(
self
,
inputs
:
...
) -> AsyncIterator[
str
]:
async
for
text_chunk
in
make_incremental_outputs(inputs):
yield
text_chunk
You are free to chose what data to represent in the byte/string chunks, it
could be raw text generated by an LLM, it could be JSON string, bytes or
anything else.
​
Server-sent events (SSEs)
A possible choice is to generate chunks that comply with the
specification
of server-sent events.
Concretely, sending JSON strings with
data
,
event
and potentially
other fields and content-type
text/event-stream
.
However, the SSE specification is not opinionated regarding what exactly is
encoded in
data
and what
event
-types exist - you have to make up your schema
that is useful for the client that consumes the data.
​
Pydantic and Chainlet-Chainlet-streams
While above low-level streaming is stable, the following helper APIs for typed
streaming are only stable for intra-Chain streaming.
If you want to use them for end clients, please reach out to Baseten support,
so we can discuss the stable solutions.
Unlike above “raw” stream example, Chains takes the general opinion that
input and output types should be definite, so that divergence and type
errors can be avoided.
Just like you type-annotate Chainlet inputs and outputs in the non-streaming
case, and use pydantic to manage more complex data structures, we built
tooling to bring the same benefits to streaming.
​
Headers and footers
This also helps to solve another challenge of streaming: you might want to
sent data of different kinds at the beginning or end of a stream than in
the “main” part.
For example if you transcribe an audio file, you might want
to send many transcription segments in a stream and at the end send some
aggregate information such as duration, detected languages etc.
We model typed streaming like this:
[optionally] send a chunk that conforms to the schema of a
Header
pydantic
model.
Send 0 to N chunks each conforming to the schema of an
Item
pydantic
model.
[optionally] send a chunk that conforms to the schema of a
Footer
pydantic
model.
​
APIs
​
StreamTypes
To have a single source of truth for the types that can be shared between
the producing Chainlet and the consuming client (either a Chainlet in the
Chain or an external client), the chains framework uses a
StreamType
-object:
Copy
Ask AI
import
pydantic
from
truss_chains
import
streaming
class
MyDataChunk
(
pydantic
.
BaseModel
):
words: list[
str
]
STREAM_TYPES
=
streaming.stream_types(
MyDataChunk,
header_type
=
...
,
footer_type
=
...
)
Note that header and footer types are optional and can be left out:
Copy
Ask AI
STREAM_TYPES
=
streaming.stream_types(MyDataChunk)
​
StreamWriter
Use the
STREAM_TYPES
to create a matching stream writer:
Copy
Ask AI
from
typing
import
AsyncIterator
import
pydantic
import
truss_chains
as
chains
from
truss_chains
import
streaming
class
MyDataChunk
(
pydantic
.
BaseModel
):
words: list[
str
]
STREAM_TYPES
=
streaming.stream_types(MyDataChunk)
class
Streamlet
(
chains
.
ChainletBase
):
async
def
run_remote
(
self
,
inputs
:
...
) -> AsyncIterator[
bytes
]:
stream_writer
=
streaming.stream_writer(
STREAM_TYPES
)
async
for
item
in
make_pydantic_items(inputs):
yield
stream_writer.yield_item(item)
If your stream types have header or footer types, corresponding
yield_header
and
yield_footer
methods are available on the writer.
The writer serializes the pydantic data to
bytes
, so you can also
efficiently represent numeric data (see the
binary IO guide
).
​
StreamReader
To consume the stream on either another Chainlet or in the external client, an
matching
StreamReader
is created form your
StreamTypes
. Besides the
types, you connect the reader to the bytes generator that you obtain from the
remote invocation of the streaming Chainlet:
Copy
Ask AI
import
truss_chains
as
chains
from
truss_chains
import
streaming
class
Consumer
(
chains
.
ChainletBase
):
def
__init__
(
self
,
streamlet
=
chains.depends(Streamlet)):
self
._streamlet
=
streamlet
async
def
run_remote
(
self
,
data
:
...
):
byte_stream
=
self
._streamlet.run_remote(data)
reader
=
streaming.stream_reader(
STREAM_TYPES
, byte_stream)
chunks
=
[]
async
for
data
in
reader.read_items():
chunks.append(data)
If you use headers or footers, the reader has async
read_header
and
read_footer
methods.
Note that the stream can only be consumed once and you have to consume
header, items and footer in order.
The implementation of
StreamReader
only needs
pydantic
, no other Chains
dependencies. So you can take that implementation code in isolation and
integrated it in your client code.
Was this page helpful?
Yes
No
Previous
Binary IO
Performant serialization of numeric data
Next
On this page
Low-level streaming
Server-sent events (SSEs)
Pydantic and Chainlet-Chainlet-streams
Headers and footers
APIs
StreamTypes
StreamWriter
StreamReader
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/development/chain/stub:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Developing a Chain
Overview
Concepts
Your first Chain
Architecture & Design
Local Development
Deploy
Invocation
Watch
Subclassing
Streaming
Binary IO
Error Handling
Truss Integration
Engine Builder Models
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Exporting metrics
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Developing a Chain
Truss Integration
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
Chains can be combined with existing Truss models using Stubs.
A Stub acts as a substitute (client-side proxy) for a remotely deployed
dependency, either a Chainlet or a Truss model. The Stub performs the remote
invocations as if it were local by taking care of the transport layer,
authentication, data serialization and retries.
Stubs can be integrated into Chainlets by passing in a URL of the deployed
model. They also require
context
to be initialized
(for authentication).
Copy
Ask AI
import
truss_chains
as
chains
class
LLMClient
(
chains
.
StubBase
):
async
def
run_remote
(
self
,
prompt
:
str
) ->
str
:
# Call the deployed model
resp
=
await
self
.predict_async(
inputs
=
{
"messages"
: [{
"role"
:
"user"
,
"content"
: prompt}],
"stream"
:
False
})
# Return a string with the model output
return
resp[
"output"
]
LLM_URL
=
...
class
MyChainlet
(
chains
.
ChainletBase
):
def
__init__
(
self
,
context
: chains.DeploymentContext
=
chains.depends_context(),
):
self
._llm
=
LLMClient.from_url(
LLM_URL
, context)
There are various ways how you can make a call to the other deployment:
Input as JSON dict (like above) or pydantic model.
Automatic parsing of the response into an pydantic model using the
output_model
argument.
predict_async
(recommended) or
predict_async
.
Streaming responses using
predict_async_stream
which returns an async
bytes iterator.
Customized with
RPCOptions
.
See the
StubBase reference
for all APIS.
Was this page helpful?
Yes
No
Previous
Engine Builder Models
Engine Builder models are pre-trained models that are optimized for specific inference tasks.
Next
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/development/chain/subclassing:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Developing a Chain
Overview
Concepts
Your first Chain
Architecture & Design
Local Development
Deploy
Invocation
Watch
Subclassing
Streaming
Binary IO
Error Handling
Truss Integration
Engine Builder Models
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Exporting metrics
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Developing a Chain
Subclassing
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
Sometimes you want to write one “main” implementation of a complicated inference
task, but then re-use it for similar variations. For example:
Deploy it on different hardware and with different concurrency.
Replace a dependency (e.g. silence detection in audio files) with a
different implementation of that step - while keeping all other processing
the same.
Deploy the same inference flow, but exchange the model weights used. E.g. for
a large and small version of an LLM or different model weights fine-tuned to
domains.
Add an adapter to convert between a different input/output schema.
In all of those cases, you can create lightweight subclasses of your main
chainlet.
Below are some example code snippets - they can all be combined with each other!
​
Example base class
Copy
Ask AI
import
asyncio
import
truss_chains
as
chains
class
Preprocess2x
(
chains
.
ChainletBase
):
async
def
run_remote
(
self
,
number
:
int
) ->
int
:
return
2
*
number
class
MyBaseChainlet
(
chains
.
ChainletBase
):
remote_config
=
chains.RemoteConfig(
compute
=
chains.Compute(
cpu_count
=
1
,
memory
=
"100Mi"
),
options
=
chains.ChainletOptions(
enable_b10_tracing
=
True
),
)
def
__init__
(
self
,
preprocess
=
chains.depends(Preprocess2x)):
self
._preprocess
=
preprocess
async
def
run_remote
(
self
,
number
:
int
) ->
float
:
return
1.0
/
await
self
._preprocess.run_remote(number)
# Assert base behavior.
with
chains.run_local():
chainlet
=
MyBaseChainlet()
result
=
asyncio.get_event_loop().run_until_complete(chainlet.run_remote(
4
))
assert
result
==
1
/
(
4
*
2
)
​
Adapter for different I/O
The base class
MyBaseChainlet
works with integer inputs and returns floats. If
you want to reuse the computation, but provide an alternative interface (e.g.
for a different client with different request/response schema), you can create
a subclass which does the I/O conversion. The actual computation is delegated to
the base classes above.
Copy
Ask AI
class
ChainletStringIO
(
MyBaseChainlet
):
async
def
run_remote
(
self
,
number
:
str
) ->
str
:
return
str
(
await
super
().run_remote(
int
(number)))
# Assert new behavior.
with
chains.run_local():
chainlet_string_io
=
ChainletStringIO()
result
=
asyncio.get_event_loop().run_until_complete(
chainlet_string_io.run_remote(
"4"
))
assert
result
==
"0.125"
​
Chain with substituted dependency
The base class
MyBaseChainlet
uses preprocessing that doubles the input. If
you want to use a different variant of preprocessing - while keeping
MyBaseChainlet.run_remote
and everything else as is - you can define a shallow
subclass of
MyBaseChainlet
where you use a different dependency
Preprocess8x
, which multiplies by 8 instead of 2.
Copy
Ask AI
class
Preprocess8x
(
chains
.
ChainletBase
):
async
def
run_remote
(
self
,
number
:
int
) ->
int
:
return
8
*
number
class
Chainlet8xPreprocess
(
MyBaseChainlet
):
def
__init__
(
self
,
preprocess
=
chains.depends(Preprocess8x)):
super
().
__init__
(
preprocess
=
preprocess)
# Assert new behavior.
with
chains.run_local():
chainlet_8x_preprocess
=
Chainlet8xPreprocess()
result
=
asyncio.get_event_loop().run_until_complete(
chainlet_8x_preprocess.run_remote(
4
))
assert
result
==
1
/
(
4
*
8
)
​
Override remote config.
If you want to re-deploy a chain, but change some deployment options, e.g. run
on different hardware, you can create a subclass and override
remote_config
.
Copy
Ask AI
class
Chainlet16Core
(
MyBaseChainlet
):
remote_config
=
chains.RemoteConfig(
compute
=
chains.Compute(
cpu_count
=
16
,
memory
=
"100Mi"
),
options
=
chains.ChainletOptions(
enable_b10_tracing
=
True
),
)
Be aware that
remote_config
is a class variable. In the example above we
created a completely new
RemoteConfig
value, because changing fields
inplace
would also affect the base class.
If you want to share config between the base class and subclasses, you can
define them in additional variables e.g. for the image:
Copy
Ask AI
DOCKER_IMAGE
=
chains.DockerImage(
pip_requirements
=
[
...
],
...
)
class
MyBaseChainlet
(
chains
.
ChainletBase
):
remote_config
=
chains.RemoteConfig(
docker_image
=
DOCKER_IMAGE
,
...
)
class
Chainlet16Core
(
MyBaseChainlet
):
remote_config
=
chains.RemoteConfig(
docker_image
=
DOCKER_IMAGE
,
...
)
Was this page helpful?
Yes
No
Previous
Streaming
Streaming outputs, reducing latency, SSEs
Next
On this page
Example base class
Adapter for different I/O
Chain with substituted dependency
Override remote config.
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/development/chain/watch:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Developing a Chain
Overview
Concepts
Your first Chain
Architecture & Design
Local Development
Deploy
Invocation
Watch
Subclassing
Streaming
Binary IO
Error Handling
Truss Integration
Engine Builder Models
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Exporting metrics
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Developing a Chain
Watch
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
The
watch command
(
truss chains watch
) combines
the best of local development and full deployment.
watch
lets you run on an
exact copy of the production hardware and interface but gives you live code
patching that lets you test changes in seconds without creating a new
deployment.
To use
truss chains watch
:
Push a chain in development mode (i.e.
publish
and
promote
flags are
false).
Run the watch command
truss chains watch SOURCE
. You can also add the
watch
option to the
push
command and combine both to a single step.
Each time you edit a file and save the changes, the watcher patches the
remote deployments. Updating the deployments might take a moment, but is
generally
much
faster than creating a new deployment.
You can call the chain with test data via
cURL
or the playground dialogue
in the UI and observe the result and logs.
Iterate steps 3. and 4. until your chain behaves in the desired way.
​
Selective Watch
Some large ML models might have a slow cycle time to reload (e.g. if the
weights are huge). For this case, we provide a “selective” watch option. For
example if you chain has such a heavy model Chainlet and other Chainlets
that contain only business logic, you can iterate on those, while not patching
and reloading the heavy model Chainlet.
This feature is really useful for advanced use case, but must be used with
caution.
If you change the code of a Chainlet not watched, in particular I/O types,
you get an inconsistent deployment.
Add the Chainlet names you want to watch as a comma separated list:
Copy
Ask AI
truss
chains
watch
...
--experimental-chainlet-names=ChainletA,ChainletB
Was this page helpful?
Yes
No
Previous
Subclassing
Modularize and re-use Chainlet implementations
Next
On this page
Selective Watch
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/development/concepts:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Developing a Chain
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Exporting metrics
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Development
Concepts
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
Baseten provides two core development workflows:
developing a model with Truss
and orchestrating models with
Chains
. Both are building blocks for production-grade ML systems, but they solve different problems.
Developing a model with Truss
Package and deploy any AI/ML model as an API with Truss or a Custom Server.
Developing a Chain
Orchestrate multiple models and logic, enabling complex inference workflows.
​
Truss vs. Chains: When to use each
​
Developing a model with Truss
Truss
is the open-source package you use to turn any ML model into a production-ready API on Baseten - without needing to learn Docker or build custom infrastructure.
Use Truss when:
You’re deploying a single model.
Whether it’s a fine-tuned transformer, diffusion model, or traditional classifier, Truss helps you package it with code, configuration, and system requirements to deploy at scale.
You want flexibility across tools and frameworks.
Build with your preferred model frameworks (e.g.
PyTorch
,
transformers
,
diffusers
), inference engines (e.g.
TensorRT-LLM
,
SGLang
,
vLLM
), and serving technologies (like
Triton
) as well as
any package
installable via
pip
or
apt
.
You need control over how your model runs.
Define pre- and post-processing, batching, logging, and custom inference logic. Truss gives you full access to environment settings and dependencies, versioned and deployable.
You want to keep development local and reproducible.
Develop locally in a containerized environment that mirrors production, test confidently, and ship your model without surprises.
​
Orchestrating with Chains
Chains
are for building inference workflows that span multiple steps, models, or tools. You define a sequence of steps — like routing, transformation, or chaining outputs — and run them as a single unit.
Use Chains when:
You’re combining multiple models or tools.
For example, running a vector search + LLM pipeline, or combining OCR, classification, and validation steps.
You want visibility into intermediate steps.
Chains let you debug and monitor each part of the workflow, retry failed steps, and trace outputs with ease — something that’s much harder with a single model endpoint.
You’re using orchestration libraries like LangChain or LlamaIndex.
Chains integrate natively with these frameworks, while still allowing you to insert your own logic or wrap Truss models as steps.
Was this page helpful?
Yes
No
Previous
Overview
Use Baseten's OpenAI-compatible Model APIs for LLMs, including structured outputs and tool calling.
Next
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/development/model-apis/overview:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Overview
Rate Limits & Budgets
Developing a model
Developing a Chain
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Exporting metrics
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Model APIs
Using Model APIs on Baseten
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
Baseten provides
OpenAI-compatible API endpoints
for all available Model APIs. This means you can use standard OpenAI client libraries—no wrappers, no rewrites, no surprises. If your code already works with OpenAI, it’ll work with Baseten.
This guide walks you through getting started, making your first call, and using advanced features like structured outputs and tool calling.
​
Prerequisites
Before you begin, make sure you have:
A
Baseten account
An
API key
The
OpenAI client library
for your language of choice
​
Supported models
Baseten currently offers several high-performing open-source LLMs as
Models APIs
:
Deepseek R1 0528
(slug:
deepseek-ai/DeepSeek-R1-0528
)
Deepseek V3 0324
(slug:
deepseek-ai/DeepSeek-V3-0324
)
Llama 4 Maverick
(slug:
meta-llama/Llama-4-Maverick-17B-128E-Instruct
)
Llama 4 Scout
(slug:
meta-llama/Llama-4-Scout-17B-16E-Instruct
)
Qwen 3
🔜
Please update the
model
in the examples below to the slug of the model you’d like to test.
​
Make your first API call
Python
JavaScript
cURL
Copy
Ask AI
client
=
OpenAI(
base_url
=
"https://inference.baseten.co/v1"
,
api_key
=
os.environ.get(
"BASETEN_API_KEY"
)
)
response
=
client.chat.completions.create(
model
=
"deepseek-ai/DeepSeek-V3-0324"
,
messages
=
[
{
"role"
:
"system"
,
"content"
:
"You are a helpful assistant."
},
{
"role"
:
"user"
,
"content"
:
"Your question here"
}
]
)
print
(response.choices[
0
].message.content)
Copy
Ask AI
client
=
OpenAI(
base_url
=
"https://inference.baseten.co/v1"
,
api_key
=
os.environ.get(
"BASETEN_API_KEY"
)
)
response
=
client.chat.completions.create(
model
=
"deepseek-ai/DeepSeek-V3-0324"
,
messages
=
[
{
"role"
:
"system"
,
"content"
:
"You are a helpful assistant."
},
{
"role"
:
"user"
,
"content"
:
"Your question here"
}
]
)
print
(response.choices[
0
].message.content)
Copy
Ask AI
const
client
=
new
OpenAI
({
baseURL:
"https://inference.baseten.co/v1"
,
apiKey:
process
.
env
.
BASETEN_API_KEY
,
});
// Use the client
try
{
const
response
=
await
client
.
chat
.
completions
.
create
({
model:
"deepseek-ai/DeepSeek-V3-0324"
,
messages:
[
{
role:
"system"
,
content:
"You are a helpful assistant."
},
{
role:
"user"
,
content:
"Hello, how are you?"
},
],
})
Copy
Ask AI
curl
https://inference.baseten.co/v1/chat/completions
\
-H
"Content-Type: application/json"
\
-H
"Authorization: Api-Key
$BASETEN_API_KEY
"
\
-d
'{
"model": "deepseek-ai/DeepSeek-V3-0324",
"messages": [{ "role": "user", "content": "Your content here" }],
"stream": true,
"max_tokens": 10
}'
echo
# Add a newline for cleaner output
​
Request parameters
Model APIs support all commonly used
OpenAI ChatCompletions
parameters, including:
model
: Slug of the model you want to call (see below)
messages
: Array of message objects (
role
+
content
)
temperature
: Controls randomness (0-2, default 1)
max_tokens
: Maximum number of tokens to generate
stream
: Boolean to enable streaming responses
​
Structured outputs
To get structured JSON output from the model, you can use the
response_format
parameter. Set
response_format={"type": "json_object"}
to enable JSON mode. For more complex schemas, you can define a JSON schema.
Let’s say you want to extract specific information from a user’s query, like a name and an email address.
Python
JavaScript
cURL
Copy
Ask AI
client
=
OpenAI(
base_url
=
"https://inference.baseten.co/v1"
,
api_key
=
os.environ.get(
"BASETEN_API_KEY"
)
)
response
=
client.chat.completions.create(
model
=
"deepseek-ai/DeepSeek-V3-0324"
,
# Or any other supported model
messages
=
[
{
"role"
:
"system"
,
"content"
:
"You are an expert at extracting information."
},
{
"role"
:
"user"
,
"content"
:
"My name is Jane Doe and my email is jane.doe@example.com. I
\'
d like to know more about your services."
}
],
response_format
=
{
"type"
:
"json_object"
,
"json_schema"
: {
"name"
:
"user_details"
,
"description"
:
"User contact information"
,
"schema"
: {
"type"
:
"object"
,
"properties"
: {
"name"
: {
"type"
:
"string"
,
"description"
:
"The user
\'
s full name"
},
"email"
: {
"type"
:
"string"
,
"description"
:
"The user
\'
s email address"
}
},
"required"
: [
"name"
,
"email"
]
},
"strict"
:
True
# Enforce schema adherence
}
}
)
output
=
json.loads(response.choices[
0
].message.content)
print
(output)
# Expected output:
# {
#   "name": "Jane Doe",
#   "email": "jane.doe@example.com"
# }
Copy
Ask AI
client
=
OpenAI(
base_url
=
"https://inference.baseten.co/v1"
,
api_key
=
os.environ.get(
"BASETEN_API_KEY"
)
)
response
=
client.chat.completions.create(
model
=
"deepseek-ai/DeepSeek-V3-0324"
,
# Or any other supported model
messages
=
[
{
"role"
:
"system"
,
"content"
:
"You are an expert at extracting information."
},
{
"role"
:
"user"
,
"content"
:
"My name is Jane Doe and my email is jane.doe@example.com. I
\'
d like to know more about your services."
}
],
response_format
=
{
"type"
:
"json_object"
,
"json_schema"
: {
"name"
:
"user_details"
,
"description"
:
"User contact information"
,
"schema"
: {
"type"
:
"object"
,
"properties"
: {
"name"
: {
"type"
:
"string"
,
"description"
:
"The user
\'
s full name"
},
"email"
: {
"type"
:
"string"
,
"description"
:
"The user
\'
s email address"
}
},
"required"
: [
"name"
,
"email"
]
},
"strict"
:
True
# Enforce schema adherence
}
}
)
output
=
json.loads(response.choices[
0
].message.content)
print
(output)
# Expected output:
# {
#   "name": "Jane Doe",
#   "email": "jane.doe@example.com"
# }
Copy
Ask AI
const
client
=
new
OpenAI
({
baseURL:
"https://inference.baseten.co/v1"
,
apiKey:
process
.
env
.
BASETEN_API_KEY
,
});
// Use the client for structured output
try
{
const
response
=
await
client
.
chat
.
completions
.
create
({
model:
"deepseek-ai/DeepSeek-V3-0324"
,
messages:
[
{
role:
"system"
,
content:
"You are an expert at extracting information."
},
{
role:
"user"
,
content:
"My name is Jane Doe and my email is jane.doe@example.com. I'd like to know more about your services."
},
],
response_format:
{
type:
"json_object"
,
json_schema:
{
name:
"user_details"
,
description:
"User contact information"
,
schema:
{
type:
"object"
,
properties:
{
name:
{
type:
"string"
,
description:
"The user's full name"
},
email:
{
type:
"string"
,
description:
"The user's email address"
}
},
required:
[
"name"
,
"email"
]
},
strict:
true
}
}
})
Copy
Ask AI
curl
-s
"https://inference.baseten.co/v1/chat/completions"
\
-H
"Content-Type: application/json"
\
-H
"Authorization: Api-Key
$BASETEN_API_KEY
"
\
-d
@-
<<
EOF
{
"model": "deepseek-ai/DeepSeek-V3-0324",
"messages": [
{"role": "system", "content": "You are an expert at extracting information."},
{"role": "user", "content": "My name is Jane Doe and my email is jane.doe@example.com. I'd like to know more about your services."}
],
"response_format": {
"type": "json_object",
"json_schema": {
"name": "user_details",
"description": "User contact information",
"schema": {
"type": "object",
"properties": {
"name": { "type": "string", "description": "The user's full name" },
"email": { "type": "string", "description": "The user's email address" }
},
"required": ["name", "email"]
},
"strict": true
}
}
}
EOF
echo
# Add a newline for cleaner output
When
strict: true
is specified within the
json_schema
, the model is constrained to produce output that strictly adheres to the provided schema. If the model cannot or will not produce output that matches the schema, it may return an error or a refusal.
​
Tool calling
Model compatibility note:
We recommend using
Deepseek V3
for tool calling functionality. We do not recommend using
Deepseek R1
for tool calling as the model was not post-trained for tool calling.
Tool calling is fully supported. Simply define a list of tools and pass them via the
tools
parameter:
type
: The type of tool to call. Currently, the only supported value is
function
.
function
: A dictionary with the following keys:
name
: The name of the function to be called
description
: A description of what the function does
parameters
: A JSON Schema object describing the function parameters
Copy
Ask AI
# Example list of tools
{
"tools"
: [
{
"type"
:
"function"
,
"function"
: {
"name"
:
"get_weather"
,
"description"
:
"Get the weather for a location"
,
"parameters"
: {
"type"
:
"object"
,
"properties"
: {
"location"
: {
"type"
:
"string"
,
"description"
:
"City and state"
}
},
"required"
: [
"location"
]
}
}
}
]
}
Here’s how you might implement tool calling:
Python
JavaScript
cURL
Copy
Ask AI
client
=
OpenAI(
base_url
=
"https://inference.baseten.co/v1"
,
api_key
=
os.environ.get(
"BASETEN_API_KEY"
)
)
# Define the message and available tools
messages
=
[{
"role"
:
"user"
,
"content"
:
"What's the weather like in Boston?"
}]
tools
=
[
{
"type"
:
"function"
,
"function"
: {
"name"
:
"get_weather"
,
"description"
:
"Get the current weather"
,
"parameters"
: {
"type"
:
"object"
,
"properties"
: {
"location"
: {
"type"
:
"string"
,
"description"
:
"City name"
},
"unit"
: {
"type"
:
"string"
,
"enum"
: [
"celsius"
,
"fahrenheit"
]}
},
"required"
: [
"location"
]
}
}
}
]
# Make the initial request
response
=
client.chat.completions.create(
model
=
"deepseek-ai/DeepSeek-V3-0324"
,
messages
=
messages,
tools
=
tools,
tool_choice
=
"auto"
)
# Process tool calls if any
if
response.choices[
0
].message.tool_calls:
# Get the function call details
tool_call
=
response.choices[
0
].message.tool_calls[
0
]
function_args
=
json.loads(tool_call.function.arguments)
# Call the function and get the result
function_response
=
get_weather(
location
=
function_args.get(
"location"
))
# Add function response to conversation
messages.append(response.choices[
0
].message)
messages.append({
"tool_call_id"
: tool_call.id,
"role"
:
"tool"
,
"name"
: tool_call.function.name,
"content"
: function_response
})
# Get the final response with the function result
final_response
=
client.chat.completions.create(
model
=
"deepseek-ai/DeepSeek-V3-0324"
,
messages
=
messages
)
print
(final_response.choices[
0
].message.content)
Copy
Ask AI
client
=
OpenAI(
base_url
=
"https://inference.baseten.co/v1"
,
api_key
=
os.environ.get(
"BASETEN_API_KEY"
)
)
# Define the message and available tools
messages
=
[{
"role"
:
"user"
,
"content"
:
"What's the weather like in Boston?"
}]
tools
=
[
{
"type"
:
"function"
,
"function"
: {
"name"
:
"get_weather"
,
"description"
:
"Get the current weather"
,
"parameters"
: {
"type"
:
"object"
,
"properties"
: {
"location"
: {
"type"
:
"string"
,
"description"
:
"City name"
},
"unit"
: {
"type"
:
"string"
,
"enum"
: [
"celsius"
,
"fahrenheit"
]}
},
"required"
: [
"location"
]
}
}
}
]
# Make the initial request
response
=
client.chat.completions.create(
model
=
"deepseek-ai/DeepSeek-V3-0324"
,
messages
=
messages,
tools
=
tools,
tool_choice
=
"auto"
)
# Process tool calls if any
if
response.choices[
0
].message.tool_calls:
# Get the function call details
tool_call
=
response.choices[
0
].message.tool_calls[
0
]
function_args
=
json.loads(tool_call.function.arguments)
# Call the function and get the result
function_response
=
get_weather(
location
=
function_args.get(
"location"
))
# Add function response to conversation
messages.append(response.choices[
0
].message)
messages.append({
"tool_call_id"
: tool_call.id,
"role"
:
"tool"
,
"name"
: tool_call.function.name,
"content"
: function_response
})
# Get the final response with the function result
final_response
=
client.chat.completions.create(
model
=
"deepseek-ai/DeepSeek-V3-0324"
,
messages
=
messages
)
print
(final_response.choices[
0
].message.content)
Copy
Ask AI
const
client
=
new
OpenAI
({
baseURL:
"https://inference.baseten.co/v1"
,
apiKey:
process
.
env
.
BASETEN_API_KEY
,
});
// Make initial request with tools
const
response
=
await
client
.
chat
.
completions
.
create
({
model:
"deepseek-ai/DeepSeek-V3-0324"
,
messages:
[{
role:
"user"
,
content:
"What's the weather like in Boston?"
}],
tools:
[{
type:
"function"
,
function:
{
name:
"get_weather"
,
description:
"Get the current weather"
,
parameters:
{
type:
"object"
,
properties:
{
location:
{
type:
"string"
,
description:
"City name"
}
},
required:
[
"location"
]
}
}
}]
});
// Process tool calls if any
if
(
response
.
choices
[
0
].
message
.
tool_calls
) {
const
toolCall
=
response
.
choices
[
0
].
message
.
tool_calls
[
0
];
const
args
=
JSON
.
parse
(
toolCall
.
function
.
arguments
);
// Call function and get result
const
functionResponse
=
getWeather
(
args
.
location
);
// Submit function result back to model
const
messages
=
[
{
role:
"user"
,
content:
"What's the weather like in Boston?"
},
response
.
choices
[
0
].
message
,
{
tool_call_id:
toolCall
.
id
,
role:
"tool"
,
name:
"get_weather"
,
content:
functionResponse
}
];
const
finalResponse
=
await
client
.
chat
.
completions
.
create
({
model:
"deepseek-ai/DeepSeek-V3-0324"
,
messages:
messages
})
Copy
Ask AI
curl
-s
"https://inference.baseten.co/v1/chat/completions"
\
-H
"Content-Type: application/json"
\
-H
"Authorization: Api-Key
$BASETEN_API_KEY
"
\
-d
'{
"model": "deepseek-ai/DeepSeek-V3-0324",
"messages": [{"role": "user", "content": "What'
\'
's the weather like in Boston?"}],
"tools": [{
"type": "function",
"function": {
"name": "get_weather",
"description": "Get the current weather",
"parameters": {
"type": "object",
"properties": {
"location": {"type": "string", "description": "City name"}
},
"required": ["location"]
}
}
}]
}'
)
# Extract tool call details
TOOL_CALL_ID
=
$(
# If we have a tool call, prepare the weather data and send it back
if
[
-n
"
$TOOL_CALL_ID
"
];
then
WEATHER_DATA
=
'{"location":"Boston","temperature":"72","forecast":"sunny"}'
# Send the function result back to the API
FINAL_RESPONSE
=
$(
curl
-s
"https://inference.baseten.co/v1/chat/completions"
\
-H
"Content-Type: application/json"
\
-H
"Authorization: Api-Key
$BASETEN_API_KEY
"
\
-d
'{
"model": "deepseek-ai/DeepSeek-V3-0324",
"messages": [
{"role": "user", "content": "What'
\'
's the weather like in Boston?"},
{"role": "assistant", "content": null, "tool_calls": [{"id": "'
$TOOL_CALL_ID
'", "type": "function", "function": {"name": "get_weather", "arguments": "{\"location\":\"Boston\"}"}}]},
{"role": "tool", "tool_call_id": "'
$TOOL_CALL_ID
'", "name": "get_weather", "content": "'
$WEATHER_DATA
'"}
]
}'
)
# Print the final response
fi
​
Error Handling
The API returns standard HTTP error codes:
400
: Bad request (malformed input)
401
: Unauthorized (invalid or missing API key)
402
: Payment required
404
: Model not found
429
: Rate limit exceeded
500
: Internal server error
Check the response body for specific error details and suggested resolutions.
​
Migrating from OpenAI
To migrate from OpenAI to Baseten’s OpenAI-compatible API, you need to make these changes to your existing code:
Replace your OpenAI API key with your Baseten API key
Change the base URL to
https://inference.baseten.co/v1.
Update model names to match Baseten-supported slugs.
Was this page helpful?
Yes
No
Previous
Rate Limits & Budgets
Learn about rate limits for Baseten's Model APIs and how to set usage budgets to control costs.
Next
On this page
Prerequisites
Supported models
Make your first API call
Request parameters
Structured outputs
Tool calling
Error Handling
Migrating from OpenAI
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/development/model-apis/rate-limits-and-budgets:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Overview
Rate Limits & Budgets
Developing a model
Developing a Chain
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Exporting metrics
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Model APIs
Rate Limits & Budgets
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
​
Rate Limits
To ensure fair use and system stability, Baseten enforces two rate limits:
Request rate limits
— maximum number of API requests per minute
Token rate limits
— maximum number of tokens processed per minute (input + output combined)
Default limits vary based on your account status.
Account
RPM
TPM
Starter
(unverified)
15
100,000
Starter
(verified)
120
500,000
Business
120
1,000,000
Enterprise
Custom
Custom
If you exceed these limits, the API will return a
429 Too Many Requests
error. Request a higher rate limit by
contacting us
.
​
Requesting higher limits
If you have high volume, are a verified customer, and need more headroom, you can
contact us
to request a rate limit increase.
​
Setting budgets
Setting a budget allows you to control your Model API usage and avoid unexpected costs. Usage budgets apply only to Model APIs (not dedicated deployments). Your team will be notified by email at 75%, 90%, and 100% of budget.
​
Enforcing budgets
When setting a budget, you can choose to enforce it or not.
If you choose to enforce it
, requests will be rejected once the budget is reached.
If you choose not to enforce it
, you will be notified at 75%, 90%, and 100% of budget and you’ll be responsible for any costs incurred over the budget.
Was this page helpful?
Yes
No
Previous
Overview
This page introduces the key concepts and workflow you'll use to package, configure, and iterate on models using Baseten’s developer tooling.
Next
On this page
Rate Limits
Requesting higher limits
Setting budgets
Enforcing budgets
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/development/model/base-images:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Overview
Your first model
Setup and dependencies
Configuration
Custom build commands
Base Docker images
Private Docker Registries
Implementation
Implementation (Advanced)
Deploy and iterate
Performance optimization
Security and secrets
Data and storage
Python driven configuration for models 🆕
Developing a Chain
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Exporting metrics
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Setup and dependencies
Base Docker images
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
Truss uses containerized environments to ensure consistent model execution across deployments. While the default Truss image works for most cases, you may need a custom base image to meet specific package or system requirements.
​
Setting a base image in
config.yaml
Specify a custom base image in
config.yaml
:
config.yaml
Copy
Ask AI
base_image
:
image
:
<image_name:tag>
python_executable_path
:
<path-to-python>
image
: The Docker image to use.
python_executable_path
: The path to the Python binary inside the container.
​
Example: NVIDIA NeMo Model
Using a custom image to deploy
NVIDIA NeMo TitaNet
model:
config.yaml
Copy
Ask AI
base_image
:
image
:
nvcr.io/nvidia/nemo:23.03
python_executable_path
:
/usr/bin/python
apply_library_patches
:
true
requirements
:
-
PySoundFile
resources
:
accelerator
:
T4
cpu
:
2500m
memory
:
4512Mi
use_gpu
:
true
secrets
: {}
system_packages
:
-
python3.8-venv
​
Using Private Base Images
If your base image is private, ensure that you have configured your model to use a
private registry
​
Creating a custom base image
You can build a new base image using Truss’s base images as a foundation. Available images are listed on
Docker Hub
.
​
Example: Customizing a Truss Base Image
Dockerfile
Copy
Ask AI
FROM
baseten/truss-server-base:3.11-gpu-v0.7.16
RUN
pip uninstall cython -y
RUN
pip install cython==0.29.30
​
Building & Pushing Your Custom Image
Ensure Docker is installed and running. Then, build, tag, and push your image:
Copy
Ask AI
docker
build
-t
my-custom-base-image:0.1
.
docker
tag
my-custom-base-image:0.1
your-docker-username/my-custom-base-image:0.1
docker
push
your-docker-username/my-custom-base-image:0.1
Was this page helpful?
Yes
No
Previous
Private Docker Registries
A guide to configuring a private container registry for your truss
Next
On this page
Setting a base image inconfig.yaml
Example: NVIDIA NeMo Model
Using Private Base Images
Creating a custom base image
Example: Customizing a Truss Base Image
Building & Pushing Your Custom Image
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/development/model/build-commands:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Overview
Your first model
Setup and dependencies
Configuration
Custom build commands
Base Docker images
Private Docker Registries
Implementation
Implementation (Advanced)
Deploy and iterate
Performance optimization
Security and secrets
Data and storage
Python driven configuration for models 🆕
Developing a Chain
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Exporting metrics
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Setup and dependencies
Custom build commands
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
The
build_commands
feature allows you to
run custom Docker commands
during the
build stage
, enabling
advanced caching
,
dependency management
,
and environment setup
.
Use Cases:
Clone GitHub repositories
Install dependencies
Create directories
Pre-download model weights
​
1. Using Build Commands in
config.yaml
Add
build_commands
to your
config.yaml
:
Copy
Ask AI
build_commands
:
-
git clone https://github.com/comfyanonymous/ComfyUI.git
-
cd ComfyUI && git checkout b1fd26fe9e55163f780bf9e5f56bf9bf5f035c93 && pip install -r requirements.txt
model_name
:
Build Commands Demo
python_version
:
py310
resources
:
accelerator
:
A100
use_gpu
:
true
What happens?
The GitHub repository is cloned.
The specified commit is checked out.
Dependencies are installed.
Everything is cached at build time
, reducing deployment cold starts.
​
2. Creating Directories in Your Truss
Use
build_commands
to
create directories
directly in the container.
Copy
Ask AI
build_commands
:
-
git clone https://github.com/comfyanonymous/ComfyUI.git
-
cd ComfyUI && mkdir ipadapter
-
cd ComfyUI && mkdir instantid
Useful for
large codebases
requiring additional structure.
​
3. Caching Model Weights Efficiently
For large weights (10GB+), use
model_cache
or
external_data
.
For smaller weights,
use
wget
in
build_commands
:
Copy
Ask AI
build_commands
:
-
git clone https://github.com/comfyanonymous/ComfyUI.git
-
cd ComfyUI && pip install -r requirements.txt
-
cd ComfyUI/models/controlnet && wget -O control-lora-canny-rank256.safetensors https://huggingface.co/stabilityai/control-lora/resolve/main/control-LoRAs-rank256/control-lora-canny-rank256.safetensors
-
cd ComfyUI/models/controlnet && wget -O control-lora-depth-rank256.safetensors https://huggingface.co/stabilityai/control-lora/resolve/main/control-LoRAs-rank256/control-lora-depth-rank256.safetensors
model_name
:
Build Commands Demo
python_version
:
py310
resources
:
accelerator
:
A100
use_gpu
:
true
system_packages
:
-
wget
Why use this?
Reduces startup time
by
preloading model weights
during the build stage.
Ensures availability
without runtime downloads.
​
4. Running Any Shell Command
The
build_commands
feature lets you execute
any
shell command as if running it locally, with the benefit of
caching the results
at build time.
Key Benefits:
Reduces cold starts
by caching dependencies & data.
Ensures reproducibility
across deployments.
Optimizes environment setup
for fast execution.
Was this page helpful?
Yes
No
Previous
Base Docker images
A guide to configuring a base image for your truss
Next
On this page
1. Using Build Commands in config.yaml
2. Creating Directories in Your Truss
3. Caching Model Weights Efficiently
4. Running Any Shell Command
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/development/model/build-your-first-model:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Overview
Your first model
Setup and dependencies
Implementation
Implementation (Advanced)
Deploy and iterate
Performance optimization
Security and secrets
Data and storage
Python driven configuration for models 🆕
Developing a Chain
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Exporting metrics
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Developing a model
Your first model
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
This quickstart guide shows you how to build and deploy your first model,
using Baseten’s Truss framework.
​
Prerequisites
To use Truss, install a recent Truss version and ensure pydantic is v2:
Copy
Ask AI
pip
install
--upgrade
truss
'pydantic>=2.0.0'
Help for setting up a clean development environment
Truss requires python
>=3.8,<3.13
. To set up a fresh development environment,
you can use the following commands, creating a environment named
truss_env
using
pyenv
:
Copy
Ask AI
curl
https://pyenv.run
|
bash
echo
'export PYENV_ROOT="$HOME/.pyenv"'
>>
~/.bashrc
echo
'[[ -d $PYENV_ROOT/bin ]] && export PATH="$PYENV_ROOT/bin:$PATH"'
>>
~/.bashrc
echo
'eval "$(pyenv init -)"'
>>
~/.bashrc
source
~/.bashrc
pyenv
install
3.11.0
ENV_NAME
=
"truss_env"
pyenv
virtualenv
3.11.0
$ENV_NAME
pyenv
activate
$ENV_NAME
pip
install
--upgrade
truss
'pydantic>=2.0.0'
To deploy Truss remotely, you also need a
Baseten account
.
It is handy to export your API key to the current shell session or permanently in your
.bashrc
:
~/.bashrc
Copy
Ask AI
export
BASETEN_API_KEY
=
"nPh8..."
​
Initialize your model
Truss is a tool that helps you package your model code and configuration, and ship it to Baseten for deployment, testing, and scaling.
To create your first model, you can use the
truss init
command.
Copy
Ask AI
$
truss
init
hello-world
?
📦 Name this model: HelloWorld
Truss
HelloWorld
was
created
in
~/hello-world
This will create a new directory called
hello-world
with the following files:
config.yaml
- A configuration file for your model.
model/model.py
- A Python file that contains your model code
packages/
- A folder to hold any dependencies your model needs
data/
- A folder to hold any data your model needs
For this example, we’ll focus on the
config.yaml
file and the
model.py
file.
​
config.yaml
The
config.yaml
file is used to configure dependencies, resources, and
other settings for your model.
Let’s take a look at the contents:
config.yaml
Copy
Ask AI
build_commands
: []
environment_variables
: {}
external_package_dirs
: []
model_metadata
: {}
model_name
:
HelloWorld
python_version
:
py311
requirements
: []
resources
:
accelerator
:
null
cpu
:
'1'
memory
:
2Gi
use_gpu
:
false
secrets
: {}
system_packages
: []
Some key fields to note:
requirements
: This is a list of
pip
packages that will be installed when
your model is deployed.
resources
: This is where you can specify the resources your model will use.
secrets
: This is where you can specify any secrets your model will need, such as
HuggingFace API keys.
See the
Configuration
page for more information on the
config.yaml
file.
​
model.py
Next, let’s take a look at the
model.py
file.
Copy
Ask AI
class
Model
:
def
__init__
(
self
,
**
kwargs
):
pass
def
load
(
self
):
pass
def
predict
(
self
,
model_input
):
return
model_input
In Truss models, we expect users to provide a Python class with the following methods:
__init__
: This is the constructor.
load
: This is called at model startup, and should include any setup logic, such as weight downloading or initialization
predict
: This is the method that is called during inference.
​
Deploy your model
To deploy your model, you can use the
truss push
command.
Copy
Ask AI
$
truss
push
This will deploy your model to Baseten.
​
Invoke your model
After deploying your model, you can invoke it with the invocation URL provided:
Copy
Ask AI
$
curl
-X
POST
https://model-{model-id}.api.baseten.co/development/predict
\
-H
"Authorization: Api-Key
$BASETEN_API_KEY
"
\
-d
'"some text"'
"some text"
​
A Real Example
To show a slightly more complex example, let’s deploy a text classification model
from HuggingFace!
In this example, we’ll use the
transformers
library to load a pre-trained model,
from HuggingFace, and use it to classify the given text.
​
config.yaml
To deploy this model, we need to add a few more dependencies to our
config.yaml
file.
config.yaml
Copy
Ask AI
requirements
:
-
transformers
-
torch
​
model.py
Next, let’s change our
model.py
file to use the
transformers
library to load the model,
and then use it to predict the sentiment of a given text.
model.py
Copy
Ask AI
from
transformers
import
pipeline
class
Model
:
def
__init__
(
self
,
**
kwargs
):
pass
def
load
(
self
):
self
._model
=
pipeline(
"text-classification"
)
def
predict
(
self
,
model_input
):
return
self
._model(model_input)
​
Running inference
Similarly to our previous example, we can deploy this model using
truss push
Copy
Ask AI
$
truss
push
And then invoke it using the invocation URL on Baseten.
Copy
Ask AI
$
curl
-X
POST
https://model-{model-id}.api.baseten.co/development/predict
\
-H
"Authorization: Api-Key
$BASETEN_API_KEY
"
\
-d
'{"text": "some text"}'
​
Next steps
Now that you’ve deployed your first model, you can learn more about more
options for
configuring your model
,
and
implementing your model
.
Was this page helpful?
Yes
No
Previous
Configuration
How to configure your model.
Next
On this page
Prerequisites
Initialize your model
config.yaml
model.py
Deploy your model
Invoke your model
A Real Example
config.yaml
model.py
Running inference
Next steps
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/development/model/code-first-development:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Overview
Your first model
Setup and dependencies
Implementation
Implementation (Advanced)
Deploy and iterate
Performance optimization
Security and secrets
Data and storage
Python driven configuration for models 🆕
Developing a Chain
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Exporting metrics
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Developing a model
Python driven configuration for models 🆕
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
This feature is still in beta.
In addition to our normal YAML configuration, we support configuring your model using pure Python. This offers the following benefits:
Typed configuration via Python code
with IDE autocomplete, instead of a separate
yaml
configuration file
Simpler directory structure
that IDEs support for module resolution
In this guide, we go through deploying a simple Model using this new framework.
​
Step 1: Initializing your project
We leverage traditional
truss init
functionality with a new flag to create the directory structure:
Copy
Ask AI
truss
init
my-new-model
--python-config
​
Step 2: Write your model
To build a model with this new framework, we require two things:
A class that inherits from
baseten.ModelBase
, which will serve as the entrypoint when invoking
/predict
A
predict
method with type hints
That’s it! The following is a contrived example of a complete model that will keep a running total of user provided input:
my_model.py
Copy
Ask AI
import
truss_chains
as
baseten
class
RunningTotalCalculator
(
baseten
.
ModelBase
):
def
__init__
(
self
):
self
._running_total
=
0
async
def
predict
(
self
,
increment
:
int
) ->
int
:
self
._running_total
+=
increment
return
self
._running_total
​
Step 3: Deploy, patch, and public your model
In order to deploy the first version of your new model, you can run:
Copy
Ask AI
truss
push
my_model.py
Please note that
push
(as well as all other commands below) will require that you pass the path to the file containing the model as the final argument.
This new workflow also supports patching, so you can quickly iterate during development without building new images every time.
Copy
Ask AI
truss
watch
my_model.py
​
Model Configuration
Models can configure requirements for compute hardware (CPU count, GPU type and count, etc) and software dependencies (Python libraries or system packages) via the
remote_config
class variable within the model:
my_model.py
Copy
Ask AI
class
RunningTotalCalculator
(
baseten
.
ModelBase
):
remote_config: baseten.RemoteConfig
=
baseten.RemoteConfig(
compute
=
baseten.Compute(
cpu_count
=
4
,
memory
=
"1Gi"
,
gpu
=
"T4"
,
gpu_count
=
2
)
)
...
See the
remote configuration reference
for a complete list of options.
​
Context (access information)
You can add
DeploymentContext
object as an optional final argument to the
__init__
-method of a Model. This allows you to use secrets within your Model, but note that they’ll also need to be added to the
assets
.
We only expose secrets to the model that were explicitly requested in
assets
to comply with best security practices.
my_model.py
Copy
Ask AI
class
RunningTotalCalculator
(
baseten
.
ModelBase
):
remote_config: baseten.RemoteConfig
=
baseten.RemoteConfig(
...
assets
=
baseten.Assets(
secret_keys
=
[
"token"
])
)
def
__init__
(
self
,
context
: baseten.DeploymentContext
=
baseten.depends_context()):
...
self
._token
=
context.secrets[
"token"
]
​
Packages
If you want to include modules in your model, you can easily create them from the root of the project:
Copy
Ask AI
my-new-model/
module_1/
submodule/
script.py
module_2/
another_script.py
my_model.py
With this file structure, you would import in
my_model.py
as follows:
my_model.py
Copy
Ask AI
import
truss_chains
as
baseten
from
module_1.submodule
import
script
from
module_2
import
another_script
class
RunningTotalCalculator
(
baseten
.
ModelBase
):
...
.
​
Known Limitations
RemoteConfig does
not
support all the options exposed by the traditional
config.yaml
. If you’re excited about this new development experience but need a specific feature ported over, please reach out to us!
This new framework does not support
preprocess
or
postprocess
hooks. We typically recommend inlining functionality from those functions if easy, or utilizing
chains
if the needs are more complex.
Was this page helpful?
Yes
No
Previous
Overview
Next
On this page
Step 1: Initializing your project
Step 2: Write your model
Step 3: Deploy, patch, and public your model
Model Configuration
Context (access information)
Packages
Known Limitations
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/development/model/configuration:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Overview
Your first model
Setup and dependencies
Configuration
Custom build commands
Base Docker images
Private Docker Registries
Implementation
Implementation (Advanced)
Deploy and iterate
Performance optimization
Security and secrets
Data and storage
Python driven configuration for models 🆕
Developing a Chain
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Exporting metrics
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Setup and dependencies
Configuration
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
ML models often have dependencies on external libraries, data,
and other resources. These models also typically have particular
hardware configurations.
In this guide, we’ll cover the basics of how to configure your model
to specify this information.
Configuration for models is specified in the
config.yaml
file. Here are some
of the common configuration options:
​
Environment variables
You can specify environment variables to be set in the model serving environment
using the
environment_variables
key.
config.yaml
Copy
Ask AI
environment_variables
:
MY_ENV_VAR
:
my_value
​
Python Packages
Python packages can be specified in two ways in the
config.yaml
file:
requirements
: A list of Python packages to install.
requirements_file
: A requirements.txt file to install pip packages from.
For example, if you have a simple list of packages, you can specify them as follows:
config.yaml
Copy
Ask AI
requirements
:
-
package_name
-
package_name2
Note that you can pin versions using the
==
operator.
config.yaml
Copy
Ask AI
requirements
:
-
package_name==1.0.0
-
package_name2==2.0.0
If you need more control over the installation process and want to use
different pip options or repositories, you can specify a
requirements_file
instead.
config.yaml
Copy
Ask AI
requirements_file
:
./requirements.txt
​
System Packages
Truss also has support for installing apt-installable Debian packages. to
add system packages to your model serving environment, add the following to
your
config.yaml
file:
config.yaml
Copy
Ask AI
system_packages
:
-
package_name
-
package_name2
For a more concrete examples,
config.yaml
Copy
Ask AI
system_packages
:
-
tesseract-ocr
​
Resources
Another key part of configuring your model is specifying hardware resources needed.
You can use the
resources
key to specify these. For a CPU model, your resources
configuration might look something like:
config.yaml
Copy
Ask AI
resources
:
accelerator
:
null
cpu
:
"1"
memory
:
2Gi
use_gpu
:
false
For a GPU model, your resources configuration might look like:
config.yaml
Copy
Ask AI
resources
:
accelerator
:
"A10G"
When you push your model, it will be assigned an instance type matching the
specifications required.
See the
Resources
page for more information on
options available.
​
Advanced configuration
There are numerous other options for configuring your model. See some
of the other guides:
Secrets
Data
Custom Build Commands
Base Docker Images
Custom Servers
Custom Health Checks
Was this page helpful?
Yes
No
Previous
Custom build commands
How to run your own docker commands during the build stage
Next
On this page
Environment variables
Python Packages
System Packages
Resources
Advanced configuration
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/development/model/custom-health-checks:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Overview
Your first model
Setup and dependencies
Implementation
Implementation (Advanced)
Request handling
Custom Responses
Custom servers
Custom health checks 🆕
Cached weights 🆕
Access model environments
Request concurrency
Streaming output
Deploy and iterate
Performance optimization
Security and secrets
Data and storage
Python driven configuration for models 🆕
Developing a Chain
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Exporting metrics
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Implementation (Advanced)
Custom health checks 🆕
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
This feature is still in beta mode.
Why use custom health checks?
Control traffic and restarts
by configuring failure thresholds to suit your needs.
Define replica health with custom logic
(e.g. fail after a certain number of 500s or a specific CUDA error).
By default, health checks run every 10 seconds to verify that each replica of your deployment is running successfully and can receive requests. If a health check fails for an extended period, one or both of the following actions may occur:
Traffic is immediately stopped from reaching the failing replica.
The failing replica is restarted.
The thresholds for each of these actions are configurable.
Custom health checks can be implemented in two ways:
Configuring thresholds
for when health check failures should stop traffic to or restart a replica.
Writing custom health check logic
to define how replica health is determined.
​
Configuring health checks
​
Parameters
You can customize the behavior of health checks on your deployments by setting the following parameters:
​
stop_traffic_threshold_seconds
integer
default:
1800
The duration that health checks must continuously fail before traffic to the failing replica is stopped.
stop_traffic_threshold_seconds
must be between
30
and
1800
seconds, inclusive.
​
restart_check_delay_seconds
integer
default:
0
How long to wait before running health checks.
restart_check_delay_seconds
must be between
0
and
1800
seconds, inclusive.
​
restart_threshold_seconds
integer
default:
1800
The duration that health checks must continuously fail before triggering a restart of the failing replica.
restart_threshold_seconds
must be between
30
and
1800
seconds, inclusive.
The combined value of
restart_check_delay_seconds
and
restart_threshold_seconds
must not exceed
1800
seconds.
​
Model and custom server deployments
Configure health checks in your
config.yaml
.
config.yaml
Copy
Ask AI
runtime
:
health_checks
:
restart_check_delay_seconds
:
60
# Waits 60 seconds after deployment before starting health checks
restart_threshold_seconds
:
300
# Triggers a restart if health checks fail for 5 minutes
stop_traffic_threshold_seconds
:
600
# Stops traffic if health checks fail for 10 minutes
You can also specify custom health check endpoints for custom servers.
See here
for more details.
​
Chains
Use
remote_config
to configure health checks for your chainlet classes.
chain.py
Copy
Ask AI
class
CustomHealthChecks
(
chains
.
ChainletBase
):
remote_config
=
chains.RemoteConfig(
options
=
chains.ChainletOptions(
health_checks
=
truss_config.HealthChecks(
restart_check_delay_seconds
=
30
,
# Waits 30 seconds before starting health checks
restart_threshold_seconds
=
120
,
# Restart replicas after 2 minutes of failure
stop_traffic_threshold_seconds
=
300
,
# Stop traffic after 5 minutes of failure
)
)
)
​
Writing custom health checks
You can write custom health checks in both
model deployments
and
chain deployments
.
Custom health checks are currently not supported in development deployments.
​
Custom health checks in models
model.py
Copy
Ask AI
class
Model
:
def
is_healthy
(
self
) ->
bool
:
# Add custom health check logic for your model here
pass
​
Custom health checks in chains
Health checks can be customized for each chainlet in your chain.
chain.py
Copy
Ask AI
@chains.mark_entrypoint
class
CustomHealthChecks
(
chains
.
ChainletBase
):
def
is_healthy
(
self
) ->
bool
:
# Add custom health check logic for your chainlet here
pass
​
Health checks in action
​
Identifying 5xx errors
You might create a custom health check to identify 5xx errors like the following:
model.py
Copy
Ask AI
class
Model
:
def
__init__
(
self
):
...
self
._is_healthy
=
True
def
load
(
self
):
...
# Set the model to healthy once loading is complete
self
._is_healthy
=
True
def
is_healthy
(
self
):
return
self
._is_healthy
def
predict
(
self
,
input
):
try
:
# Perform inference
...
except
Some5xxError:
self
._is_healthy
=
False
raise
Custom health check failures are indicated by the following log:
Example health check failure log line
Copy
Ask AI
Jan 27 10:36:03pm md2pg Health check failed.
Deployment restarts due to health check failures are indicated by the following log:
Example restart log line
Copy
Ask AI
Jan 27 12:02:47pm zgbmb Model terminated unexpectedly. Exit code: 0, reason: Completed, restart count: 1
​
FAQs
​
Is there a rule of thumb for configuring thresholds for stopping traffic and restarting?
It depends on your health check implementation. If your health check relies on conditions that only change during inference (e.g.,
_is_healthy
is set in
predict
), restarting before stopping traffic is generally better, as it allows recovery without disrupting traffic.
Stopping traffic first may be preferable if a failing replica is actively degrading performance or causing inference errors, as it prevents the failing replica from affecting the overall deployment while allowing time for debugging or recovery.
​
When should I configure
restart_check_delay_seconds
?
Configure
restart_check_delay_seconds
to allow replicas sufficient time to initialize after deployment or a restart. This delay helps reduce unnecessary restarts, particularly for services with longer startup times.
​
Why am I seeing two health check failure logs in my logs?
These refer to two separate health checks we run every 10 seconds:
One to determine when to stop traffic to a replica.
The other to determine when to restart a replica.
​
Does stopped traffic or replica restarts affect autoscaling?
Yes, both can impact autoscaling. If traffic stops or replicas restart, the remaining replicas handle more load. If the load exceeds the concurrency target during the autoscaling window, additional replicas are spun up. Similarly, when traffic stabilizes, excess replicas are scaled down after the scale down delay.
See here
for more details on autoscaling.
​
How does billing get affected?
You are billed for the uptime of your deployment. This includes the time a replica is running, even if it is failing health checks, until it scales down.
​
Will failing health checks cause my deployment to stay up forever?
No. If your deployment is configured with a scale down delay and the minimum number of replicas is set to 0, the replicas will scale down once the model is no longer receiving traffic for the duration of the scale down delay. This applies even if the replicas are failing health checks.
See here
for more details on autoscaling.
Was this page helpful?
Yes
No
Previous
Cached weights 🆕
Accelerate cold starts and availability by prefetching and caching your weights.
Next
On this page
Configuring health checks
Parameters
Model and custom server deployments
Chains
Writing custom health checks
Custom health checks in models
Custom health checks in chains
Health checks in action
Identifying 5xx errors
FAQs
Is there a rule of thumb for configuring thresholds for stopping traffic and restarting?
When should I configure restart_check_delay_seconds?
Why am I seeing two health check failure logs in my logs?
Does stopped traffic or replica restarts affect autoscaling?
How does billing get affected?
Will failing health checks cause my deployment to stay up forever?
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/development/model/custom-server:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Overview
Your first model
Setup and dependencies
Implementation
Implementation (Advanced)
Request handling
Custom Responses
Custom servers
Custom health checks 🆕
Cached weights 🆕
Access model environments
Request concurrency
Streaming output
Deploy and iterate
Performance optimization
Security and secrets
Data and storage
Python driven configuration for models 🆕
Developing a Chain
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Exporting metrics
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Implementation (Advanced)
Deploy Custom servers from Docker Images
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
If you have an existing API server packaged in a
Docker image
—whether an open-source server like
vLLM
or a custom-built image—you can deploy it on Baseten
with just a
config.yaml
file
.
​
1. Configuring a Custom Server in
config.yaml
Define a
Docker-based server
by adding
docker_server
:
config.yaml
Copy
Ask AI
base_image
:
image
:
vllm/vllm-openai:latest
docker_server
:
start_command
:
vllm serve meta-llama/Meta-Llama-3.1-8B-Instruct --port 8000 --max-model-len 1024
readiness_endpoint
:
/health
liveness_endpoint
:
/health
predict_endpoint
:
/v1/chat/completions
server_port
:
8000
​
Key Configurations
start_command
(
required
) – Command to start the server.
predict_endpoint
(
required
) – Endpoint for serving requests (only one per model).
server_port
(
required
) – Port where the server runs.
readiness_endpoint
(
required
) – Used for
Kubernetes readiness probes
to determine when the container is ready to accept traffic.
liveness_endpoint
(
required
) – Used for
Kubernetes liveness probes
to determine if the container
needs to be restarted
.
​
2. Example: Running a vLLM Server
This example deploys
Meta-Llama-3.1-8B-Instruct
using
vLLM
on an
A10G GPU
, with
/health
as the readiness and liveness probe.
config.yaml
Copy
Ask AI
base_image
:
image
:
vllm/vllm-openai:latest
docker_server
:
start_command
:
sh -c "HF_TOKEN=$(cat /secrets/hf_access_token) vllm serve meta-llama/Meta-Llama-3.1-8B-Instruct --port 8000 --max-model-len 1024"
readiness_endpoint
:
/health
liveness_endpoint
:
/health
predict_endpoint
:
/v1/chat/completions
server_port
:
8000
resources
:
accelerator
:
A10G
model_name
:
vllm-model-server
secrets
:
hf_access_token
:
null
runtime
:
predict_concurrency
:
128
vLLM’s /health endpoint is used to determine when the server is ready or needs
restarting.
More examples available in Truss examples repo.
​
3. Installing custom Python packages
To install additional Python dependencies, add a
requirements.txt
file to your Truss.
​
Example: Infinity embedding model server
config.yaml
Copy
Ask AI
base_image
:
image
:
python:3.11-slim
docker_server
:
start_command
:
sh -c "infinity_emb v2 --model-id BAAI/bge-small-en-v1.5"
readiness_endpoint
:
/health
liveness_endpoint
:
/health
predict_endpoint
:
/embeddings
server_port
:
7997
resources
:
accelerator
:
L4
use_gpu
:
true
model_name
:
infinity-embedding-server
requirements
:
-
infinity-emb[all]
environment_variables
:
hf_access_token
:
null
​
4. Accessing secrets in custom servers
To use
API keys or other secrets
, store them in Baseten and
access them from
/secrets
in the container.
​
Example: Accessing a Hugging Face token
config.yaml
Copy
Ask AI
secrets
:
hf_access_token
:
null
Inside your server, access it like this:
Copy
Ask AI
HF_TOKEN
=
$(
cat
/secrets/hf_access_token
)
More on secrets management
here
.
Was this page helpful?
Yes
No
Previous
Custom health checks 🆕
Customize the health of your deployments.
Next
On this page
1. Configuring a Custom Server in config.yaml
Key Configurations
2. Example: Running a vLLM Server
3. Installing custom Python packages
Example: Infinity embedding model server
4. Accessing secrets in custom servers
Example: Accessing a Hugging Face token
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/development/model/data-directory:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Overview
Your first model
Setup and dependencies
Implementation
Implementation (Advanced)
Deploy and iterate
Performance optimization
Security and secrets
Data and storage
Python driven configuration for models 🆕
Developing a Chain
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Exporting metrics
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Developing a model
Data and storage
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
Model files, such as weights, can be
large
(often
multiple GBs
). Truss supports
multiple ways
to load them efficiently:
Public Hugging Face models
(default)
Bundled directly in Truss
​
1. Bundling model weights in Truss
Store model files
inside Truss
using the
data/
directory.
Example: Stable Diffusion 2.1 Truss structure
Copy
Ask AI
data/
scheduler/
scheduler_config.json
text_encoder/
config.json
diffusion_pytorch_model.bin
tokenizer/
merges.txt
tokenizer_config.json
vocab.json
unet/
config.json
diffusion_pytorch_model.bin
vae/
config.json
diffusion_pytorch_model.bin
model_index.json
Access bundled files in
model.py
:
Copy
Ask AI
class
Model
:
def
__init__
(
self
,
**
kwargs
):
self
._data_dir
=
kwargs[
"data_dir"
]
def
load
(
self
):
self
.model
=
StableDiffusionPipeline.from_pretrained(
str
(
self
._data_dir),
revision
=
"fp16"
,
torch_dtype
=
torch.float16,
).to(
"cuda"
)
Limitation: Large weights increase deployment size, making it slower. Consider
cloud storage instead.
​
2. Loading private model weights from S3
If using
private S3 storage
, first
configure secure authentication
.
​
Step 1: Define AWS secrets in
config.yaml
Copy
Ask AI
secrets
:
aws_access_key_id
:
null
aws_secret_access_key
:
null
aws_region
:
null
# e.g., us-east-1
aws_bucket
:
null
Do not store actual credentials here. Add them securely to
Baseten secrets
manager
.
​
Step 2: Authenticate with AWS in
model.py
Copy
Ask AI
import
boto3
def
__init__
(
self
,
**
kwargs
):
self
._config
=
kwargs.get(
"config"
)
secrets
=
kwargs.get(
"secrets"
)
self
.s3_client
=
boto3.client(
"s3"
,
aws_access_key_id
=
secrets[
"aws_access_key_id"
],
aws_secret_access_key
=
secrets[
"aws_secret_access_key"
],
region_name
=
secrets[
"aws_region"
],
)
self
.s3_bucket
=
secrets[
"aws_bucket"
]
​
Step 3: Deploy
Copy
Ask AI
truss
push
Was this page helpful?
Yes
No
Previous
Python driven configuration for models 🆕
Use code-first development tools to streamline model production.
Next
On this page
1. Bundling model weights in Truss
2. Loading private model weights from S3
Step 1: Define AWS secrets in config.yaml
Step 2: Authenticate with AWS in model.py
Step 3: Deploy
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/development/model/deploy-and-iterate:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Overview
Your first model
Setup and dependencies
Implementation
Implementation (Advanced)
Deploy and iterate
Performance optimization
Security and secrets
Data and storage
Python driven configuration for models 🆕
Developing a Chain
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Exporting metrics
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Developing a model
Deploy and iterate
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
In
Your First Model
, we walked through
how to deploy a basic model to Baseten. If you are trying to rapidly make changes
and iterate on your model, you’ll notice that there is quite a bit of time between
running
truss push
and when the changes are reflected on Baseten.
Also, a lot of models require special hardware that you may not immediately have
access to.
To solve this problem, we have a feature called
Truss Watch
, that allows you to
live reload your model as you work.
​
Truss Watch
To make use of
truss watch
, start by deploying your model:
Copy
Ask AI
$
truss
push
By default, this will deploy a “development” version of your model. This means that the model
has a live reload server attached to it and supports hot reloading. To get the hot reload
loop working, simply run
truss watch
afterwards:
Copy
Ask AI
$
truss
watch
Now, if you make changes to your model, you’ll see them reflected in the model logs!
You can now happily iterate on your model without having to go through the entire
build & deploy loop between each change.
​
Ready for Production?
Once you’ve iterated on your model, and you’re ready to deploy it to production,
you can use the
truss push --publish
command. This will deploy a “published”
version of your model
Copy
Ask AI
truss
push
--publish
Note that development models have slightly worse performance, and have more
limited scaling properites, so it’s highly recommend to not use these for
any production use-case.
Was this page helpful?
Yes
No
Previous
Concepts
Improve your latency and throughput
Next
On this page
Truss Watch
Ready for Production?
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/development/model/environments:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Overview
Your first model
Setup and dependencies
Implementation
Implementation (Advanced)
Request handling
Custom Responses
Custom servers
Custom health checks 🆕
Cached weights 🆕
Access model environments
Request concurrency
Streaming output
Deploy and iterate
Performance optimization
Security and secrets
Data and storage
Python driven configuration for models 🆕
Developing a Chain
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Exporting metrics
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Implementation (Advanced)
Access model environments
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
Model environments help configure behavior based on
deployment stage
(e.g., production vs. staging). You can access the environment details via
kwargs
in the
Model
class.
​
1. Retrieve Environment Variables
Access the environment in
__init__
:
Copy
Ask AI
def
__init__
(
self
,
**
kwargs
):
self
._environment
=
kwargs[
"environment"
]
​
2. Configure Behavior Based on Environment
Use environment variables in the
load
function:
Copy
Ask AI
def
load
(
self
):
if
self
._environment.get(
"name"
)
==
"production"
:
# Production setup
self
.setup_sentry()
self
.setup_logging(
level
=
"INFO"
)
self
.load_production_weights()
else
:
# Default setup for staging or development deployments
self
.setup_logging(
level
=
"DEBUG"
)
self
.load_default_weights()
Why use this?
Customize logging levels
Load environment-specific model weights
Enable monitoring tools (e.g., Sentry)
Was this page helpful?
Yes
No
Previous
Request concurrency
A guide to setting concurrency for your model
Next
On this page
1. Retrieve Environment Variables
2. Configure Behavior Based on Environment
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/development/model/implementation:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Overview
Your first model
Setup and dependencies
Implementation
Implementation (Advanced)
Deploy and iterate
Performance optimization
Security and secrets
Data and storage
Python driven configuration for models 🆕
Developing a Chain
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Exporting metrics
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Developing a model
Implementation
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
In this section, we’ll cover how to implement the actual logic for your model.
As was mentioned in
Your First Model
, the
logic for the model itself is specified in a
model/model.py
file. To recap, the simplest
directory structure for a model is:
Copy
Ask AI
model/
model.py
config.yaml
It’s expected that the
model.py
file contains a class with particular methods:
model.py
Copy
Ask AI
class
Model
:
def
__init__
(
self
):
pass
def
load
(
self
):
pass
def
predict
(
self
,
input_data
):
pass
The
__init__
method is used to initialize the
Model
class, and allows you to read
in configuration parameters and other information.
The
load
method is where you define the logic for initializing the model. This might
include downloading model weights, or loading them onto a GPU.
The
predict
method is where you define the logic for inference.
In the next sections, we’ll cover each of these methods in more detail.
​
init
As mentioned above, the
__init__
method is used to initialize the
Model
class, and allows you to
read in configuration parameters and runtime information.
The simplest signature for
__init__
is:
model.py
Copy
Ask AI
def
__init__
(
self
):
pass
If you need more information, however, you have the option to define your
init
method
such that it accepts the following parameters:
model.py
Copy
Ask AI
def
__init__
(
self
,
config
:
dict
,
data_dir
:
str
,
secrets
:
dict
,
environment
:
str
):
pass
config
: A dictionary containing the config.yaml for the model.
data_dir
: A string containing the path to the data directory for the model.
secrets
: A dictionary containing the secrets for the model. Note that at runtime,
these will be populated with the actual values as stored on Baseten.
environment
: A string containing the environment for the model, if the model has been
deployed to an environment.
You can then make use of these parameters in the rest of your model but saving these as
attributes:
model.py
Copy
Ask AI
def
__init__
(
self
,
config
:
dict
,
data_dir
:
str
,
secrets
:
dict
,
environment
:
str
):
self
._config
=
config
self
._data_dir
=
data_dir
self
._secrets
=
secrets
self
._environment
=
environment
​
load
The
load
method is where you define the logic for initializing the model. As
mentioned before, this might include downloading model weights or loading them
onto the GPU.
load
, unlike the other method mentioned, does not accept any parameters:
model.py
Copy
Ask AI
def
load
(
self
):
pass
After deploying your model, the deployment will not be considered “Ready” until
load
has
completed successfully. Note that there is a
timeout of 30 minutes
for this, after which,
if
load
has not completed, the deployment will be marked as failed.
​
predict
The
predict
method is where you define the logic for performing inference.
The simplest signature for
predict
is:
model.py
Copy
Ask AI
def
predict
(
self
,
input_data
) ->
str
:
return
"Hello"
The return type of
predict
must be JSON-serializable, so it can be:
dict
list
str
If you would like to return a more strictly typed object, you can return a
Pydantic
object.
model.py
Copy
Ask AI
from
pydantic
import
BaseModel
class
Result
(
BaseModel
):
value:
str
You can then return an instance of this model from
predict
:
model.py
Copy
Ask AI
def
predict
(
self
,
input_data
) -> Prediction:
return
Result(
value
=
"Hello"
)
​
Streaming
In addition to supporting a single request/response cycle, Truss also supports streaming.
See the
Streaming
guide for more information.
​
Async vs. Sync
Note that the
predict
method is synchronous by default. However, if your model inference
depends on APIs require
asyncio
,
predict
can also be written as a coroutine.
model.py
Copy
Ask AI
import
asyncio
async
def
predict
(
self
,
input_data
) ->
dict
:
# Async logic here
await
asyncio.sleep(
1
)
return
{
"value"
:
"Hello"
}
If you are using
asyncio
in your
predict
method, be sure not to perform any blocking
operations, such as a synchronous file download. This can result in degraded performance.
Was this page helpful?
Yes
No
Previous
Request handling
Get more control by directly using the request object.
Next
On this page
init
load
predict
Streaming
Async vs. Sync
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/development/model/model-cache:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Overview
Your first model
Setup and dependencies
Implementation
Implementation (Advanced)
Request handling
Custom Responses
Custom servers
Custom health checks 🆕
Cached weights 🆕
Access model environments
Request concurrency
Streaming output
Deploy and iterate
Performance optimization
Security and secrets
Data and storage
Python driven configuration for models 🆕
Developing a Chain
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Exporting metrics
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Implementation (Advanced)
Cached weights 🆕
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
​
What is a “cold start”?
Cold start” is a term used to describe the time taken when a request is received when the model is scaled to 0 until it is ready to handle the first request. This process is a critical factor in allowing your deployments to be responsive to traffic while maintaining your SLAs and lowering your costs.
To optimize cold starts, we will go over the following stategies: Downloading them in a background thread in Rust that runs during the module import, caching weights in a distributed filesystem, and moving weights into the docker image.
In practice, this reduces the cold start for large models to just a few seconds. For example, Stable Diffusion XL can take a few minutes to boot up without caching. With caching, it takes just under 10 seconds.
​
Enabling Caching + Prefetching for a Model
To enable caching, simply add
model_cache
to your
config.yaml
with a valid
repo_id
. The
model_cache
has a few key configurations:
repo_id
(required): The repo name from Hugging Face.
revision
(required): The revision of the huggingface repo, such as the sha or branch name such as
refs/pr/1
or
main
.
use_volume
: Boolean flag to determine if the weights are downloaded to the Baseten Filesystem at runtime (recommended) or bundled into the container image (not recommended).
volume_folder
: string, folder name under which the model weights appear. Setting it to
my-llama-model
will mount the repo to
/app/model_cache/my-llama-model
at runtime.
allow_patterns
: Only cache files that match specified patterns. Utilize Unix shell-style wildcards to denote these patterns.
ignore_patterns
: Conversely, you can also denote file patterns to ignore, hence streamlining the caching process.
Here is an example of a well written
model_cache
for Stable Diffusion XL. Note how it only pulls the model weights that it needs using
allow_patterns
.
config.yaml
Copy
Ask AI
model_cache
:
-
repo_id
:
madebyollin/sdxl-vae-fp16-fix
revision
:
207b116dae70ace3637169f1ddd2434b91b3a8cd
use_volume
:
true
volume_folder
:
sdxl-vae-fp16
allow_patterns
:
-
config.json
-
diffusion_pytorch_model.safetensors
-
repo_id
:
stabilityai/stable-diffusion-xl-base-1.0
revision
:
462165984030d82259a11f4367a4eed129e94a7b
use_volume
:
true
volume_folder
:
stable-diffusion-xl-base
allow_patterns
:
-
"*.json"
-
"*.fp16.safetensors"
-
sd_xl_base_1.0.safetensors
-
repo_id
:
stabilityai/stable-diffusion-xl-refiner-1.0
revision
:
5d4cfe854c9a9a87939ff3653551c2b3c99a4356
use_volume
:
true
volume_folder
:
stable-diffusion-xl-refiner
allow_patterns
:
-
"*.json"
-
"*.fp16.safetensors"
-
sd_xl_refiner_1.0.safetensors
Many Hugging Face repos have model weights in different formats (
.bin
,
.safetensors
,
.h5
,
.msgpack
, etc.). You only need one of these most of the time. To minimize cold starts, ensure that you only cache the weights you need.
​
What is weight “pre-fetching”?
With
model_cache
, weights are pre-fetched by downloading your weights ahead of time in a dedicated Rust thread.
This means, you can perform all kinds of preparation work (importing libraries, jit compilation of torch/triton modules), until you need access to the files.
In practice, executing statements like
import tensorrt_llm
typically take 10–15 seconds. By that point, the first 5–10GB of the weights will have already been downloaded.
To use the
model_cache
config with truss,  we require you to actively interact with the
lazy_data_resolver
.
Before using any of the downloaded files, you must call the
lazy_data_resolver.block_until_download_complete()
. This will block until all files in the
/app/model_cache
directory are downloaded & ready to use.
This call must be either part of your
__init__
or
load
implementation.
model.py
Copy
Ask AI
# <- download is invoked before here.
import
torch
# this line usually takes 2-5 seconds.
import
tensorrt_llm
# this line usually takes 10-15 seconds
import
onnxruntime
# this line usually takes 5-10 seconds
class
Model
:
"""example usage of `model_cache` in truss"""
def
__init__
(
self
,
*
args
,
**
kwargs
):
# `lazy_data_resolver` is passed as keyword-argument in init
self
._lazy_data_resolver
=
kwargs[
"lazy_data_resolver"
]
def
load
():
# work that does not require the download may be done beforehand
random_vector
=
torch.randn(
1000
)
# important to collect the download before using any incomplete data
self
._lazy_data_resolver.block_until_download_complete()
# after the call, you may use the /app/model_cache directory
torch.load(
"/app/model_cache/your_model.pt"
)
*
random_vector
​
Private Hugging Face repositories 🤗
For any public Hugging Face repo, you don’t need to do anything else. Adding the
model_cache
key with an appropriate
repo_id
should be enough.
However, if you want to deploy a model from a gated repo like
Llama 2
to Baseten, there are a few steps you need to take:
1
Get Hugging Face API Key
Grab an API key
from Hugging Face with
read
access. Make sure you have access to the model you want to serve.
2
Add it to Baseten Secrets Manager
Paste your API key in your
secrets manager in Baseten
under the key
hf_access_token
. You can read more about secrets
here
.
3
Update Config
In your Truss’s
config.yaml
, add the following code:
config.yaml
Copy
Ask AI
secrets
:
hf_access_token
:
null
Make sure that the key
secrets
only shows up once in your
config.yaml
.
If you run into any issues, run through all the steps above again and make sure you did not misspell the name of the repo or paste an incorrect API key.
​
model_cache
within Chains
To use
model_cache
for
chains
- use the
Assets
specifier. In the example below, we will download
llama-3.2-1B
.
As this model is a gated huggingface model, we are setting the mounting token as part of the assets
chains.Assets(..., secret_keys=["hf_access_token"])
.
The model is quite small - in many cases, we will be able to download the model while
from transformers import pipeline
and
import torch
are running.
chain_cache.py
Copy
Ask AI
import
random
import
truss_chains
as
chains
try
:
# imports on global level for PoemGeneratorLM, to save time during the download.
from
transformers
import
pipeline
import
torch
except
ImportError
:
# RandInt does not have these dependencies.
pass
class
RandInt
(
chains
.
ChainletBase
):
async
def
run_remote
(
self
,
max_value
:
int
) ->
int
:
return
random.randint(
1
, max_value)
@chains.mark_entrypoint
class
PoemGeneratorLM
(
chains
.
ChainletBase
):
from
truss
import
truss_config
LLAMA_CACHE
=
truss_config.ModelRepo(
repo_id
=
"meta-llama/Llama-3.2-1B-Instruct"
,
revision
=
"c4219cc9e642e492fd0219283fa3c674804bb8ed"
,
use_volume
=
True
,
volume_folder
=
"llama_mini"
,
ignore_patterns
=
[
"*.pth"
,
"*.onnx"
]
)
remote_config
=
chains.RemoteConfig(
docker_image
=
chains.DockerImage(
# The phi model needs some extra python packages.
pip_requirements
=
[
"transformers==4.48.0"
,
"torch==2.6.0"
,
]
),
compute
=
chains.Compute(
gpu
=
"L4"
),
# The phi model needs a GPU and more CPUs.
# compute=chains.Compute(cpu_count=2, gpu="T4"),
# Cache the model weights in the image
assets
=
chains.Assets(
cached
=
[
LLAMA_CACHE
],
secret_keys
=
[
"hf_access_token"
]),
)
# <- Download happens before __init__ is called.
def
__init__
(
self
,
rand_int
=
chains.depends(RandInt,
retries
=
3
)) ->
None
:
self
._rand_int
=
rand_int
print
(
"loading cached llama_mini model"
)
self
.pipeline
=
pipeline(
"text-generation"
,
model
=
f
"/app/model_cache/llama_mini"
,
)
async
def
run_remote
(
self
,
max_value
:
int
=
3
) ->
str
:
num_repetitions
=
await
self
._rand_int.run_remote(max_value)
print
(
"writing poem with num_repetitions"
, num_repetitions)
poem
=
str
(
self
.pipeline(
text_inputs
=
"Write a beautiful and descriptive poem about the ocean. Focus on its vastness, movement, and colors."
,
max_new_tokens
=
150
,
do_sample
=
True
,
return_full_text
=
False
,
temperature
=
0.7
,
top_p
=
0.9
,
)[
0
][
'generated_text'
])
return
poem
*
num_repetitions
​
model_cache
for custom servers
If you are not using Python’s
model.py
and
custom servers
such as
vllm
, TEI or
sglang
,
you are required to use the
truss-transfer-cli
command, to force population of the
/app/model_cache
location. The command will block until the weights are downloaded.
Here is an example for how to use text-embeddings-inference on a L4 to populate a jina embeddings model from huggingface into the model_cache.
config.yaml
Copy
Ask AI
base_image
:
image
:
baseten/text-embeddings-inference-mirror:89-1.6
docker_server
:
liveness_endpoint
:
/health
predict_endpoint
:
/v1/embeddings
readiness_endpoint
:
/health
server_port
:
7997
# using `truss-transfer-cli` to download the weights to `cached_model`
start_command
:
bash -c "truss-transfer-cli && text-embeddings-router --port 7997
--model-id /app/model_cache/my_jina --max-client-batch-size 128 --max-concurrent-requests
128 --max-batch-tokens 16384 --auto-truncate"
model_cache
:
-
repo_id
:
jinaai/jina-embeddings-v2-base-code
revision
:
516f4baf13dec4ddddda8631e019b5737c8bc250
use_volume
:
true
volume_folder
:
my_jina
ignore_patterns
: [
"*.onnx"
]
model_metadata
:
example_model_input
:
encoding_format
:
float
input
:
text string
model
:
model
model_name
:
TEI-jinaai-jina-embeddings-v2-base-code-truss-example
resources
:
accelerator
:
L4
​
Optimizing access time futher with b10cache enabled
b10cache is currently in beta mode
To further reduce weights loading time, we can enable Baseten’s Distributed Filesystem (b10cache) for your account.
You can validate that this is enabled for your account by viewing the logs of your deployment.
Copy
Ask AI
[2025-09-10 01:04:35] [INFO ] b10cache is enabled.
[2025-09-10 01:04:35] [INFO ] Symlink created successfully. Skipping download for /app/model_cache/cached_model/model.safetensors
Once b10cache is active, we will skip downloads that are cached in the filesystem of the region your deployment is running in.
b10cache acts like a content delivery network: Initial cache misses are populating the filesystem, unused files are garbage collected after 14 days.
Once b10cache is active, it will pull from the fastest source. If another pod is active on the same physical node, artifacts may be hot-cached, and shared among your deployments.
Downloads are fully isolated from other organizations.
If b10cache is not available for your account, we will provision the model_cache with a optimized download from HuggingFace.co.
The download is parallellized, achieving typical download speeds of greater than 1GB/s on a 10Gbit ethernet connection.
If you want to enable b10cache, feel free to reach out to our support.
​
Legacy cache - weights in container
A slower way to make sure your weights are always available, is to download them into the docker image at build time.
We recommend this only for small models, of up to a size of ~1GB.
Tradeoffs:
highest availability: model weights will never depend on S3/huggingface uptime. High availability on b10cache.
slower cold-starts: docker images may need to be pulled from a slower source that has lower speed S3 or Huggingface.
unsuitable for very large-models: We don’t recommend placing large model artifacts into the docker image, and may lead to build failures when larger than 50GB.
​
Download weights into the image via
build_commands
The most flexible way to download weights into the docker image is the usage of custom
build_commands
.
You can read more on build_commands
here.
config.yaml
Copy
Ask AI
build_commands
:
-
'apt-get install git git-lfs'
-
'git lfs install'
-
'git clone https://huggingface.co/nomic-ai/nomic-embed-text-v1.5 /data/local-model'
-
echo 'Model downloaded to /data/local-model via git clone'
​
Download the weights via
model_cache
and
use_volume: false
If you are setting
use_volume: false
, we will not use b10cache to mount the model weights at runtime, and rather vendor them into the docker image.
​
Huggingface
config.yaml
Copy
Ask AI
model_cache
:
-
repo_id
:
madebyollin/sdxl-vae-fp16-fix
revision
:
207b116dae70ace3637169f1ddd2434b91b3a8cd
use_volume
:
false
allow_patterns
:
-
config.json
-
diffusion_pytorch_model.safetensors
Weights will be cached in the default Hugging Face cache directory,
~/.cache/huggingface/hub/models--{your_model_name}/
. You can change this directory by setting the
HF_HOME
or
HUGGINGFACE_HUB_CACHE
environment variable in your
config.yaml
.
Read more here
.
Huggingface libraries will use this directly.
model.py
Copy
Ask AI
from
transformers
import
AutoModel
AutoModel.from_pretrained(
"madebyollin/sdxl-vae-fp16-fix"
)
​
Google Cloud Storage
Google Cloud Storage is a great alternative to Hugging Face when you have a custom model or fine-tune you want to gate, especially if you are already using GCP and care about security and compliance.
Your
model_cache
should look something like this:
config.yaml
Copy
Ask AI
model_cache
:
-
repo_id
:
gs://path-to-my-bucket
use_volume
:
false
If you are accessing a public GCS bucket, you can ignore the following steps, but make sure you set appropriate permissions on your bucket. Users should be able to list and view all files. Otherwise, the model build will fail.
For a private GCS bucket, first export your service account key. Rename it to be
service_account.json
and add it to the
data
directory of your Truss.
Your file structure should look something like this:
Copy
Ask AI
your-truss
|--model
| └── model.py
|--data
|. └── service_account.json
If you are using version control, like git, for your Truss, make sure to add
service_account.json
to your
.gitignore
file. You don’t want to accidentally expose your service account key.
Weights will be cached at
/app/model_cache/{your_bucket_name}
.
​
Amazon Web Services S3
Another popular cloud storage option for hosting model weights is AWS S3, especially if you’re already using AWS services.
Your
model_cache
should look something like this:
config.yaml
Copy
Ask AI
model_cache
:
-
repo_id
:
s3://path-to-my-bucket
use_volume
:
false
If you are accessing a public S3 bucket, you can ignore the subsequent steps, but make sure you set an appropriate policy on your bucket. Users should be able to list and view all files. Otherwise, the model build will fail.
However, for a private S3 bucket, you need to first find your
aws_access_key_id
,
aws_secret_access_key
, and
aws_region
in your AWS dashboard. Create a file named
s3_credentials.json
. Inside this file, add the credentials that you identified earlier as shown below. Place this file into the
data
directory of your Truss.
The key
aws_session_token
can be included, but is optional.
Here is an example of how your
s3_credentials.json
file should look:
Copy
Ask AI
{
"aws_access_key_id"
:
"YOUR-ACCESS-KEY"
,
"aws_secret_access_key"
:
"YOUR-SECRET-ACCESS-KEY"
,
"aws_region"
:
"YOUR-REGION"
}
Your overall file structure should now look something like this:
Copy
Ask AI
your-truss
|--model
| └── model.py
|--data
|. └── s3_credentials.json
When you are generating credentials, make sure that the resulting keys have at minimum the following IAM policy:
Copy
Ask AI
{
"Version"
:
"2012-10-17"
,
"Statement"
: [
{
"Action"
: [
"s3:GetObject"
,
"s3:ListObjects"
,
],
"Effect"
:
"Allow"
,
"Resource"
: [
"arn:aws:s3:::S3_BUCKET/PATH_TO_MODEL/*"
]
},
{
"Action"
: [
"s3:ListBucket"
,
],
"Effect"
:
"Allow"
,
"Resource"
: [
"arn:aws:s3:::S3_BUCKET"
]
}
]
}
If you are using version control, like git, for your Truss, make sure to add
s3_credentials.json
to your
.gitignore
file. You don’t want to accidentally expose your service account key.
Weights will be cached at
/app/model_cache/{your_bucket_name}
.
Was this page helpful?
Yes
No
Previous
Access model environments
A guide to leveraging environments in your models
Next
On this page
Enabling Caching + Prefetching for a Model
Private Hugging Face repositories 🤗
model_cache within Chains
model_cache for custom servers
Optimizing access time futher with b10cache enabled
Legacy cache - weights in container
Download weights into the image via build_commands
Download the weights via model_cache and use_volume: false
Huggingface
Google Cloud Storage
Amazon Web Services S3
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/development/model/overview:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Overview
Your first model
Setup and dependencies
Implementation
Implementation (Advanced)
Deploy and iterate
Performance optimization
Security and secrets
Data and storage
Python driven configuration for models 🆕
Developing a Chain
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Exporting metrics
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Developing a model
Developing a Model on Baseten
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
Baseten makes it easy to go from a trained machine learning model to a fully-deployed, production-ready API. You’ll use Truss—our open-source model packaging tool—to containerize your model code and configuration, and ship it to Baseten for deployment, testing, and scaling.
​
What does it mean to develop a model?
In Baseten, developing a model means:
Packaging your model code and weights
:
Wrap your trained model into a structured project that includes your inference logic and dependencies.
Configuring the model environment
:
Define everything needed to run your model—from Python packages to system dependencies and secrets.
Deploying and iterating quickly
:
Push your model to Baseten in development mode and make live edits with instant feedback.
Once your model works the way you want, you can promote it to
production
, ready for live traffic.
​
Development flow on Baseten
Here’s what the typical model development loop looks like:
Initialize a new model project
using the Truss CLI.
Add your model logic
to a Python class (model.py), specifying how to load and run inference.
Configure dependencies
in a YAML or Python config.
Deploy the model
in development mode using truss push.
Iterate fast
with truss watch—live-reload your dev deployment as you make changes.
Test and tune
the model until it’s production-ready.
Promote the model
to production when you’re ready to scale.
Note:
Truss runs your model in a standardized container without needing
Docker installed locally. It also gives you a fast developer loop and a
consistent way to configure and serve models.
​
What is Truss?
Truss is the tool you use to:
Scaffold a new model project
Serve models locally or in the cloud
Package your code, config, and model files
Push to Baseten for deployment
You can think of it as the developer toolkit for building and managing model servers—built specifically for machine learning workflows.
With Truss, you can create a containerized model server
without needing to learn Docker
, and define everything about how your model runs: Python and system packages, GPU settings, environment variables, and custom inference logic. It gives you a fast, reproducible dev loop—test changes locally or in a remote environment that mirrors production.
Truss is
flexible enough to support a wide range of ML stacks
, including:
Model frameworks like
PyTorch
,
transformers
, and
diffusers
Inference engines
like
TensorRT-LLM
,
SGLang
,
vLLM
Serving technologies like
Triton
Any package installable with
pip
or
apt
We’ll use Truss throughout this guide, but the focus will stay on
how you develop models
, not just how Truss works.
​
From model to server: the key components
When you develop a model on Baseten, you define:
A
Model
class
: This is where your model is loaded, preprocessed, run, and the results returned.
A
configuration file
(
config.yaml
or Python config): Defines the runtime environment, dependencies, and deployment settings.
Optional
extra assets
, like model weights, secrets, or external packages.
These components together form a
Truss
, which is what you deploy to Baseten.
Truss simplifies and standardizes model packaging for seamless deployment. It encapsulates model code, dependencies, and configurations into a
portable, reproducible structure
, enabling efficient development, scaling, and optimization.
​
Development vs. other deployments
The only special deployment is
development
.
Development deployment
Meant for iteration and testing. It supports
live-reloading
for quick feedback loops and will only scale to
one replica
, no autoscaling.
All others deployments
Stable, autoscaled, and ready for live traffic but
don’t support live-reloading
.
You’ll use the dev deployment to build and test, then promote it to an environment like
staging
or
production
once you’re satisfied.
Was this page helpful?
Yes
No
Previous
Your first model
Build and deploy your first model
Next
On this page
What does it mean to develop a model?
Development flow on Baseten
What is Truss?
From model to server: the key components
Development vs. other deployments
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/development/model/performance/concepts:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Overview
Your first model
Setup and dependencies
Implementation
Implementation (Advanced)
Deploy and iterate
Performance optimization
Concepts
Engine builder overview
Engine control in Python
Engine builder configuration
Security and secrets
Data and storage
Python driven configuration for models 🆕
Developing a Chain
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Exporting metrics
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Performance optimization
Concepts
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
Model performance means optimizing every layer of your model serving infrastructure to balance four goals:
Latency
: on a per-request basis, how quickly does each user get output from the model?
Throughput
: how many requests or users can the deployment handle at once?
Cost
: how much does a standardized unit of work (e.g. 1M tokens from an LLM) cost?
Quality
: does your model consistently deliver high-quality output after optimization?
​
Model performance tooling
TensorRT-LLM Engine Builder
Baseten’s TensorRT-LLM engine builder simplifies and automates the process of
using TensorRT-LLM for development and production.
​
Full-stack model performance
​
Model and GPU selection
Two of the highest-impact choices for model performance come before the optimization process: picking the best model size and implementation and picking the right GPU to run it on.
Model selection
Tradeoff: Latency/Throughput/Cost vs Quality
The biggest factor in your latency, throughput, cost, and quality is what model you use. Before you jump into optimizing a foundation model, consider:
Can you use a smaller size, like Llama 8B instead of 70B? Can you fine-tune the smaller model for your use case?
Can you use a different model, like
SDXL Lightning
instead of SDXL?
Can you use a different implementation, like
Faster Whisper
instead of Whisper?
Usually, model selection is bound by quality. For example SDXL Lightning makes images incredibly quickly, but they may not be detailed enough for your use case.
Experiment with alternative models to see if they can reset your performance expectations while meeting your quality bar.
GPU selection
Tradeoff: Latency/Throughput vs Cost
The minimum requirement for a GPU instance is that it must have enough VRAM to load model weights with headroom left for inference.
It often makes sense to use a more powerful (but more expensive) GPU than the minimum requirement, especially if you have ambitious latency goals and/or high utilization.
For example, you might choose:
(Multiple) H100 GPUs for
deployments optimized with TensorRT/TensorRT-LLM
H100 MIGs for
high-throughput deployments of smaller models like Llama 3 8B and SDXL
L4 GPUs for autoscaling Whisper deployments
The
GPU instance reference
lists all available options.
​
GPU-level optimizations
Our first goal is to get the best possible performance out of a single GPU or GPU cluster.
Inference engine
Benefit: Latency/Throughput/Cost
You can just use
transformers
and
pytorch
out of the box to serve your model. But best-in-class performance comes from using a dedicated inference engine, like:
TensorRT
/
TensorRT-LLM
, maintained by NVIDIA
vLLM
, an independent open source project
TGI
, maintained by Hugging Face
We
often recommend TensorRT/TensorRT-LLM
for best performance. The easiest way to get started with TensorRT-LLM is our
TRT-LLM engine builder
.
Inference server
Benefit: Latency/Throughput
In addition to an optimized inference engine, you need an inference server to handle requests and supply features like in-flight batching.
Baseten runs a modified version of Triton for compatible model deployments. Other models use
TrussServer
, a capable general-purpose model inference server built into Truss.
Quantization
Tradeoff: Latency/Throughput/Cost vs Quality
By default, model inference happens in
fp16
, meaning that model weights and other values are represented as 16-bit floating-point numbers.
Through a process called
post-training quantization
, you can instead run inference in a different format, like
fp8
,
int8
, or
int4
. This has massive benefits: more teraFLOPS at lower precision means lower latency, smaller numbers being retrieved from VRAM means higher throughput, and smaller model weights means saving on cost and potentially using fewer GPUs.
However, quantization can affect output quality. Thoroughly review quantized model outputs by hand and with standard checks like perplexity to ensure that the output of the quantized model matches the original.
We’ve had a lot of success with
fp8 for faster inference without quality loss
and encourage experimenting with quantization, especially when using the TRT-LLM engine builder.
Model-level optimizations
Tradeoff: Latency/Throughput/Cost vs Quality
There are a number of exciting cutting-edge techniques for model inference that can massively improve latency and/or throughput for a model. For example, LLMs can use Speculative Decoding or Medusa to generate multiple tokens per forward pass, improving TPS.
When using a new technique to improve model performance, always run real-world benchmarks and carefully validate output quality to ensure the performance improvements aren’t undermining the model’s usefulness.
Batching (GPU concurrency)
Tradeoff: Latency vs Throughput/Cost
Batch size is how many requests are processed concurrently on the GPU. It is a direct tradeoff between latency and throughput:
Increase batch size to improve throughput and cost
Reduce batch size to improve latency
​
Infrastructure-level optimizations
Once we squeeze as much TPS as possible out of the GPU, we scale that out horizontally with infrastructure optimization.
Autoscaling
Tradeoff: Latency/Throughput vs Cost
If traffic to a deployment is high enough, even an optimized model server won’t be able to keep up. By creating replicas, you keep latency consistent for all users.
Learn more about
autoscaling model replicas
.
Replica-level concurrency
Tradeoff: Latency vs Throughput/Cost
Replica-level concurrency sets the number of requests that can be sent to the model server at one time. This is different from the on-GPU concurrency as your model server may perform pre- and post-processing tasks on CPU.
Replica-level concurrency should always be greater than or equal to on-device concurrency (batch size).
Network latency
Tradeoff: Latency vs Cost
If your GPU is in us-east-1 and your customer is in Australia, it doesn’t matter how much you’ve optimized TTFT — your real-world latency will be terrible.
Region-specific deployments are available on a per-customer basis. Contact us at
support@baseten.co
to discuss your needs.
​
Application-level optimizations
There are also application-level steps that you can take to make sure you’re getting the most value from your optimized endpoint.
Good prompts
Benefits: Latency, Quality
Every token an LLM doesn’t have to process or generate is a token that you don’t have to wait for or pay for.
Prompt engineering can be as simple as saying “be concise” or as complex as making sure your RAG system returns the minimum number of highly-relevant retrievals.
Consistent sequence shapes
Benefits: Latency, Throughput
When using TensorRT-LLM, make sure that your input and output sequences are a consistent length. The inference engine is built for a specific number of tokens, and going outside of those sequence shapes will hurt performance.
Chains for multi-step inference
Benefits: Latency, Cost
The only thing running on your GPU should be the AI model. Other tasks like retrievals, secondary models, and business logic should be deployed and scaled separately to avoid bottlenecks.
Use
Chains
for performant multi-step and multi-model inference.
Session reuse during inference
Benefit: Latency
Use sessions rather than individual requests to avoid unnecessary network latency. See
inference documentation
for details.
Was this page helpful?
Yes
No
Previous
Engine builder overview
Deploy optimized model inference servers in minutes
Next
On this page
Model performance tooling
Full-stack model performance
Model and GPU selection
GPU-level optimizations
Infrastructure-level optimizations
Application-level optimizations
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/development/model/performance/concurrency:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Overview
Your first model
Setup and dependencies
Implementation
Implementation (Advanced)
Request handling
Custom Responses
Custom servers
Custom health checks 🆕
Cached weights 🆕
Access model environments
Request concurrency
Streaming output
Deploy and iterate
Performance optimization
Security and secrets
Data and storage
Python driven configuration for models 🆕
Developing a Chain
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Exporting metrics
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Implementation (Advanced)
Request concurrency
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
Configuring concurrency optimizes
model performance
, balancing
throughput
and
latency
.
In Baseten & Truss, concurrency is managed at
two levels
:
Concurrency Target
– Limits the number of requests
sent
to a single replica.
Predict Concurrency
– Limits how many requests the predict function handles
inside the model container
.
​
1. Concurrency Target
Set in the Baseten UI
– Defines how many requests a single replica can process at once.
Triggers autoscaling
– If all replicas hit the concurrency target, additional replicas spin up.
Example:
Concurrency Target = 2, Single Replica
5 requests arrive
→ 2 are processed immediately,
3 are queued
.
If max replicas aren’t reached,
autoscaling spins up a new replica
.
​
2. Predict Concurrency
Set in
config.yaml
– Controls how many requests can be
processed by
predict simultaneously.
Protects GPU resources
– Prevents multiple requests from overloading the GPU.
​
Configuring Predict Concurrency
config.yaml
Copy
Ask AI
model_name
:
"My model with concurrency limits"
runtime
:
predict_concurrency
:
2
# Default is 1
​
How It Works Inside a Model Pod
Requests arrive
→ All begin preprocessing (e.g., downloading images from S3).
Predict runs on GPU
→ Limited by
predict_concurrency
.
Postprocessing begins
→ Can run while other requests are still in inference.
​
When to Use Predict Concurrency
✅
Protect GPU resources
– Prevent multiple requests from degrading performance.
✅
Allow parallel preprocessing/postprocessing
– I/O tasks can continue even when inference is blocked.
Ensure
Concurrency Target
is set high enough to send enough requests to the container.
Was this page helpful?
Yes
No
Previous
Streaming output
Streaming Output for LLMs
Next
On this page
1. Concurrency Target
2. Predict Concurrency
Configuring Predict Concurrency
How It Works Inside a Model Pod
When to Use Predict Concurrency
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/development/model/performance/engine-builder-config:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Overview
Your first model
Setup and dependencies
Implementation
Implementation (Advanced)
Deploy and iterate
Performance optimization
Concepts
Engine builder overview
Engine control in Python
Engine builder configuration
Security and secrets
Data and storage
Python driven configuration for models 🆕
Developing a Chain
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Exporting metrics
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Performance optimization
Engine builder configuration
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
This reference lists every configuration option for the TensorRT-LLM Engine Builder. These options are used in
config.yaml
, such as for this Llama 3.1 8B example:
config.yaml
Copy
Ask AI
model_name
:
Llama 3.1 8B Engine
resources
:
accelerator
:
H100:1
secrets
:
hf_access_token
:
"set token in baseten workspace"
trt_llm
:
build
:
base_model
:
decoder
checkpoint_repository
:
repo
:
meta-llama/Llama-3.1-8B-Instruct
source
:
HF
​
trt_llm.build
TRT-LLM engine build configuration. TensorRT-LLM attempts to build a highly optimized network based on input shapes representative of your workload.
​
base_model
The base model architecture of your model checkpoint. Supported architectures include:
decoder
- for CausalLM such as
Llama/Mistral/Qwen3ForCausalLM
encoder
- for
Bert/Roberta/LLamaForSequenceClassification
, sentence-transformer models, embedding models
Deprecated:
llama
(decrecated, use
decoder
)
mistral
(decrecated, use
decoder
)
deepseek
(decrecated, use
decoder
)
qwen
(decrecated, use
decoder
)
whisper
(deprecated, part of a separate product line)
​
checkpoint_repository
Specification of the model checkpoint to be leveraged for engine building. E.g.
Copy
Ask AI
checkpoint_repository
:
source
:
HF | GCS | REMOTE_URL
repo
:
meta-llama/Llama-3.1-8B-Instruct | gs://bucket_name | https://your-checkpoint.com/model.tar.gz
To configure access to private model checkpoints,
register secrets in your Baseten workspace
, namely the
hf_access_token
or
trt_llm_gcs_service_account
secrets with a valid service account json for HuggingFace or GCS, respectively.
​
checkpoint_repository.source
Source where the checkpoint is stored. This should contain assets as if using git clone with lfs for a Hugging Face repository.
This includes the tokenizer files, remote code and safetensor files and any json file related to configuration.
For GCS/REMOTE_URL, we require the files to be organized in the same folder structured to a huggingface transformers repository.
Supported sources include:
HF
(HuggingFace)
GCS
(Google Cloud Storage)
REMOTE_URL
A tarball containing your checkpoint.
Important
: the archive must unpack with all required files (e.g.,
config.json
) at the root level. For example,
config.json
should be directly in the tarball, not nested under a subdirectory like
model_name/config.json
.
​
checkpoint_repository.repo
Checkpoint repository name, bucket, or url.
​
max_batch_size
(default:
256
)
Maximum number of input sequences to pass through the engine concurrently. Batch size and throughput share a direct relation, whereas batch size and single request latency share an indirect relation.
Tune this value according to your SLAs and latency budget.
​
max_seq_len
(default: max_position_embeddings from the model repo)
Defines the maximum sequence length (context) of single request​.
​
max_num_tokens
(default:
8192
)
Defines the maximum number of batched input tokens after padding is removed in each batch. Tuning this value more efficiently allocates memory to KV cache and executes more requests together.
​
max_prompt_embedding_table_size
(default:
0
)
Maximum prompt embedding table size for
prompt tuning
.
​
num_builder_gpus
(default:
auto
)
Number of GPUs to be used at build time, defaults to configured
resource.accelerator
count – useful for FP8 quantization in particular, when more GPU memory is required at build time relative to memory usage at inference.
​
plugin_configuration
Config for inserting plugin nodes into network graph definition for execution of user-defined kernels.
​
plugin_configuration.paged_kv_cache
(default:
True
)
Decompose KV cache into page blocks. Read more about what this does
here
.
​
plugin_configuration.use_paged_context_fmha
(default:
True
)
Utilize paged context for fused multihead attention. This configuration is necessary to enable KV cache reuse. Read more about this configuration
here
.
​
plugin_configuration.use_fp8_context_fmha
(default:
False
)
Utilize FP8 quantization for context fused multihead attention to accelerate attention. To use this configuration, also set
plugin_configuration.use_paged_context_fmha
. Read more about when to enable this
here
.
​
quantization_type
(default:
no_quant
)
Quantization format with which to build the engine. Supported formats include:
no_quant
(meaning bf16)
fp8
fp8_kv
The following quantization
smooth_quant
weights_int8
weights_kv_int8
weights_int4
weights_int4_kv_int8
Read more about different post training quantization techniques supported by TRT-LLM
here
.
Additionally, refer to the hardware and quantization technique
support matrix
.
​
strongly_typed
(default:
False
)
Whether to build the engine using strong typing, enabling TensorRT’s optimizer to statically infer intermediate tensor types which can speed up build time for some formats.
Weak typing enables the optimizer to elect tensor types, which may result in a faster runtime. For more information refer to TensorRT documentation
here
.
​
tensor_parallel_count
(default:
1
)
Tensor parallelism count. For more information refer to NVIDIA documentation
here
.
​
speculator
(default:
None
)
Config for inserting optional speculative decoding options.
​
Speculation with lookahead decoding
Speculation with lookahead decoding can be used by any model and does not require training.
The implemenation is based on the work at
lmsys.
We currently disallow performing structured generation and tool-calling with this optimization.
Copy
Ask AI
model_name
:
Llama-3.1-8B-Instruct (lookahead decoding)
resources
:
accelerator
:
H100
use_gpu
:
true
trt_llm
:
build
:
base_model
:
llama
checkpoint_repository
:
repo
:
meta-llama/Llama-3.1-8B-Instruct
source
:
HF
max_batch_size
:
32
quantization_type
:
fp8_kv
speculator
:
speculative_decoding_mode
:
LOOKAHEAD_DECODING
windows_size
:
7
ngram_size
:
5
verification_set_size
:
7
runtime
:
kv_cache_free_gpu_mem_fraction
:
0.62
​
Speculation with external draft model
Speculative decoding with draft models (e.g., using DRAFT_TOKENS_EXTERNAL) is
an advanced feature that requires careful GPU memory allocation to accommodate
both models simultaneously. If you plan to use this technique, consider
consulting the Baseten support team for guidance on optimal configuration.
Speculative draft model configuration to be used for speculative decoding. By default, the speculator build will attempt to reuse as much of the target model build configuration.
To fully specify your own speculator build, define
speculator.build
.
For example, here is a sample configuration for utilizing speculative decoding for Llama-3-70B-Instruct:
Copy
Ask AI
model_name
:
Llama-3.1-70B-Instruct (External Token Spec-Dec)
resources
:
accelerator
:
H100
use_gpu
:
true
trt_llm
:
build
:
base_model
:
qwen
checkpoint_repository
:
repo
:
meta-llama/Llama-3.3-8B-Instruct
source
:
HF
max_batch_size
:
32
quantization_type
:
fp8_kv
tensor_parallel_count
:
2
speculator
:
speculative_decoding_mode
:
DRAFT_TOKENS_EXTERNAL
checkpoint_repository
:
repo
:
meta-llama/Llama-3.2-1B-Instruct
source
:
HF
num_draft_tokens
:
4
runtime
:
kv_cache_free_gpu_mem_fraction
:
0.62
​
speculator.speculative_decoding_mode
The type of speculative decoding tactic. Supported are:
“DRAFT_TOKENS_EXTERNAL”
“LOOKAHEAD_DECODING” (recommend)
​
speculator.num_draft_tokens
Number of draft tokens to sample from the speculative model. This depends on how many tokens are expected to be accepted, a good range of values to start with are between 2-8.
Automatically calculated field for lookahead decoding.
​
speculator.checkpoint_repository
See
checkpoint_repository
for details.
​
speculator.lookahead_ngram_size
,
speculator.lookahead_windows_size
,
speculator.lookahead_verification_set_size
Usage of ngram size, window size, verification_set_size in the lookahead algorithm.
windows_size
is the Jacobi window size, meaning number of n-grams in lookahead branch that explores future draft tokens.
ngram_size
is the n-gram size, meaning the maximum number of draft tokens accepted per iteration.
verification_set_size
is the maximum number of n-grams considered for verification, meaning the number of draft token beam hypotheses.
A good default value could be [5,5,5]. Often, lookahead_verification_set_size is set to lookahead_windows_size.
lookahead_ngram_size
is often increased when the generated tokens are simlar to contents of the prompt, and decreased if dissimilar.
​
lora_adapters
(default:
None
)
A mapping from LoRA names to checkpoint repositories.
For example,
Copy
Ask AI
checkpoint_repository
:
repo
:
meta-llama/Llama-2-13b-hf
source
:
HF
lora_adapters
:
lora1
:
repo
:
hfl/chinese-llama-2-lora-13b
source
:
HF
See
checkpoint_repository
for details on how to configure checkpoint repositories.
In addition to specifying the LoRAs here, you need to specify the
served_model_name
that is used to refer to the base model.
The
served_model_name
is required for deploying LoRAs.
The LoRA name (in the example above, this is “lora1”) is used to query the model using the specified LoRA.
​
trt_llm.runtime
TRT-LLM engine runtime configuration.
​
kv_cache_free_gpu_mem_fraction
(default:
0.9
)
Used to control the fraction of free gpu memory allocated for the KV cache. For more information, refer to the documentation
here
.
If you are using DRAFT_TOKENS_EXTERNAL, we recommend to lower this, depending on the draft model size.
​
enable_chunked_context
(default:
False
)
Enables chunked context, increasing the chance of batch processing between context and generation phase – which may be useful to increase throughput.
Note that one must set
plugin_configuration.use_paged_context_fmha: True
in order to leverage this feature.
​
batch_scheduler_policy
(default:
guaranteed_no_evict
)
Supported scheduler policies are as follows:
guaranteed_no_evict
max_utilization
guaranteed_no_evict
ensures that an in progress request is never evicted by reserving KV cache space for the maximum possible tokens that can be returned for a request.
max_utilization
packs as many requests as possible during scheduling, which may increase throughput at the expense of additional latency.
For more information refer to the NVIDIA documentation
here
.
​
request_default_max_tokens
(default:
None
)
Default server configuration for the maximum number of tokens to generate for a single sequence, if one is not provided in the request body.
Sensible settings depend on your use case, a general value to set can be around 1000 tokens.
​
served_model_name
(default:
None
)
The name used to refer to the base model when using LoRAs.
At least one LoRA must be specified under
lora_adapters
to use LoRAs.
Was this page helpful?
Yes
No
Previous
Security and secrets
Using secrets securely in your ML models
Next
On this page
trt_llm.build
base_model
checkpoint_repository
checkpoint_repository.source
checkpoint_repository.repo
max_batch_size
max_seq_len
max_num_tokens
max_prompt_embedding_table_size
num_builder_gpus
plugin_configuration
plugin_configuration.paged_kv_cache
plugin_configuration.use_paged_context_fmha
plugin_configuration.use_fp8_context_fmha
quantization_type
strongly_typed
tensor_parallel_count
speculator
Speculation with lookahead decoding
Speculation with external draft model
speculator.speculative_decoding_mode
speculator.num_draft_tokens
speculator.checkpoint_repository
speculator.lookahead_ngram_size, speculator.lookahead_windows_size, speculator.lookahead_verification_set_size
lora_adapters
trt_llm.runtime
kv_cache_free_gpu_mem_fraction
enable_chunked_context
batch_scheduler_policy
request_default_max_tokens
served_model_name
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/development/model/performance/engine-builder-customization:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Overview
Your first model
Setup and dependencies
Implementation
Implementation (Advanced)
Deploy and iterate
Performance optimization
Concepts
Engine builder overview
Engine control in Python
Engine builder configuration
Security and secrets
Data and storage
Python driven configuration for models 🆕
Developing a Chain
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Exporting metrics
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Performance optimization
Engine control in Python
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
When you create a new Truss with
truss init
, it creates two files:
config.yaml
and
model/model.py
. While you configure the Engine Builder in
config.yaml
, you may use
model/model.py
to access and control the engine object during inference.
You have two options:
Delete the
model/model.py
file and your TensorRT-LLM engine will run according to its base spec.
Update the code to support TensorRT-LLM.
You must either update
model/model.py
to pass
trt_llm
as an argument to the
__init__
method OR delete the file. Otherwise you will get an error on deployment as the default
model/model.py
file is not written for TensorRT-LLM.
The
engine
object is a property of the
trt_llm
argument and must be initialized in
__init__
to be accessed in
load()
(which runs once on server start-up) and
predict()
(which runs for each request handled by the server).
This example applies a chat template with the Llama 3.1 8B tokenizer to the model prompt:
model/model.py
Copy
Ask AI
import
orjson
# faster serialization/deserialization than built-in json
from
typing
import
Any, AsyncIterator
from
transformers
import
AutoTokenizer
from
fastapi.responses
import
StreamingResponse
SSE_PREFIX
=
"data: "
class
Model
:
def
__init__
(
self
,
trt_llm
,
**
kwargs
) ->
None
:
self
._secrets
=
kwargs[
"secrets"
]
self
._engine
=
trt_llm[
"engine"
]
self
._model
=
None
self
._tokenizer
=
None
def
load
(
self
) ->
None
:
self
._tokenizer
=
AutoTokenizer.from_pretrained(
"meta-llama/Llama-3.1-8B-Instruct"
,
token
=
self
._secrets[
"hf_access_token"
])
async
def
predict
(
self
,
model_input
: Any) -> Any:
# Apply chat template to prompt
model_input[
"prompt"
]
=
self
._tokenizer.apply_chat_template(model_input[
"prompt"
],
tokenize
=
False
)
response
=
await
self
._engine.predict(model_input)
# If the response is streaming, post-process each chunk
if
isinstance
(response, StreamingResponse):
token_gen
=
response.body_iterator
async
def
processed_stream
():
async
for
chunk
in
some_post_processing_function(token_gen):
yield
chunk
return
StreamingResponse(processed_stream(),
media_type
=
"text/event-stream"
)
# Otherwise, return the raw output
else
:
return
response
# --- Post-processing helpers for SSE ---
def
parse_sse_chunk
(
chunk
:
bytes
) ->
dict
|
None
:
"""Parses an SSE-formatted chunk and returns the JSON payload."""
try
:
text
=
chunk.decode(
"utf-8"
).strip()
if
not
text.startswith(
SSE_PREFIX
):
return
None
return
orjson.loads(text[
len
(
SSE_PREFIX
):])
except
Exception
:
return
None
def
format_sse_chunk
(
payload
:
dict
) ->
bytes
:
"""Formats a JSON payload back into an SSE chunk."""
return
f
"
{
SSE_PREFIX
}
"
.encode(
"utf-8"
)
+
orjson.dumps(payload)
+
b
"
\n\n
"
def
transform_payload
(
payload
:
dict
) ->
dict
:
"""Add a new field to the SSE payload."""
payload[
"my_new_field"
]
=
"my_new_value"
return
payload
async
def
some_post_processing_function
(
token_gen
: AsyncIterator[
bytes
]
) -> AsyncIterator[
bytes
]:
"""Post-process each SSE chunk in the stream."""
async
for
chunk
in
token_gen:
payload
=
parse_sse_chunk(chunk)
if
payload
is
None
:
yield
chunk
continue
transformed
=
transform_payload(payload)
yield
format_sse_chunk(transformed)
Was this page helpful?
Yes
No
Previous
Engine builder configuration
Configure your TensorRT-LLM inference engine
Next
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/development/model/performance/engine-builder-overview:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Overview
Your first model
Setup and dependencies
Implementation
Implementation (Advanced)
Deploy and iterate
Performance optimization
Concepts
Engine builder overview
Engine control in Python
Engine builder configuration
Security and secrets
Data and storage
Python driven configuration for models 🆕
Developing a Chain
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Exporting metrics
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Performance optimization
Engine builder overview
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
If you have a foundation model like Llama 3 or a fine-tuned variant and want to create a low-latency, high-throughput model inference server, TensorRT-LLM via the Engine Builder is likely the tool for you.
TensorRT-LLM is an open source performance optimization toolbox created by NVIDIA. It helps you build TensorRT engines for large language models like Llama and Mistral as well as certain other models like Whisper and large vision models.
Baseten’s TensorRT-LLM Engine Builder simplifies and automates the process of using TensorRT-LLM for development and production. All you need to do is write a few lines of configuration and an optimized model serving engine will be built automatically during the model deployment process.
​
FAQs
​
Where are the engines stored?
The engines are stored in Baseten but owned by the user — we’re working on a mechanism for downloading them. In the meantime, reach out if you need access to an engine that you created using the Engine Builder.
​
Does the Engine Builder support quantization?
Yes. The Engine Builder can perform post-training quantization during the building process. For supported options, see
quantization in the config reference
.
​
Can I customize the engine behavior?
For further control over the TensorRT-LLM engine during inference, use the
model/model.py
file to access the engine object at runtime. See
controlling engines with Python
for details.
Was this page helpful?
Yes
No
Previous
Engine control in Python
Use `model.py` to customize engine behavior
Next
On this page
FAQs
Where are the engines stored?
Does the Engine Builder support quantization?
Can I customize the engine behavior?
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/development/model/private-registries:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Overview
Your first model
Setup and dependencies
Configuration
Custom build commands
Base Docker images
Private Docker Registries
Implementation
Implementation (Advanced)
Deploy and iterate
Performance optimization
Security and secrets
Data and storage
Python driven configuration for models 🆕
Developing a Chain
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Exporting metrics
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Setup and dependencies
Private Docker Registries
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
Truss uses containerized environments to ensure consistent model execution across deployments. When deploying a custom base image or a custom server from a private registry, you must grant Baseten access to download that image.
​
AWS Elastic Cloud Registry (ECR)
AWS supports using either
service accounts
, or
access tokens
for short lived access for container registry authentication.
​
AWS IAM Service accounts
To use an IAM service account for long-lived access, you can use the
AWS_IAM
authentication method in Truss.
Get an AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY from the AWS dashboard
Add these as
secrets
in Baseten. These should be named
aws_access_key_id
and
aws_secret_access_key
respectively.
Choose the
AWS_IAM
authentication method when setting up your Truss. The
config.yaml
file should look something like this:
Copy
Ask AI
...
base_image:
image: <aws account id>.dkr.ecr.<region>.amazonaws.com/path/to/image
docker_auth:
auth_method: AWS_IAM
registry: <aws account id>.dkr.ecr.<region>.amazonaws.com
secrets:
aws_access_key_id: null
aws_secret_access_key: null
...
Note here that you need to specify the registry and image separately.
If you’d like to use different secret names besides the default, you can configure these using the
aws_access_key_id_secret_name
and
aws_secret_access_key_secret_name
options
under
docker_auth
:
Copy
Ask AI
...
base_image:
...
docker_auth:
auth_method: AWS_IAM
registry: <aws account id>.dkr.ecr.<region>.amazonaws.com
aws_access_key_id_secret_name: custom_aws_access_key_secret
aws_secret_access_key_secret_name: custom_aws_secret_key_secret
secrets:
custom_aws_access_key_secret: null
custom_aws_secret_key_secret: null
​
Access Token
Get the a
Base64-encoded
secret:
Copy
Ask AI
PASSWORD
=
`
aws
ecr get-login-password
--region
{us-east-1}`
echo
-n
"AWS:
$PASSWORD
"
|
base64
Add a new
secret
to Baseten named
DOCKER_REGISTRY_{aws account id}.dkr.ecr.{us-east-1}.amazonaws.com
with the
Base64-encoded secret
as the value.
Add the secret name to the
secrets
section of the
config.yaml
to allow this model to access the secret when it is pushed.
config.yaml
Copy
Ask AI
secrets
:
DOCKER_REGISTRY_{aws account id}.dkr.ecr.{us-east-1}.amazonaws.com
:
null
​
Google Cloud Artifact Registry
GCP supports using either
access tokens
for short lived access or
service accounts
for container registry authentication.
​
Service Account
Get your
service account key
as a JSON key blob.
Add a new
secret
to Baseten named
gcp-service-account
(or similar) with the JSON key blob as the value.
Add the secret name that you used to the
secrets
section of the
config.yaml
to allow this model to access the secret when it is pushed.
config.yaml
Copy
Ask AI
secrets
:
gcp-service-account
:
null
Configure the
docker_auth
section of your
base_image:
to ensure that the service account authentication method will be used.
Copy
Ask AI
base_image:
...
docker_auth:
auth_method: GCP_SERVICE_ACCOUNT_JSON
secret_name: gcp-service-account
registry: {us-west2}-docker.pkg.dev
Note that here,
secret_name
should match the name of the secret that is contains the JSON key blob.
​
Access Token
Get your
access token
Add a new
secret
to Baseten named
DOCKER_REGISTRY_{us-west2}-docker.pkg.dev
with the
Base64-encoded secret
as the value.
Add the secret name to the
secrets
section of the
config.yaml
to allow this model to access the secret when it is pushed.
config.yaml
Copy
Ask AI
secrets
:
DOCKER_REGISTRY_{us-west2}-docker.pkg.dev
:
null
​
Docker Hub
Get the a
Base64-encoded
secret:
Copy
Ask AI
echo
-n
'username:password'
|
base64
Add a new
secret
to Baseten named
DOCKER_REGISTRY_https://index.docker.io/v1/
with the
Base64-encoded secret
as the value.
Add the secret name to the
secrets
section of the
config.yaml
to allow this model to access the secret when it is pushed.
Copy
Ask AI
Name: DOCKER_REGISTRY_https://index.docker.io/v1/
Token: <Base64-encoded secret>
Then, this to
config.yaml
:
config.yaml
Copy
Ask AI
secrets
:
DOCKER_REGISTRY_https://index.docker.io/v1/
:
null
Was this page helpful?
Yes
No
Previous
Implementation
How to implement your model.
Next
On this page
AWS Elastic Cloud Registry (ECR)
AWS IAM Service accounts
Access Token
Google Cloud Artifact Registry
Service Account
Access Token
Docker Hub
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/development/model/requests:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Overview
Your first model
Setup and dependencies
Implementation
Implementation (Advanced)
Request handling
Custom Responses
Custom servers
Custom health checks 🆕
Cached weights 🆕
Access model environments
Request concurrency
Streaming output
Deploy and iterate
Performance optimization
Security and secrets
Data and storage
Python driven configuration for models 🆕
Developing a Chain
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Exporting metrics
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Implementation (Advanced)
Using request objects / cancellation
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
Truss processes client requests by extracting and validating payloads. For
advanced use cases
, you can access the raw request object to:
Customize payload deserialization
(e.g., binary protocol buffers).
Handle disconnections & cancel long-running predictions.
You can mix request objects with standard inputs or use requests exclusively for performance optimization.
​
Using Request Objects in Truss
You can define request objects in
preprocess
,
predict
, and
postprocess
:
Copy
Ask AI
import
fastapi
class
Model
:
def
preprocess
(
self
,
request
: fastapi.Request):
...
def
predict
(
self
,
inputs
,
request
: fastapi.Request):
...
def
postprocess
(
self
,
inputs
,
request
: fastapi.Request):
...
​
Rules for Using Requests
The request must be
type-annotated
as
fastapi.Request
.
If
only
requests are used, Truss
skips payload extraction
for better performance.
If
both
request objects and standard inputs are used:
Request
must be the second argument
.
Preprocessing transforms inputs
, but the request object remains unchanged.
postprocess
cannot use only the request—it must receive the model’s output.
If
predict
only uses the request,
preprocess
cannot be used.
Copy
Ask AI
import
fastapi, asyncio, logging
class
Model
:
async
def
predict
(
self
,
inputs
,
request
: fastapi.Request):
await
asyncio.sleep(
1
)
if
await
request.is_disconnected():
logging.warning(
"Cancelled before generation."
)
return
# Cancel request on the model engine here.
for
i
in
range
(
5
):
await
asyncio.sleep(
1.0
)
logging.warning(i)
yield
str
(i)
# Streaming response
if
await
request.is_disconnected():
logging.warning(
"Cancelled during generation."
)
return
# Cancel request on the model engine here.
You must implement request cancellation at the model level, which varies by framework.
​
Cancelling Requests in Specific Frameworks
​
TRT-LLM (Polling-Based Cancellation)
For
TensorRT-LLM
, use
response_iterator.cancel()
to terminate streaming requests:
Copy
Ask AI
async
for
request_output
in
response_iterator:
if
await
is_cancelled_fn():
logging.info(
"Request cancelled. Cancelling Triton request."
)
response_iterator.cancel()
return
See full example in
TensorRT-LLM Docs
.
​
vLLM (Abort API)
For
vLLM
, use
engine.abort()
to stop processing:
Copy
Ask AI
async
for
request_output
in
results_generator:
if
await
request.is_disconnected():
await
engine.abort(request_id)
return
See full example in
vLLM Docs
.
​
Unsupported Request Features
Streaming file uploads
– Use URLs instead of embedding large data in the request.
Client-side headers
– Most headers are stripped; include necessary metadata in the payload.
Was this page helpful?
Yes
No
Previous
Custom Responses
Get more control by directly creating the response object.
Next
On this page
Using Request Objects in Truss
Rules for Using Requests
Cancelling Requests in Specific Frameworks
TRT-LLM (Polling-Based Cancellation)
vLLM (Abort API)
Unsupported Request Features
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/development/model/responses:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Overview
Your first model
Setup and dependencies
Implementation
Implementation (Advanced)
Request handling
Custom Responses
Custom servers
Custom health checks 🆕
Cached weights 🆕
Access model environments
Request concurrency
Streaming output
Deploy and iterate
Performance optimization
Security and secrets
Data and storage
Python driven configuration for models 🆕
Developing a Chain
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Exporting metrics
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Implementation (Advanced)
Custom Responses
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
By default, Truss wraps prediction results into an HTTP response. For
advanced use cases
, you can create response objects manually to:
Control HTTP status codes.
Use server-sent events (SSEs) for streaming responses.
You can return a response from predict or postprocess, but not both.
​
Returning Custom Response Objects
Any subclass of starlette.responses.Response is supported.
Copy
Ask AI
import
fastapi
class
Model
:
def
predict
(
self
,
inputs
) -> fastapi.Response:
return
fastapi.Response(
...
)
If
predict
returns a response,
postprocess
cannot be used.
​
Example: Streaming with SSEs
For
server-sent events (SSEs)
, use
StreamingResponse
:
Copy
Ask AI
import
time
from
starlette.responses
import
StreamingResponse
class
Model
:
def
predict
(
self
,
model_input
):
def
event_stream
():
while
True
:
time.sleep(
1
)
yield
f
"data: Server Time:
{
time.strftime(
'%Y-%m-
%d
%H:%M:%S'
)
}
\n\n
"
return
StreamingResponse(event_stream(),
media_type
=
"text/event-stream"
)
​
Limitations
Response headers are not fully propagated
– include metadata in the response body.
Also see
Using Request Objects
for handling raw requests.
Was this page helpful?
Yes
No
Previous
Custom servers
A config.yaml is all you need
Next
On this page
Returning Custom Response Objects
Example: Streaming with SSEs
Limitations
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/development/model/secrets:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Overview
Your first model
Setup and dependencies
Implementation
Implementation (Advanced)
Deploy and iterate
Performance optimization
Security and secrets
Data and storage
Python driven configuration for models 🆕
Developing a Chain
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Exporting metrics
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Developing a model
Security and secrets
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
Truss allows you to securely manage
API keys
,
access tokens
,
passwords
,
and other secrets
without exposing them in code.
​
1. Define Secrets in
config.yaml
Add secrets with
placeholder
values in
config.yaml
:
Copy
Ask AI
secrets
:
hf_access_token
:
null
Never store actual secret values in
config.yaml
. Store secrets in the
workspace settings
.
​
2. Access Secrets in
model.py
Secrets are passed as
keyword arguments
to the
Model
class:
Copy
Ask AI
def
__init__
(
self
,
**
kwargs
):
self
._secrets
=
kwargs[
"secrets"
]
Use secrets inside load or predict:
Copy
Ask AI
def
load
(
self
):
self
._model
=
pipeline(
"fill-mask"
,
model
=
"baseten/docs-example-gated-model"
,
use_auth_token
=
self
._secrets[
"hf_access_token"
]
)
​
3. Store Secrets on Your Remote
On
Baseten
, add secrets in the
workspace settings
.
Use the
exact name
from
config.yaml
(case-sensitive).
​
4. Deploying with Secrets
By default, models have access to any secrets on a workspace.
Was this page helpful?
Yes
No
Previous
Data and storage
Load model weights without Hugging Face or S3
Next
On this page
1. Define Secrets in config.yaml
2. Access Secrets in model.py
3. Store Secrets on Your Remote
4. Deploying with Secrets
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/development/model/streaming:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Overview
Your first model
Setup and dependencies
Implementation
Implementation (Advanced)
Request handling
Custom Responses
Custom servers
Custom health checks 🆕
Cached weights 🆕
Access model environments
Request concurrency
Streaming output
Deploy and iterate
Performance optimization
Security and secrets
Data and storage
Python driven configuration for models 🆕
Developing a Chain
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Exporting metrics
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Implementation (Advanced)
Streaming output
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
Streaming output significantly reduces wait time for generative AI models by returning results as they are generated instead of waiting for the full response.
​
Why Streaming?
✅
Faster response time
– Get initial results in under
1 second
instead of waiting
10+ seconds
.
✅
Improved user experience
– Partial outputs are
immediately usable
.
This guide walks through
deploying Falcon 7B
with streaming enabled.
​
1. Initialize Truss
Copy
Ask AI
truss
init
falcon-7b
&&
cd
falcon-7b
​
2: Implement Model (Non-Streaming)
This first version loads the Falcon 7B model
without
streaming:
model/model.py
Copy
Ask AI
import
torch
from
transformers
import
AutoTokenizer, AutoModelForCausalLM, GenerationConfig
from
typing
import
Dict
CHECKPOINT
=
"tiiuae/falcon-7b-instruct"
DEFAULT_MAX_NEW_TOKENS
=
150
DEFAULT_TOP_P
=
0.95
class
Model
:
def
__init__
(
self
,
**
kwargs
) ->
None
:
self
.tokenizer
=
None
self
.model
=
None
def
load
(
self
):
self
.tokenizer
=
AutoTokenizer.from_pretrained(
CHECKPOINT
)
self
.model
=
AutoModelForCausalLM.from_pretrained(
CHECKPOINT
,
torch_dtype
=
torch.bfloat16,
trust_remote_code
=
True
,
device_map
=
"auto"
)
def
predict
(
self
,
request
: Dict) -> Dict:
prompt
=
request[
"prompt"
]
inputs
=
self
.tokenizer(prompt,
return_tensors
=
"pt"
,
max_length
=
512
,
truncation
=
True
,
padding
=
True
)
input_ids
=
inputs[
"input_ids"
].to(
"cuda"
)
generation_config
=
GenerationConfig(
temperature
=
1
,
top_p
=
DEFAULT_TOP_P
,
top_k
=
40
)
with
torch.no_grad():
return
self
.model.generate(
input_ids
=
input_ids,
generation_config
=
generation_config,
return_dict_in_generate
=
True
,
output_scores
=
True
,
pad_token_id
=
self
.tokenizer.eos_token_id,
max_new_tokens
=
DEFAULT_MAX_NEW_TOKENS
,
)
​
3. Add Streaming Support
To enable streaming, we:
Use
TextIteratorStreamer
to stream tokens as they are generated.
Run
generate()
in a
separate thread
to prevent blocking.
Return a
generator
that streams results.
model/model.py
Copy
Ask AI
import
torch
from
transformers
import
AutoTokenizer, AutoModelForCausalLM, GenerationConfig, TextIteratorStreamer
from
threading
import
Thread
from
typing
import
Dict
CHECKPOINT
=
"tiiuae/falcon-7b-instruct"
class
Model
:
def
__init__
(
self
,
**
kwargs
) ->
None
:
self
.tokenizer
=
None
self
.model
=
None
def
load
(
self
):
self
.tokenizer
=
AutoTokenizer.from_pretrained(
CHECKPOINT
)
self
.model
=
AutoModelForCausalLM.from_pretrained(
CHECKPOINT
,
torch_dtype
=
torch.bfloat16,
trust_remote_code
=
True
,
device_map
=
"auto"
)
def
predict
(
self
,
request
: Dict):
prompt
=
request[
"prompt"
]
inputs
=
self
.tokenizer(prompt,
return_tensors
=
"pt"
,
max_length
=
512
,
truncation
=
True
,
padding
=
True
)
input_ids
=
inputs[
"input_ids"
].to(
"cuda"
)
streamer
=
TextIteratorStreamer(
self
.tokenizer)
generation_config
=
GenerationConfig(
temperature
=
1
,
top_p
=
0.95
,
top_k
=
40
)
def
generate
():
self
.model.generate(
input_ids
=
input_ids,
generation_config
=
generation_config,
return_dict_in_generate
=
True
,
output_scores
=
True
,
pad_token_id
=
self
.tokenizer.eos_token_id,
max_new_tokens
=
150
,
streamer
=
streamer,
)
thread
=
Thread(
target
=
generate)
thread.start()
def
stream_output
():
for
text
in
streamer:
yield
text
thread.join()
return
stream_output()
​
4. Configure
config.yaml
config.yaml
Copy
Ask AI
model_name
:
falcon-streaming
requirements
:
-
torch==2.0.1
-
peft==0.4.0
-
scipy==1.11.1
-
sentencepiece==0.1.99
-
accelerate==0.21.0
-
bitsandbytes==0.41.1
-
einops==0.6.1
-
transformers==4.31.0
resources
:
cpu
:
"3"
memory
:
14Gi
use_gpu
:
true
accelerator
:
A10G
​
5. Deploy & Invoke
Deploy the model:
Copy
Ask AI
truss
push
Invoke with:
Copy
Ask AI
truss
predict
-d
'{"prompt": "Tell me about falcons", "do_sample": true}'
Was this page helpful?
Yes
No
Previous
Deploy and iterate
Deploy your model and quickly iterate on it.
Next
On this page
Why Streaming?
1. Initialize Truss
2: Implement Model (Non-Streaming)
3. Add Streaming Support
4. Configure config.yaml
5. Deploy & Invoke
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/examples/bei:

Baseten
home page
Search...
⌘
K
Examples
Overview
Deploy your first model
Fast LLMs with TensorRT-LLM
Run any LLM with vLLM
Deploy LLMs with SGLang
RAG pipeline with Chains
Transcribe audio with Chains
Image generation
Deploy a ComfyUI project
Embeddings with BEI
Dockerized model
LLM with Streaming
Text to speech
Model library
Overview
Deepseek
Llama
Qwen
Gemma
Stable Diffusion
Flux
Kokoro
Microsoft
Nomic
Whisper
Mars
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Examples
Embeddings with BEI
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
Baseten Embeddings Inference is Baseten’s solution for production grade inference on embedding, classification and reranking models using TensorRT-LLM.
With Baseten Embeddings Inference you get the following benefits:
Lowest-latency inference across any embedding solution (vLLM, SGlang, Infinity, TEI, Ollama)
1
Highest-throughput inference across any embedding solution (vLLM, SGlang, Infinity, TEI, Ollama) - thanks to XQA kernels, FP8 and dynamic batching.
2
High parallelism: up to 1400 client embeddings per second
Cached model weights for fast vertical scaling and high availability - no Hugging Face hub dependency at runtime
Ahead-of-time compilation, memory allocation and fp8 post-training quantization
​
Getting started with embedding models:
Embedding models are LLMs without a lm_head for language generation.
Typical architectures that are supported for embeddings are
LlamaModel
,
BertModel
,
RobertaModel
or
Gemma2Model
, and contain the safetensors, config, tokenizer and sentence-transformer config files.
A good example is the repo
BAAI/bge-multilingual-gemma2
.
To deploy a model for embeddings, set the following config in your local directory.
config.yaml
Copy
Ask AI
model_name
:
BEI-Linq-Embed-Mistral
resources
:
accelerator
:
H100_40GB
use_gpu
:
true
trt_llm
:
build
:
base_model
:
encoder
checkpoint_repository
:
# for a different model, change the repo to e.g. to "Salesforce/SFR-Embedding-Mistral"
# "BAAI/bge-en-icl" or "BAAI/bge-m3"
repo
:
"Linq-AI-Research/Linq-Embed-Mistral"
revision
:
main
source
:
HF
# only Llama, Mistral and Qwen Models support quantization.
# others, use: "quantization_type: no_quant"
quantization_type
:
fp8
With
config.yaml
in your local directory, you can deploy the model to Baseten.
Copy
Ask AI
truss
push
--publish
--promote
Deployed embedding models are OpenAI compatible without any additional settings.
You may use the client code below to consume the model.
Copy
Ask AI
from
openai
import
OpenAI
import
os
client
=
OpenAI(
api_key
=
os.environ[
'BASETEN_API_KEY'
],
# add the deployment URL
base_url
=
"https://model-xxxxxx.api.baseten.co/environments/production/sync/v1"
)
embedding
=
client.embeddings.create(
input
=
[
"Baseten Embeddings are fast."
,
"Embed this sentence!"
],
model
=
"not-required"
)
​
Example deployment of a classification, reranking and classification models
Besides embedding models, BEI deploys high-throughput rerank and classification models.
You can identify suitable architectures by their
ForSequenceClassification
suffix in the huggingface repo.
The use-case for these models is either Reward Modeling, Reranking documents in RAG or tasks like content moderation.
Copy
Ask AI
model_name
:
BEI-mixedbread-rerank-large-v2-fp8
resources
:
accelerator
:
H100_40GB
cpu
:
'1'
memory
:
10Gi
use_gpu
:
true
trt_llm
:
build
:
base_model
:
encoder
checkpoint_repository
:
repo
:
michaelfeil/mxbai-rerank-large-v2-seq
revision
:
main
source
:
HF
# only Llama, Mistral and Qwen Models support quantization
quantization_type
:
fp8
As OpenAI does not offer reranking or classification, we are sending a simple request to the endpoint.
Depending on the model, you might want to apply a specific prompt template first.
Copy
Ask AI
import
requests
import
os
headers
=
{
f
"Authorization"
:
f
"Api-Key
{
os.environ[
'BASETEN_API_KEY'
]
}
"
}
# model specific prompt for mixedbread's reranker v2.
prompt
=
(
"<|endoftext|><|im_start|>system
\n
You are Qwen, created by Alibaba Cloud. You are a helpful assistant.
\n
<|im_end|>
\n
<|im_start|>user
\n
"
"query:
{query}
\n
document:
{doc}
\n
You are a search relevance expert who evaluates how well documents match search queries. For each query-document pair, carefully analyze the semantic relationship between them, then provide your binary relevance judgment (0 for not relevant, 1 for relevant).
\n
Relevance:<|im_end|>
\n
<|im_start|>assistant
\n
"
).format(
query
=
"What is Baseten?"
,
doc
=
"Baseten is a fast inference provider."
)
requests.post(
headers
=
headers,
url
=
"https://model-xxxxxx.api.baseten.co/environments/production/sync/predict"
,
json
=
{
"inputs"
: prompt,
"raw_scores"
:
True
,
}
)
​
Benchmarks and Performance optimizations
Embedding models on BEI are fast, and offer currently the fastest implementation for embeddings across all open-source and closed-source providers.
The team behind the implementation are the authors of
infinity
.
We recommend using fp8 quantization for LLama, Mistral and Qwen2 models on L4 or newer (L4, H100, H200 and B200).
Quality difference between fp8 and bfloat16 is often negligible - embedding models often retentain of >99% cosine simalarity between both presisions,
and reranking models retain the ranking order - despite a difference in the retained output.
For more details, check out the
technical launch post
.
The team at Baseten has additional options for sharing cached model weights across replicas - for very fast horizontal scaling.
Please contact us to enable this option.
​
Deploy custom or fine-tuned models on BEI:
We support the deployment of of the below models, as well all finetuned variants of these models (same architecture & customized weights).
The following repositories are supported - this list is not exhaustive.
Model Repository
Architecture
Function
Salesforce/SFR-Embedding-Mistral
MistralModel
embedding
BAAI/bge-m3
BertModel
embedding
BAAI/bge-multilingual-gemma2
Gemma2Model
embedding
mixedbread-ai/mxbai-embed-large-v1
BertModel
embedding
BAAI/bge-large-en-v1.5
BertModel
embedding
allenai/Llama-3.1-Tulu-3-8B-RM
LlamaForSequenceClassification
classifier
ncbi/MedCPT-Cross-Encoder
BertForSequenceClassification
reranker/classifier
SamLowe/roberta-base-go_emotions
XLMRobertaForSequenceClassification
classifier
mixedbread/mxbai-rerank-large-v2-seq
Qwen2ForSequenceClassification
reranker/classifier
BAAI/bge-en-icl
LlamaModel
embedding
BAAI/bge-reranker-v2-m3
BertForSequenceClassification
reranker/classifier
Skywork/Skywork-Reward-Llama-3.1-8B-v0.2
LlamaForSequenceClassification
classifier
Snowflake/snowflake-arctic-embed-l
BertModel
embedding
nomic-ai/nomic-embed-code
Qwen2Model
embedding
1
measured on H100-HBM3 (bert-large-335M, for BAAI/bge-en-icl: 9ms)
2
measured on H100-HBM3 (leading model architecture on MTEB, MistralModel-7B)
Was this page helpful?
Yes
No
Previous
Dockerized model
Deploy any model in a pre-built Docker container
Next
On this page
Getting started with embedding models:
Example deployment of a classification, reranking and classification models
Benchmarks and Performance optimizations
Deploy custom or fine-tuned models on BEI:
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/examples/chains-audio-transcription:

Baseten
home page
Search...
⌘
K
Examples
Overview
Deploy your first model
Fast LLMs with TensorRT-LLM
Run any LLM with vLLM
Deploy LLMs with SGLang
RAG pipeline with Chains
Transcribe audio with Chains
Image generation
Deploy a ComfyUI project
Embeddings with BEI
Dockerized model
LLM with Streaming
Text to speech
Model library
Overview
Deepseek
Llama
Qwen
Gemma
Stable Diffusion
Flux
Kokoro
Microsoft
Nomic
Whisper
Mars
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Examples
Transcribe audio with Chains
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
View example on GitHub
This guide walks through building an audio transcription pipeline using Chains. You’ll break down large media files, distribute transcription tasks across autoscaling deployments, and leverage high-performance GPUs for rapid inference.
​
1. Overview
This Chain enables fast, high-quality transcription by:
Partitioning
long files (10+ hours) into smaller segments.
Detecting silence
to optimize split points.
Parallelizing inference
across multiple GPU-backed deployments.
Batching requests
to maximize throughput.
Using range downloads
for efficient data streaming.
Leveraging
asyncio
for concurrent execution.
​
2. Chain Structure
Transcription is divided into two processing layers:
Macro chunks:
Large segments (~300s) split from the source media file. These are processed in parallel to handle massive files efficiently.
Micro chunks:
Smaller segments (~5–30s) extracted from macro chunks and sent to the Whisper model for transcription.
​
3. Implementing the Chainlets
​
Transcribe
(Entrypoint Chainlet)
Handles transcription requests and dispatches tasks to worker Chainlets.
Function signature:
Copy
Ask AI
async
def
run_remote
(
self
,
media_url
:
str
,
params
: data_types.TranscribeParams
) -> data_types.TranscribeOutput:
Steps:
Validates that the media source supports
range downloads
.
Uses
FFmpeg
to extract metadata and duration.
Splits the file into
macro chunks
, optimizing split points at silent sections.
Dispatches
macro chunk tasks
to the MacroChunkWorker for processing.
Collects
micro chunk transcriptions
, merges results, and returns the final text.
Example request:
Copy
Ask AI
curl
-X
POST
$INVOCATION_URL
\
-H
"Authorization: Api-Key
$BASETEN_API_KEY
"
\
-d
'<JSON_INPUT>'
Copy
Ask AI
{
"media_url"
:
"http://commondatastorage.googleapis.com/gtv-videos-bucket/sample/TearsOfSteel.mp4"
,
"params"
: {
"micro_chunk_size_sec"
:
30
,
"macro_chunk_size_sec"
:
300
}
}
​
MacroChunkWorker
(Processing Chainlet)
Processes
macro chunks
by:
Extracting
relevant time segments using
FFmpeg
.
Streaming audio
instead of downloading full files for low latency.
Splitting segments
at silent points.
Encoding
audio in base64 for efficient transfer.
Distributing micro chunks
to the Whisper model for transcription.
This Chainlet
runs in parallel
with multiple instances autoscaled dynamically.
​
WhisperModel
(Inference Model)
A separately deployed
Whisper
model Chainlet handles speech-to-text transcription.
Deployed
independently
to allow fast iteration on business logic without redeploying the model.
Used
across different Chains
or accessed directly as a standalone model.
Supports
multiple environments
(e.g., dev, prod) using the same instance.
Whisper can also be deployed as a
standard Truss model
, separate from the Chain.
​
4. Optimizing Performance
Even for very large files,
processing time remains bounded
by parallel execution.
​
Key performance tuning parameters:
micro_chunk_size_sec
→ Balance GPU utilization and inference latency.
macro_chunk_size_sec
→ Adjust chunk size for optimal parallelism.
Autoscaling settings
→ Tune concurrency and replica counts for load balancing.
Example speedup:
Copy
Ask AI
{
"input_duration_sec"
:
734.26
,
"processing_duration_sec"
:
82.42
,
"speedup"
:
8.9
}
​
5. Deploy & Run the Chain
​
Deploy WhisperModel first:
Copy
Ask AI
truss
chains
push
whisper_chainlet.py
Copy the
invocation URL
and update
WHISPER_URL
in
transcribe.py
.
​
Deploy the transcription Chain:
Copy
Ask AI
truss
chains
push
transcribe.py
​
Run transcription on a sample file:
Copy
Ask AI
curl
-X
POST
$INVOCATION_URL
\
-H
"Authorization: Api-Key
$BASETEN_API_KEY
"
\
-d
'<JSON_INPUT>'
​
Next Steps
Learn more about
Chains
.
Optimize GPU
autoscaling
for peak efficiency.
Extend the pipeline with
custom business logic
.
Was this page helpful?
Yes
No
Previous
Image generation
Building a text-to-image model with Flux Schnell
Next
On this page
1. Overview
2. Chain Structure
3. Implementing the Chainlets
Transcribe (Entrypoint Chainlet)
MacroChunkWorker (Processing Chainlet)
WhisperModel (Inference Model)
4. Optimizing Performance
Key performance tuning parameters:
5. Deploy & Run the Chain
Deploy WhisperModel first:
Deploy the transcription Chain:
Run transcription on a sample file:
Next Steps
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/examples/chains-build-rag:

Baseten
home page
Search...
⌘
K
Examples
Overview
Deploy your first model
Fast LLMs with TensorRT-LLM
Run any LLM with vLLM
Deploy LLMs with SGLang
RAG pipeline with Chains
Transcribe audio with Chains
Image generation
Deploy a ComfyUI project
Embeddings with BEI
Dockerized model
LLM with Streaming
Text to speech
Model library
Overview
Deepseek
Llama
Qwen
Gemma
Stable Diffusion
Flux
Kokoro
Microsoft
Nomic
Whisper
Mars
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Examples
RAG pipeline with Chains
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
Learn more about Chains
​
Prerequisites
To use Chains, install a recent Truss version and ensure pydantic is v2:
Copy
Ask AI
pip
install
--upgrade
truss
'pydantic>=2.0.0'
Help for setting up a clean development environment
Truss requires python
>=3.8,<3.13
. To set up a fresh development environment,
you can use the following commands, creating a environment named
chains_env
using
pyenv
:
Copy
Ask AI
curl
https://pyenv.run
|
bash
echo
'export PYENV_ROOT="$HOME/.pyenv"'
>>
~/.bashrc
echo
'[[ -d $PYENV_ROOT/bin ]] && export PATH="$PYENV_ROOT/bin:$PATH"'
>>
~/.bashrc
echo
'eval "$(pyenv init -)"'
>>
~/.bashrc
source
~/.bashrc
pyenv
install
3.11.0
ENV_NAME
=
"chains_env"
pyenv
virtualenv
3.11.0
$ENV_NAME
pyenv
activate
$ENV_NAME
pip
install
--upgrade
truss
'pydantic>=2.0.0'
To deploy Chains remotely, you also need a
Baseten account
.
It is handy to export your API key to the current shell session or permanently in your
.bashrc
:
~/.bashrc
Copy
Ask AI
export
BASETEN_API_KEY
=
"nPh8..."
If you want to run this example in
local debugging mode
, you’ll also need to
install chromadb:
Copy
Ask AI
pip
install
chromadb
The complete code used in this tutorial can also be found in the
Chains examples repo
.
​
Overview
Retrieval-augmented generation (RAG) is a multi-model pipeline for generating
context-aware answers from LLMs.
There are a number of ways to build a RAG system. This tutorial shows a minimum
viable implementation with a basic vector store and retrieval function. It’s
intended as a starting point to show how Chains helps you flexibly combine model
inference and business logic.
In this tutorial, we’ll build a simple RAG pipeline for a hypothetical alumni
matching service for a university. The system:
Takes a bio with information about a new graduate
Uses a vector database to retrieve semantically similar bios of other alums
Uses an LLM to explain why the new graduate should meet the selected alums
Returns the writeup from the LLM
Let’s dive in!
​
Building the Chain
Create a file
rag.py
in a new directory with:
Copy
Ask AI
mkdir
rag
touch
rag/rag.py
cd
rag
Our RAG Chain is composed of three parts:
VectorStore
, a Chainlet that implements a vector database with a retrieval
function.
LLMClient
, a Stub for connecting to a deployed LLM.
RAG
, the entrypoint Chainlet that orchestrates the RAG pipeline and
has
VectorStore
and
LLMClient
as dependencies.
We’ll examine these components one by one and then see how they all work
together.
​
Vector store Chainlet
A real production RAG system would use a hosted vector database with a massive
number of stored embeddings. For this example, we’re using a small local vector
store built with
chromadb
to stand in for a more complex system.
The Chainlet has three parts:
remote_config
, which
configures a Docker image on deployment with dependencies.
__init__()
, which runs once when the Chainlet is spun up, and creates the
vector database with ten sample bios.
run_remote()
, which runs
each time the Chainlet is called and is the sole public interface for the
Chainlet.
rag/rag.py
Copy
Ask AI
import
truss_chains
as
chains
# Create a Chainlet to serve as our vector database.
class
VectorStore
(
chains
.
ChainletBase
):
# Add chromadb as a dependency for deployment.
remote_config
=
chains.RemoteConfig(
docker_image
=
chains.DockerImage(
pip_requirements
=
[
"chromadb"
]
)
)
# Runs once when the Chainlet is deployed or scaled up.
def
__init__
(
self
):
# Import Chainlet-specific dependencies in init, not at the top of
# the file.
import
chromadb
self
._chroma_client
=
chromadb.EphemeralClient()
self
._collection
=
self
._chroma_client.create_collection(
name
=
"bios"
)
# Sample documents are hard-coded for your convenience
documents
=
[
"Angela Martinez is a tech entrepreneur based in San Francisco. As the founder and CEO of a successful AI startup, she is a leading figure in the tech community. Outside of work, Angela enjoys hiking the trails around the Bay Area and volunteering at local animal shelters."
,
"Ravi Patel resides in New York City, where he works as a financial analyst. Known for his keen insight into market trends, Ravi spends his weekends playing chess in Central Park and exploring the city's diverse culinary scene."
,
"Sara Kim is a digital marketing specialist living in San Francisco. She helps brands build their online presence with creative strategies. Outside of work, Sara is passionate about photography and enjoys hiking the trails around the Bay Area."
,
"David O'Connor calls New York City his home and works as a high school teacher. He is dedicated to inspiring the next generation through education. In his free time, David loves running along the Hudson River and participating in local theater productions."
,
"Lena Rossi is an architect based in San Francisco. She designs sustainable and innovative buildings that contribute to the city's skyline. When she's not working, Lena enjoys practicing yoga and exploring art galleries."
,
"Akio Tanaka lives in Tokyo and is a software developer specializing in mobile apps. Akio is an avid gamer and enjoys attending eSports tournaments. He also has a passion for cooking and often experiments with new recipes in his spare time."
,
"Maria Silva is a nurse residing in New York City. She is dedicated to providing compassionate care to her patients. Maria finds joy in gardening and often spends her weekends tending to her vibrant flower beds and vegetable garden."
,
"John Smith is a journalist based in San Francisco. He reports on international politics and has a knack for uncovering compelling stories. Outside of work, John is a history buff who enjoys visiting museums and historical sites."
,
"Aisha Mohammed lives in Tokyo and works as a graphic designer. She creates visually stunning graphics for a variety of clients. Aisha loves to paint and often showcases her artwork in local exhibitions."
,
"Carlos Mendes is an environmental engineer in San Francisco. He is passionate about developing sustainable solutions for urban areas. In his leisure time, Carlos enjoys surfing and participating in beach clean-up initiatives."
]
# Add all documents to the database
self
._collection.add(
documents
=
documents,
ids
=
[
f
"id
{
n
}
"
for
n
in
range
(
len
(documents))]
)
# Runs each time the Chainlet is called
async
def
run_remote
(
self
,
query
:
str
) -> list[
str
]:
# This call to includes embedding the query string.
results
=
self
._collection.query(
query_texts
=
[query],
n_results
=
2
)
if
results
is
None
or
not
results:
raise
ValueError
(
"No bios returned from the query"
)
if
not
results[
"documents"
]
or
not
results[
"documents"
][
0
]:
raise
ValueError
(
"Bios are empty"
)
return
results[
"documents"
][
0
]
​
LLM inference stub
Now that we can retrieve relevant bios from the vector database, we need to pass
that information to an LLM to generate our final output.
Chains can integrate previously deployed models using a Stub. Like Chainlets,
Stubs implement
run_remote()
, but as a call
to the deployed model.
For our LLM, we’ll use Phi-3 Mini Instruct, a small-but-mighty open source LLM.
Deploy Phi-3 Mini Instruct 4k
One-click model deployment from Baseten’s model library.
While the model is deploying, be sure to note down the models’ invocation URL from
the model dashboard for use in the next step.
To use our deployed LLM in the RAG Chain, we define a Stub:
rag/rag.py
Copy
Ask AI
class
LLMClient
(
chains
.
StubBase
):
# Runs each time the Stub is called
async
def
run_remote
(
self
,
new_bio
:
str
,
bios
: list[
str
]) ->
str
:
# Use the retrieved bios to augment the prompt -- here's the "A" in RAG!
prompt
=
f
"""You are matching alumni of a college to help them make connections. Explain why the person described first would want to meet the people selected from the matching database.
Person you're matching:
{
new_bio
}
People from database:
{
" "
.join(bios)
}
"""
# Call the deployed model.
resp
=
await
self
._remote.predict_async(
json_payload
=
{
"messages"
: [{
"role"
:
"user"
,
"content"
: prompt}],
"stream"
:
False
})
return
resp[
"output"
][
len
(prompt) :].strip()
​
RAG entrypoint Chainlet
The entrypoint to a Chain is the Chainlet that specifies the public-facing input
and output of the Chain and orchestrates calls to dependencies.
The
__init__
function in this Chainlet takes two new arguments:
Add dependencies to any Chainlet with
chains.depends()
. Only
Chainlets, not Stubs, need to be added in this fashion.
Use
chains.depends_context()
to inject a context object at runtime. This context object is required to
initialize the
LLMClient
stub.
Visit your
baseten workspace
to find your
the URL of the previously deployed Phi-3 model and insert if as value
for
LLM_URL
.
rag/rag.py
Copy
Ask AI
# Insert the URL from the previously deployed Phi-3 model.
LLM_URL
=
...
@chains.mark_entrypoint
class
RAG
(
chains
.
ChainletBase
):
# Runs once when the Chainlet is spun up
def
__init__
(
self
,
# Declare dependency chainlets.
vector_store
: VectorStore
=
chains.depends(VectorStore),
context
: chains.DeploymentContext
=
chains.depends_context(),
):
self
._vector_store
=
vector_store
# The stub needs the context for setting up authentication.
self
._llm
=
LLMClient.from_url(
LLM_URL
, context)
# Runs each time the Chain is called
async
def
run_remote
(
self
,
new_bio
:
str
) ->
str
:
# Use the VectorStore Chainlet for context retrieval.
bios
=
await
self
._vector_store.run_remote(new_bio)
# Use the LLMClient Stub for augmented generation.
contacts
=
await
self
._llm.run_remote(new_bio, bios)
return
contacts
​
Testing locally
Because our Chain uses a Stub for the LLM call, we can run the whole Chain
locally without any GPU resources.
Before running the Chainlet, make sure to set your Baseten API key as an
environment variable
BASETEN_API_KEY
.
rag/rag.py
Copy
Ask AI
if
__name__
==
"__main__"
:
import
os
import
asyncio
with
chains.run_local(
# This secret is needed even locally, because part of this chain
# calls the separately deployed Phi-3 model. Only the Chainlets
# actually run locally.
secrets
=
{
"baseten_chain_api_key"
: os.environ[
"BASETEN_API_KEY"
]}
):
rag_client
=
RAG()
result
=
asyncio.get_event_loop().run_until_complete(
rag_client.run_remote(
"""
Sam just moved to Manhattan for his new job at a large bank.
In college, he enjoyed building sets for student plays.
"""
)
)
print
(result)
We can run our Chain locally:
Copy
Ask AI
python
rag.py
After a few moments, we should get a recommendation for why Sam should meet the
alumni selected from the database.
​
Deploying to production
Once we’re satisfied with our Chain’s local behavior, we can deploy it to
production on Baseten. To deploy the Chain, run:
Copy
Ask AI
truss
chains
push
rag.py
This will deploy our Chain as a development deployment. Once the Chain is
deployed, we can call it from its API endpoint.
You can do this in the console with cURL:
Copy
Ask AI
curl
-X
POST
'https://chain-5wo86nn3.api.baseten.co/development/run_remote'
\
-H
"Authorization: Api-Key
$BASETEN_API_KEY
"
\
-d
'{"new_bio": "Sam just moved to Manhattan for his new job at a large bank.In college, he enjoyed building sets for student plays."}'
Alternatively, you can also integrate this in a Python application:
call_chain.py
Copy
Ask AI
import
requests
import
os
# Insert the URL from the deployed rag chain. You can get it from the CLI
# output or the status page, e.g.
# "https://chain-6wgeygoq.api.baseten.co/production/run_remote".
RAG_CHAIN_URL
=
""
baseten_api_key
=
os.environ[
"BASETEN_API_KEY"
]
if
not
RAG_CHAIN_URL
:
raise
ValueError
(
"Please insert the URL for the RAG chain."
)
resp
=
requests.post(
RAG_CHAIN_URL
,
headers
=
{
"Authorization"
:
f
"Api-Key
{
baseten_api_key
}
"
},
json
=
{
"new_bio"
: new_bio},
)
print
(resp.json())
When we’re happy with the deployed Chain, we can promote it to production via
the UI or by running:
Copy
Ask AI
truss
chains
push
--promote
rag.py
Once in production, the Chain will have access to full autoscaling settings.
Both the development and production deployments will scale to zero when not in
use.
Was this page helpful?
Yes
No
Previous
Transcribe audio with Chains
Process hours of audio in seconds using efficient chunking, distributed inference, and optimized GPU resources.
Next
On this page
Prerequisites
Overview
Building the Chain
Vector store Chainlet
LLM inference stub
RAG entrypoint Chainlet
Testing locally
Deploying to production
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/examples/comfyui:

Baseten
home page
Search...
⌘
K
Examples
Overview
Deploy your first model
Fast LLMs with TensorRT-LLM
Run any LLM with vLLM
Deploy LLMs with SGLang
RAG pipeline with Chains
Transcribe audio with Chains
Image generation
Deploy a ComfyUI project
Embeddings with BEI
Dockerized model
LLM with Streaming
Text to speech
Model library
Overview
Deepseek
Llama
Qwen
Gemma
Stable Diffusion
Flux
Kokoro
Microsoft
Nomic
Whisper
Mars
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Examples
Deploy a ComfyUI project
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
View example on GitHub
In this example, we’ll deploy an
anime style transfer
ComfyUI workflow using truss.
This example won’t require any Python code, but there are a few pre-requisites in order to get started.
Pre-Requisites:
Convert your ComfyUI workflow to an
API compatible JSON format
. The regular JSON format that is used to export Comfy workflows will not work here.
Have a list of the models your workflow requires along with URLs to where each model can be downloaded
​
Setup
Clone the truss-examples repository and navigate to the
comfyui-truss
directory
Copy
Ask AI
git
clone
https://github.com/basetenlabs/truss-examples.git
cd
truss-examples/comfyui-truss
This repository already contains all the files we need to deploy our ComfyUI workflow.
There are just two files we need to modify:
config.yaml
data/comfy_ui_workflow.json
​
Setting up the
config.yaml
Copy
Ask AI
build_commands
:
-
git clone https://github.com/comfyanonymous/ComfyUI.git
-
cd ComfyUI && git checkout b1fd26fe9e55163f780bf9e5f56bf9bf5f035c93 && pip install -r requirements.txt
-
cd ComfyUI/custom_nodes && git clone https://github.com/LykosAI/ComfyUI-Inference-Core-Nodes --recursive && cd ComfyUI-Inference-Core-Nodes && pip install -e .[cuda12]
-
cd ComfyUI/custom_nodes && git clone https://github.com/ZHO-ZHO-ZHO/ComfyUI-Gemini --recursive && cd ComfyUI-Gemini && pip install -r requirements.txt
-
cd ComfyUI/custom_nodes && git clone https://github.com/kijai/ComfyUI-Marigold --recursive && cd ComfyUI-Marigold && pip install -r requirements.txt
-
cd ComfyUI/custom_nodes && git clone https://github.com/omar92/ComfyUI-QualityOfLifeSuit_Omar92 --recursive
-
cd ComfyUI/custom_nodes && git clone https://github.com/Fannovel16/comfyui_controlnet_aux --recursive && cd comfyui_controlnet_aux && pip install -r requirements.txt
-
cd ComfyUI/models/controlnet && wget -O control-lora-canny-rank256.safetensors https://huggingface.co/stabilityai/control-lora/resolve/main/control-LoRAs-rank256/control-lora-canny-rank256.safetensors
-
cd ComfyUI/models/controlnet && wget -O control-lora-depth-rank256.safetensors https://huggingface.co/stabilityai/control-lora/resolve/main/control-LoRAs-rank256/control-lora-depth-rank256.safetensors
-
cd ComfyUI/models/checkpoints && wget -O dreamshaperXL_v21TurboDPMSDE.safetensors https://civitai.com/api/download/models/351306
-
cd ComfyUI/models/loras && wget -O StudioGhibli.Redmond-StdGBRRedmAF-StudioGhibli.safetensors https://huggingface.co/artificialguybr/StudioGhibli.Redmond-V2/resolve/main/StudioGhibli.Redmond-StdGBRRedmAF-StudioGhibli.safetensors
environment_variables
: {}
external_package_dirs
: []
model_metadata
: {}
model_name
:
Anime Style Transfer
python_version
:
py310
requirements
:
-
websocket-client
-
accelerate
-
opencv-python
resources
:
accelerator
:
H100
use_gpu
:
true
secrets
: {}
system_packages
:
-
wget
-
ffmpeg
-
libgl1-mesa-glx
The main part that needs to get filled out is under
build_commands
. Build commands are shell commands that get run during the build stage of the docker image.
In this example, the first two lines clone the ComfyUI repository and install the python requirements.
The latter commands install various custom nodes and models and place them in their respective directory within the ComfyUI repository.
​
Modifying
data/comfy_ui_workflow.json
The
comfy_ui_workflow.json
contains the entire ComfyUI workflow in an API compatible format. This is the workflow that will get executed by the ComfyUI server.
Here is the workflow we will be using for this example.
Anime Style Transfer Workflow
Copy
Ask AI
{
"1"
: {
"inputs"
: {
"ckpt_name"
:
"dreamshaperXL_v21TurboDPMSDE.safetensors"
},
"class_type"
:
"CheckpointLoaderSimple"
,
"_meta"
: {
"title"
:
"Load Checkpoint"
}
},
"3"
: {
"inputs"
: {
"image"
:
"{{input_image}}"
,
"upload"
:
"image"
},
"class_type"
:
"LoadImage"
,
"_meta"
: {
"title"
:
"Load Image"
}
},
"4"
: {
"inputs"
: {
"text"
: [
"160"
,
0
],
"clip"
: [
"154"
,
1
]
},
"class_type"
:
"CLIPTextEncode"
,
"_meta"
: {
"title"
:
"CLIP Text Encode (Prompt)"
}
},
"12"
: {
"inputs"
: {
"strength"
:
0.8
,
"conditioning"
: [
"131"
,
0
],
"control_net"
: [
"13"
,
0
],
"image"
: [
"71"
,
0
]
},
"class_type"
:
"ControlNetApply"
,
"_meta"
: {
"title"
:
"Apply ControlNet"
}
},
"13"
: {
"inputs"
: {
"control_net_name"
:
"control-lora-canny-rank256.safetensors"
},
"class_type"
:
"ControlNetLoader"
,
"_meta"
: {
"title"
:
"Load ControlNet Model"
}
},
"15"
: {
"inputs"
: {
"strength"
:
0.8
,
"conditioning"
: [
"12"
,
0
],
"control_net"
: [
"16"
,
0
],
"image"
: [
"18"
,
0
]
},
"class_type"
:
"ControlNetApply"
,
"_meta"
: {
"title"
:
"Apply ControlNet"
}
},
"16"
: {
"inputs"
: {
"control_net_name"
:
"control-lora-depth-rank256.safetensors"
},
"class_type"
:
"ControlNetLoader"
,
"_meta"
: {
"title"
:
"Load ControlNet Model"
}
},
"18"
: {
"inputs"
: {
"seed"
:
995352869972963
,
"denoise_steps"
:
4
,
"n_repeat"
:
10
,
"regularizer_strength"
:
0.02
,
"reduction_method"
:
"median"
,
"max_iter"
:
5
,
"tol"
:
0.001
,
"invert"
:
true
,
"keep_model_loaded"
:
true
,
"n_repeat_batch_size"
:
2
,
"use_fp16"
:
true
,
"scheduler"
:
"LCMScheduler"
,
"normalize"
:
true
,
"model"
:
"marigold-lcm-v1-0"
,
"image"
: [
"3"
,
0
]
},
"class_type"
:
"MarigoldDepthEstimation"
,
"_meta"
: {
"title"
:
"MarigoldDepthEstimation"
}
},
"19"
: {
"inputs"
: {
"images"
: [
"71"
,
0
]
},
"class_type"
:
"PreviewImage"
,
"_meta"
: {
"title"
:
"Preview Image"
}
},
"20"
: {
"inputs"
: {
"images"
: [
"18"
,
0
]
},
"class_type"
:
"PreviewImage"
,
"_meta"
: {
"title"
:
"Preview Image"
}
},
"21"
: {
"inputs"
: {
"seed"
:
358881677137626
,
"steps"
:
20
,
"cfg"
:
7
,
"sampler_name"
:
"dpmpp_2m_sde"
,
"scheduler"
:
"karras"
,
"denoise"
:
0.7000000000000001
,
"model"
: [
"154"
,
0
],
"positive"
: [
"15"
,
0
],
"negative"
: [
"4"
,
0
],
"latent_image"
: [
"25"
,
0
]
},
"class_type"
:
"KSampler"
,
"_meta"
: {
"title"
:
"KSampler"
}
},
"25"
: {
"inputs"
: {
"pixels"
: [
"70"
,
0
],
"vae"
: [
"1"
,
2
]
},
"class_type"
:
"VAEEncode"
,
"_meta"
: {
"title"
:
"VAE Encode"
}
},
"27"
: {
"inputs"
: {
"samples"
: [
"21"
,
0
],
"vae"
: [
"1"
,
2
]
},
"class_type"
:
"VAEDecode"
,
"_meta"
: {
"title"
:
"VAE Decode"
}
},
"70"
: {
"inputs"
: {
"upscale_method"
:
"lanczos"
,
"megapixels"
:
1
,
"image"
: [
"3"
,
0
]
},
"class_type"
:
"ImageScaleToTotalPixels"
,
"_meta"
: {
"title"
:
"ImageScaleToTotalPixels"
}
},
"71"
: {
"inputs"
: {
"low_threshold"
:
50
,
"high_threshold"
:
150
,
"resolution"
:
1024
,
"image"
: [
"3"
,
0
]
},
"class_type"
:
"CannyEdgePreprocessor"
,
"_meta"
: {
"title"
:
"Canny Edge"
}
},
"123"
: {
"inputs"
: {
"images"
: [
"27"
,
0
]
},
"class_type"
:
"PreviewImage"
,
"_meta"
: {
"title"
:
"Preview Image"
}
},
"131"
: {
"inputs"
: {
"text"
: [
"159"
,
0
],
"clip"
: [
"154"
,
1
]
},
"class_type"
:
"CLIPTextEncode"
,
"_meta"
: {
"title"
:
"CLIP Text Encode (Prompt)"
}
},
"152"
: {
"inputs"
: {
"text"
:
"{{prompt}}"
},
"class_type"
:
"Text _O"
,
"_meta"
: {
"title"
:
"Text_1"
}
},
"154"
: {
"inputs"
: {
"lora_name"
:
"StudioGhibli.Redmond-StdGBRRedmAF-StudioGhibli.safetensors"
,
"strength_model"
:
0.6
,
"strength_clip"
:
1
,
"model"
: [
"1"
,
0
],
"clip"
: [
"1"
,
1
]
},
"class_type"
:
"LoraLoader"
,
"_meta"
: {
"title"
:
"Load LoRA"
}
},
"156"
: {
"inputs"
: {
"text_1"
: [
"152"
,
0
],
"text_2"
: [
"158"
,
0
]
},
"class_type"
:
"ConcatText_Zho"
,
"_meta"
: {
"title"
:
"✨ConcatText_Zho"
}
},
"157"
: {
"inputs"
: {
"text"
:
"StdGBRedmAF,Studio Ghibli,"
},
"class_type"
:
"Text _O"
,
"_meta"
: {
"title"
:
"Text _2"
}
},
"158"
: {
"inputs"
: {
"text"
:
"looking at viewer, anime artwork, anime style, key visual, vibrant, studio anime, highly detailed"
},
"class_type"
:
"Text _O"
,
"_meta"
: {
"title"
:
"Text _O"
}
},
"159"
: {
"inputs"
: {
"text_1"
: [
"156"
,
0
],
"text_2"
: [
"157"
,
0
]
},
"class_type"
:
"ConcatText_Zho"
,
"_meta"
: {
"title"
:
"✨ConcatText_Zho"
}
},
"160"
: {
"inputs"
: {
"text"
:
"photo, deformed, black and white, realism, disfigured, low contrast"
},
"class_type"
:
"Text _O"
,
"_meta"
: {
"title"
:
"Text _O"
}
}
}
Important:
If you look at the JSON file above, you’ll notice we have templatized a few items using the
{{handlebars}}
templating style.
If there are any inputs in your ComfyUI workflow that should be variables such as input prompts, images, etc, you should templatize them using the handlebars format.
In this example workflow, there are two inputs:
{{input_image}}
and
{{prompt}}
When making an API call to this workflow, we will be able to pass in any variable for these two inputs.
​
Deploying the Workflow to Baseten
Once you have both your
config.yaml
and
data/comfy_ui_workflow.json
filled out we can deploy this workflow just like any other model on Baseten.
pip install truss --upgrade
truss push --publish
​
Running Inference
When you deploy the truss, it will spin up a new deployment in your Baseten account. Each deployment will expose a REST API endpoint which we can use to call this workflow.
Copy
Ask AI
import
requests
import
os
import
base64
from
PIL
import
Image
from
io
import
BytesIO
# Replace the empty string with your model id below
model_id
=
""
baseten_api_key
=
os.environ[
"BASETEN_API_KEY"
]
BASE64_PREAMBLE
=
"data:image/png;base64,"
def
pil_to_b64
(
pil_img
):
buffered
=
BytesIO()
pil_img.save(buffered,
format
=
"PNG"
)
img_str
=
base64.b64encode(buffered.getvalue()).decode(
"utf-8"
)
return
img_str
def
b64_to_pil
(
b64_str
):
return
Image.open(BytesIO(base64.b64decode(b64_str.replace(
BASE64_PREAMBLE
,
""
))))
values
=
{
"prompt"
:
"american Shorthair"
,
"input_image"
: {
"type"
:
"image"
,
"data"
: pil_to_b64(Image.open(
"/path/to/cat.png"
))}
}
resp
=
requests.post(
f
"https://model-
{
model_id
}
.api.baseten.co/production/predict"
,
headers
=
{
"Authorization"
:
f
"Api-Key
{
baseten_api_key
}
"
},
json
=
{
"workflow_values"
: values}
)
res
=
resp.json()
results
=
res.get(
"result"
)
for
item
in
results:
if
item.get(
"format"
)
==
"png"
:
data
=
item.get(
"data"
)
img
=
b64_to_pil(data)
img.save(
f
"pet-style-transfer-1.png"
)
If you recall, we templatized two variables in our workflow:
prompt
and
input_image
. In our API call we can specify the values for these two variables like so:
Copy
Ask AI
values = {
"prompt"
:
"Maltipoo"
,
"input_image"
: {
"type"
:
"image"
,
"data"
:
pil_to_b
64
(Image.open(
"/path/to/dog.png"
))
}
}
If your workflow contains more variables, simply add them to the dictionary above.
The API call returns an image in the form of a base64 string, which we convert to a PNG image.
Was this page helpful?
Yes
No
Previous
Embeddings with BEI
Serve embedding, reranking, and classification models
Next
On this page
Setup
Setting up the config.yaml
Modifying data/comfy_ui_workflow.json
Deploying the Workflow to Baseten
Running Inference
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/examples/deploy-your-first-model:

Baseten
home page
Search...
⌘
K
Examples
Overview
Deploy your first model
Fast LLMs with TensorRT-LLM
Run any LLM with vLLM
Deploy LLMs with SGLang
RAG pipeline with Chains
Transcribe audio with Chains
Image generation
Deploy a ComfyUI project
Embeddings with BEI
Dockerized model
LLM with Streaming
Text to speech
Model library
Overview
Deepseek
Llama
Qwen
Gemma
Stable Diffusion
Flux
Kokoro
Microsoft
Nomic
Whisper
Mars
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Examples
Deploy your first model
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
This guide walks through packaging and deploying
Phi-3-mini-4k-instruct
, a 3.8B parameter LLM, as a production-ready API endpoint.
We’ll cover:
Loading model weights
from Hugging Face
Running inference
on a GPU
Configuring dependencies and infrastructure
Iterating with live reload development
Deploying to production with autoscaling
By the end, you’ll have an AI model running on scalable infrastructure, callable via an API.
​
1. Setup
Before you begin:
Sign up
or
sign in
to Baseten
Generate an
API key
and store it securely
Install
Truss
, our model packaging framework
Copy
Ask AI
pip
install
--upgrade
truss
New accounts include free credits—this guide should use less than $1 in GPU
costs.
​
2. Create a Truss
A
Truss
packages your model into a
deployable container
with all dependencies and configurations.
Create a new Truss:
Copy
Ask AI
truss
init
phi-3-mini
&&
cd
phi-3-mini
When prompted, give your Truss a name like
Phi 3 Mini
.
You should see the following file structure:
Copy
Ask AI
phi-3-mini/
data/
model/
__init__.py
model.py
packages/
config.yaml
You’ll primarily edit
model/model.py
and
config.yaml
.
​
3. Load model weights
Phi-3-mini-4k-instruct
is available on Hugging Face. We’ll
load its weights using
transformers.
Edit
model/model.py
:
model/model.py
Copy
Ask AI
import
torch
from
transformers
import
AutoModelForCausalLM, AutoTokenizer
class
Model
:
def
__init__
(
self
,
**
kwargs
):
self
._model
=
None
self
._tokenizer
=
None
def
load
(
self
):
self
._model
=
AutoModelForCausalLM.from_pretrained(
"microsoft/Phi-3-mini-4k-instruct"
,
device_map
=
"cuda"
,
torch_dtype
=
"auto"
)
self
._tokenizer
=
AutoTokenizer.from_pretrained(
"microsoft/Phi-3-mini-4k-instruct"
)
​
4. Implement Model Inference
Define how the model processes incoming requests by implementing the
predict()
function:
model/model.py
Copy
Ask AI
class
Model
:
...
def
predict
(
self
,
request
):
messages
=
request.pop(
"messages"
)
model_inputs
=
self
._tokenizer.apply_chat_template(
messages,
tokenize
=
False
,
add_generation_prompt
=
True
)
inputs
=
self
._tokenizer(model_inputs,
return_tensors
=
"pt"
).to(
"cuda"
)
with
torch.no_grad():
outputs
=
self
._model.generate(
input_ids
=
inputs[
"input_ids"
],
max_length
=
256
)
return
{
"output"
:
self
._tokenizer.decode(outputs[
0
],
skip_special_tokens
=
True
)}
This function:
✅ Accepts a list of messages
✅ Uses Hugging Face’s tokenizer
✅ Generates a response with max 256 tokens
​
5. Configure Dependencies & GPU
In
config.yaml
, define the
Python environment
and
compute resources
:
​
Set Dependencies
config.yaml
Copy
Ask AI
requirements
:
-
six==1.17.0
-
accelerate==0.30.1
-
einops==0.8.0
-
transformers==4.41.2
-
torch==2.3.0
​
Allocate a GPU
Phi-3-mini needs ~7.6GB VRAM. A T4 GPU (16GB VRAM) is a good choice.
config.yaml
Copy
Ask AI
resources
:
accelerator
:
T4
use_gpu
:
true
​
6. Deploy the Model
​
1. Get Your API Key
🔗 Generate an API Key
You can generate the API key from the Baseten UI. Click on the User icon at the top-right, then click API keys. Save your API-key, because we will use it in the next step.
​
2. Push Your Model to Baseten
Copy
Ask AI
truss
push
Since this is a first-time deployment,
truss
will ask for your API-key and save it for future runs.
Monitor the deployment from
your Baseten dashboard
.
​
7. Call the Model API
After the deployment is complete, we can call the model API. First, store the Baseten API key as an environment variable:
Copy
Ask AI
export
BASETEN_API_KEY
=<
your_api_key
>
Below is the client code. Be sure to replace
model_id
from your deployment.
Copy
Ask AI
import
requests
import
os
model_id
=
"your_model_id"
baseten_api_key
=
os.environ[
"BASETEN_API_KEY"
]
resp
=
requests.post(
f
"https://model-
{
model_id
}
.api.baseten.co/development/predict"
,
headers
=
{
"Authorization"
:
f
"Api-Key
{
baseten_api_key
}
"
},
json
=
{
"messages"
: [
"What is AGI?"
]}
)
print
(resp.json())
​
8. Live Reload for Development
Avoid long deploy times when testing changes—use
live reload
:
Copy
Ask AI
truss
watch
Saves time by
patching only the updated code
Skips rebuilding Docker containers
Keeps the model server running while iterating
Make changes to
model.py
, save, and test the API again.
​
9. Promote to Production
Once you’re happy with the model, deploy it to production:
Copy
Ask AI
truss
push
--publish
This updates the
API endpoint
from:
❌
Development
: /development/predict
✅
Production
: /production/predict
Copy
Ask AI
resp
=
requests.post(
f
"https://model-
{
model_id
}
.api.baseten.co/production/predict"
,
headers
=
{
"Authorization"
:
f
"Api-Key
{
baseten_api_key
}
"
},
json
=
{
"messages"
: [
{
"role"
:
"user"
,
"content"
:
"What is AGI?"
}
],
}
)
​
Next Steps
🚀 You’ve successfully packaged, deployed, and invoked an AI model with Truss!
Explore more:
Learning more about
model serving with Truss
.
Example implementations
for dozens of open source models.
Inference examples
and
Baseten integrations
.
Using
autoscaling settings
to spin up and down multiple GPU replicas.
Was this page helpful?
Yes
No
Previous
Fast LLMs with TensorRT-LLM
Optimize LLMs for low latency and high throughput
Next
On this page
1. Setup
2. Create a Truss
3. Load model weights
4. Implement Model Inference
5. Configure Dependencies & GPU
Set Dependencies
Allocate a GPU
6. Deploy the Model
1. Get Your API Key
2. Push Your Model to Baseten
7. Call the Model API
8. Live Reload for Development
9. Promote to Production
Next Steps
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/examples/docker:

Baseten
home page
Search...
⌘
K
Examples
Overview
Deploy your first model
Fast LLMs with TensorRT-LLM
Run any LLM with vLLM
Deploy LLMs with SGLang
RAG pipeline with Chains
Transcribe audio with Chains
Image generation
Deploy a ComfyUI project
Embeddings with BEI
Dockerized model
LLM with Streaming
Text to speech
Model library
Overview
Deepseek
Llama
Qwen
Gemma
Stable Diffusion
Flux
Kokoro
Microsoft
Nomic
Whisper
Mars
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Examples
Dockerized model
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
View on Github
In this example, we deploy a dockerized model for
infinity embedding server
, a high-throughput, low-latency REST API server for serving vector embeddings.
​
Setting up the
config.yaml
To deploy a dockerized model, all you need is a
config.yaml
. It specifies how to build your Docker image, start the server, and manage resources. Let’s break down each section.
​
Base Image
Sets the foundational Docker image to a lightweight Python 3.11 environment.
config.yaml
Copy
Ask AI
base_image
:
image
:
python:3.11-slim
​
Docker Server Configuration
Configures the server’s startup command, health check endpoints, prediction endpoint, and the port on which the server will run.
config.yaml
Copy
Ask AI
docker_server
:
start_command
:
sh -c "HF_TOKEN=$(cat /secrets/hf_access_token) infinity_emb v2 --batch-size 64 --model-id BAAI/bge-small-en-v1.5 --revision main"
readiness_endpoint
:
/health
liveness_endpoint
:
/health
predict_endpoint
:
/embeddings
server_port
:
7997
​
Build Commands (Optional)
Pre-downloads model weights during the build phase to ensure the model is ready at container startup.
config.yaml
Copy
Ask AI
build_commands
:
# optional step to download the weights of the model into the image
-
sh -c "HF_TOKEN=$(cat /secrets/hf_access_token) infinity_emb v2 --preload-only --no-model-warmup --model-id BAAI/bge-small-en-v1.5 --revision main"
​
Configure resources
Note that we need an L4 to run this model.
config.yaml
Copy
Ask AI
resources
:
accelerator
:
L4
use_gpu
:
true
​
Requirements
Lists the Python package dependencies required for the infinity embedding server.
config.yaml
Copy
Ask AI
requirements
:
-
infinity-emb[all]==0.0.72
​
Runtime Settings
Sets the server to handle up to 40 concurrent inferences to manage load efficiently.
config.yaml
Copy
Ask AI
runtime
:
predict_concurrency
:
40
​
Environment Variables
Defines essential environment variables including the Hugging Face access token, request batch size, queue size limit, and a flag to disable tracking.
config.yaml
Copy
Ask AI
environment_variables
:
hf_access_token
:
null
# constrain api to at most 256 sentences per request, for better load-balancing
INFINITY_MAX_CLIENT_BATCH_SIZE
:
256
# constrain model to a max backpressure of INFINITY_MAX_CLIENT_BATCH_SIZE * predict_concurrency = 10241 requests
INFINITY_QUEUE_SIZE
:
10241
DO_NOT_TRACK
:
1
​
Deploy dockerized model
Deploy the model like you would other Trusses, with:
Copy
Ask AI
truss
push
infinity-embedding-server
--publish
Was this page helpful?
Yes
No
Previous
LLM with Streaming
Building an LLM with streaming output
Next
On this page
Setting up the config.yaml
Base Image
Docker Server Configuration
Build Commands (Optional)
Configure resources
Requirements
Runtime Settings
Environment Variables
Deploy dockerized model
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/examples/image-generation:

Baseten
home page
Search...
⌘
K
Examples
Overview
Deploy your first model
Fast LLMs with TensorRT-LLM
Run any LLM with vLLM
Deploy LLMs with SGLang
RAG pipeline with Chains
Transcribe audio with Chains
Image generation
Deploy a ComfyUI project
Embeddings with BEI
Dockerized model
LLM with Streaming
Text to speech
Model library
Overview
Deepseek
Llama
Qwen
Gemma
Stable Diffusion
Flux
Kokoro
Microsoft
Nomic
Whisper
Mars
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Examples
Image generation
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
View example on GitHub
In this example, we go through a Truss that serves a text-to-image model. We
use Flux Schnell, which is one of the highest performing text-to-image models out
there today.
​
Set up imports and torch settings
In this example, we use the Hugging Face diffusers library to build our text-to-image model.
model/model.py
Copy
Ask AI
import
base64
import
random
import
logging
from
io
import
BytesIO
import
numpy
as
np
import
torch
from
diffusers
import
FluxPipeline
from
PIL
import
Image
logging.basicConfig(
level
=
logging.
INFO
)
MAX_SEED
=
np.iinfo(np.int32).max
​
Define the
Model
class and load function
In the
load
function of the Truss, we implement logic involved in
downloading and setting up the model. For this model, we use the
FluxPipeline
class in
diffusers
to instantiate our Flux pipeline,
and configure a number of relevant parameters.
See the
diffusers docs
for details
on all of these parameters.
model/model.py
Copy
Ask AI
class
Model
:
def
__init__
(
self
,
**
kwargs
):
self
.pipe
=
None
self
.repo_id
=
"black-forest-labs/FLUX.1-schnell"
def
load
(
self
):
self
.pipe
=
FluxPipeline.from_pretrained(
self
.repo_id,
torch_dtype
=
torch.bfloat16).to(
"cuda"
)
This is a utility function for converting a PIL image to base64.
model/model.py
Copy
Ask AI
def
convert_to_b64
(
self
,
image
: Image) ->
str
:
buffered
=
BytesIO()
image.save(buffered,
format
=
"JPEG"
)
img_b64
=
base64.b64encode(buffered.getvalue()).decode(
"utf-8"
)
return
img_b64
​
Define the predict function
The
predict
function contains the actual inference logic. The steps here are:
Setting up the generation params. These include things like the prompt, image width, image height, number of inference steps, etc.
Running the Diffusion Pipeline
Convert the resulting image to base64 and return it
model/model.py
Copy
Ask AI
def
predict
(
self
,
model_input
):
seed
=
model_input.get(
"seed"
)
prompt
=
model_input.get(
"prompt"
)
prompt2
=
model_input.get(
"prompt2"
)
max_sequence_length
=
model_input.get(
"max_sequence_length"
,
256
)
# 256 is max for FLUX.1-schnell
guidance_scale
=
model_input.get(
"guidance_scale"
,
0.0
)
# 0.0 is the only value for FLUX.1-schnell
num_inference_steps
=
model_input.get(
"num_inference_steps"
,
4
)
# schnell is timestep-distilled
width
=
model_input.get(
"width"
,
1024
)
height
=
model_input.get(
"height"
,
1024
)
if
not
math.isclose(guidance_scale,
0.0
):
logging.warning(
"FLUX.1-schnell does not support guidance_scale other than 0.0"
)
guidance_scale
=
0.0
if
not
seed:
seed
=
random.randint(
0
,
MAX_SEED
)
if
len
(prompt.split())
>
max_sequence_length:
logging.warning(
"FLUX.1-schnell does not support prompts longer than 256 tokens, truncating"
)
tokens
=
prompt.split()
prompt
=
" "
.join(tokens[:
min
(
len
(tokens), max_sequence_length)])
generator
=
torch.Generator().manual_seed(seed)
image
=
self
.pipe(
prompt
=
prompt,
guidance_scale
=
guidance_scale,
max_sequence_length
=
max_sequence_length,
num_inference_steps
=
num_inference_steps,
width
=
width,
height
=
height,
output_type
=
"pil"
,
generator
=
generator,
).images[
0
]
b64_results
=
self
.convert_to_b64(image)
return
{
"data"
: b64_results}
​
Setting up the
config.yaml
Running Flux Schnell requires a handful of Python libraries, including
diffusers
,
transformers
, and others.
config.yaml
Copy
Ask AI
external_package_dirs
: []
model_cache
:
-
repo_id
:
black-forest-labs/FLUX.1-schnell
allow_patterns
:
-
"*.json"
-
"*.safetensors"
ignore_patterns
:
-
"flux1-schnell.safetensors"
model_metadata
:
example_model_input
: {
"prompt"
:
'black forest gateau cake spelling out the words "FLUX SCHNELL", tasty, food photography, dynamic shot'
}
model_name
:
Flux.1-schnell
python_version
:
py311
requirements
:
-
git+https://github.com/huggingface/diffusers.git@v0.32.2
-
transformers
-
accelerate
-
sentencepiece
-
protobuf
resources
:
accelerator
:
H100_40GB
use_gpu
:
true
secrets
: {}
system_packages
:
-
ffmpeg
-
libsm6
-
libxext6
​
Configuring resources for Flux Schnell
Note that we need an H100 40GB GPU to run this model.
config.yaml
Copy
Ask AI
resources
:
accelerator
:
H100_40GB
use_gpu
:
true
secrets
: {}
​
System Packages
Running diffusers requires
ffmpeg
and a couple other system
packages.
config.yaml
Copy
Ask AI
system_packages
:
-
ffmpeg
-
libsm6
-
libxext6
​
Enabling Caching
Flux Schnell is a large model, and downloading it could take several minutes. This means
that the cold start time for this model is long. We can solve that by using our build
caching feature. This moves the model download to the build stage of your model—
caching the model will take about 15 minutes initially but you will get ~20s cold starts
subsequently.
To enable caching, add the following to the config:
Copy
Ask AI
model_cache
:
-
repo_id
:
black-forest-labs/FLUX.1-schnell
allow_patterns
:
-
"*.json"
-
"*.safetensors"
ignore_patterns
:
-
"flux1-schnell.safetensors"
​
Deploy the model
Deploy the model like you would other Trusses, with:
Copy
Ask AI
truss
push
flux/schnell
--publish
​
Run an inference
Use a Python script to call the model once its deployed and parse its response. We parse the resulting base64-encoded string output into an actual image file:
output_image.jpg
.
infer.py
Copy
Ask AI
import
httpx
import
os
import
base64
from
PIL
import
Image
from
io
import
BytesIO
# Replace the empty string with your model id below
model_id
=
""
baseten_api_key
=
os.environ[
"BASETEN_API_KEY"
]
# Function used to convert a base64 string to a PIL image
def
b64_to_pil
(
b64_str
):
return
Image.open(BytesIO(base64.b64decode(b64_str)))
data
=
{
"prompt"
:
'red velvet cake spelling out the words "FLUX SCHNELL", tasty, food photography, dynamic shot'
}
# Call model endpoint
res
=
httpx.post(
f
"https://model-
{
model_id
}
.api.baseten.co/production/predict"
,
headers
=
{
"Authorization"
:
f
"Api-Key
{
baseten_api_key
}
"
},
json
=
data
)
# Get output image
res
=
res.json()
output
=
res.get(
"data"
)
# Convert the base64 model output to an image
img
=
b64_to_pil(output)
img.save(
"output_image.jpg"
)
Was this page helpful?
Yes
No
Previous
Deploy a ComfyUI project
Deploy your ComfyUI workflow as an API endpoint
Next
On this page
Set up imports and torch settings
Define the Model class and load function
Define the predict function
Setting up the config.yaml
Configuring resources for Flux Schnell
System Packages
Enabling Caching
Deploy the model
Run an inference
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/examples/models/deepseek/deepseek-r1:

Baseten
home page
Search...
⌘
K
Examples
Overview
Deploy your first model
Fast LLMs with TensorRT-LLM
Run any LLM with vLLM
Deploy LLMs with SGLang
RAG pipeline with Chains
Transcribe audio with Chains
Image generation
Deploy a ComfyUI project
Embeddings with BEI
Dockerized model
LLM with Streaming
Text to speech
Model library
Overview
Deepseek
Deepseek R1
DeepSeek-R1 Qwen 7B
Llama
Qwen
Gemma
Stable Diffusion
Flux
Kokoro
Microsoft
Nomic
Whisper
Mars
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Deepseek
Deepseek R1
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
Deploy Deepseek R1
​
Example usage
DeepSeek-R1 is optimized using SGLang and uses an OpenAI-compatible API endpoint.
​
Input
Copy
Ask AI
import
httpx
import
os
MODEL_ID
=
"abcd1234"
# Replace with your model ID
DEPLOYMENT_ID
=
"abcd1234"
# [Optional] Replace with your deployment ID
API_KEY
=
os.environ[
"BASETEN_API_KEY"
]
resp
=
httpx.post(
f
"https://model-
{
MODEL_ID
}
.api.baseten.co/environments/production/sync/v1/chat/completions"
,
headers
=
{
"Authorization"
:
f
"Api-Key
{
API_KEY
}
"
},
json
=
{
"model"
:
"deepseek_v3"
,
"messages"
: [
{
"role"
:
"system"
,
"content"
:
"You are a helpful AI assistant."
},
{
"role"
:
"user"
,
"content"
:
"What weighs more, a pound of bricks or a pound of feathers?"
},
],
"max_tokens"
:
1024
,
},
timeout
=
None
)
print
(resp.json())
​
Output
Copy
Ask AI
{
"id"
:
"8456fe51db3548789f199cfb8c8efd35"
,
"object"
:
"text_completion"
,
"created"
:
1735236968
,
"model"
:
"/models/deepseek_r1"
,
"choices"
: [
{
"index"
:
0
,
"text"
:
"Let's think through this step by step..."
,
"logprobs"
:
null
,
"finish_reason"
:
"stop"
,
"matched_stop"
:
1
}
],
"usage"
: {
"prompt_tokens"
:
14
,
"total_tokens"
:
240
,
"completion_tokens"
:
226
,
"prompt_tokens_details"
:
null
}
}
Was this page helpful?
Yes
No
Previous
DeepSeek-R1 Qwen 7B
Qwen 7B fine-tuned for CoT reasoning capabilities with DeepSeek R1
Next
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/examples/models/deepseek/deepseek-r1-qwen-7b:

Baseten
home page
Search...
⌘
K
Examples
Overview
Deploy your first model
Fast LLMs with TensorRT-LLM
Run any LLM with vLLM
Deploy LLMs with SGLang
RAG pipeline with Chains
Transcribe audio with Chains
Image generation
Deploy a ComfyUI project
Embeddings with BEI
Dockerized model
LLM with Streaming
Text to speech
Model library
Overview
Deepseek
Deepseek R1
DeepSeek-R1 Qwen 7B
Llama
Qwen
Gemma
Stable Diffusion
Flux
Kokoro
Microsoft
Nomic
Whisper
Mars
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Deepseek
DeepSeek-R1 Qwen 7B
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
Deploy DeepSeek-R1 Qwen 7B
​
Example usage
The fine-tuned version of Qwen is OpenAI compatible and can be called using the OpenAI client.
Copy
Ask AI
import
os
from
openai
import
OpenAI
# https://model-XXXXXXX.api.baseten.co/environments/production/sync/v1
model_url
=
""
client
=
OpenAI(
base_url
=
model_url,
api_key
=
os.environ.get(
"BASETEN_API_KEY"
),
)
stream
=
client.chat.completions.create(
model
=
"baseten"
,
messages
=
[
{
"role"
:
"user"
,
"content"
:
"Which weighs more, a pound of bricks or a pound of feathers?"
},
],
stream
=
True
,
)
for
chunk
in
stream:
if
chunk.choices[
0
].delta.content
is
not
None
:
print
(chunk.choices[
0
].delta.content,
end
=
""
)
​
JSON output
Copy
Ask AI
[
"streaming"
,
"output"
,
"text"
]
Was this page helpful?
Yes
No
Previous
Llama 3.3 70B Instruct
Llama 3.3 70B Instruct is a large language model that is optimized for instruction following.
Next
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/examples/models/flux/flux-schnell:

Baseten
home page
Search...
⌘
K
Examples
Overview
Deploy your first model
Fast LLMs with TensorRT-LLM
Run any LLM with vLLM
Deploy LLMs with SGLang
RAG pipeline with Chains
Transcribe audio with Chains
Image generation
Deploy a ComfyUI project
Embeddings with BEI
Dockerized model
LLM with Streaming
Text to speech
Model library
Overview
Deepseek
Llama
Qwen
Gemma
Stable Diffusion
Flux
Flux-Schnell
Kokoro
Microsoft
Nomic
Whisper
Mars
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Flux
Flux-Schnell
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
Deploy Flux-Schnell
​
Example usage
The model accepts a
prompt
which is some text describing the image you want to generate. The output images tend to get better as you add more descriptive words to the prompt.
The output JSON object contains a key called
data
which represents the generated image as a base64 string.
​
Input
Copy
Ask AI
import
httpx
import
os
import
base64
from
PIL
import
Image
from
io
import
BytesIO
# Replace the empty string with your model id below
model_id
=
""
baseten_api_key
=
os.environ[
"BASETEN_API_KEY"
]
# Function used to convert a base64 string to a PIL image
def
b64_to_pil
(
b64_str
):
return
Image.open(BytesIO(base64.b64decode(b64_str)))
data
=
{
"prompt"
:
'red velvet cake spelling out the words "FLUX SCHNELL", tasty, food photography, dynamic shot'
}
# Call model endpoint
res
=
httpx.post(
f
"https://model-
{
model_id
}
.api.baseten.co/production/predict"
,
headers
=
{
"Authorization"
:
f
"Api-Key
{
baseten_api_key
}
"
},
json
=
data
)
# Get output image
res
=
res.json()
output
=
res.get(
"data"
)
# Convert the base64 model output to an image
img
=
b64_to_pil(output)
img.save(
"output_image.jpg"
)
​
JSON output
Copy
Ask AI
{
"output"
:
"iVBORw0KGgoAAAANSUhEUgAABAAAAAQACAIAAA..."
}
Was this page helpful?
Yes
No
Previous
Kokoro
Kokoro is a frontier TTS model for its size of 82 million parameters (text in/audio out).
Next
On this page
Example usage
Input
JSON output
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/examples/models/gemma/gemma-3-27b-it:

Baseten
home page
Search...
⌘
K
Examples
Overview
Deploy your first model
Fast LLMs with TensorRT-LLM
Run any LLM with vLLM
Deploy LLMs with SGLang
RAG pipeline with Chains
Transcribe audio with Chains
Image generation
Deploy a ComfyUI project
Embeddings with BEI
Dockerized model
LLM with Streaming
Text to speech
Model library
Overview
Deepseek
Llama
Qwen
Gemma
Gemma 3 27B IT
Stable Diffusion
Flux
Kokoro
Microsoft
Nomic
Whisper
Mars
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Gemma
Gemma 3 27B IT
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
Deploy Gemma 3 27B IT
​
Example usage
Gemma 3 is an OpenAI-compatible model and can be called using the OpenAI SDK in any language.
Copy
Ask AI
from
openai
import
OpenAI
import
os
model_url
=
""
# Copy in from API pane in Baseten model dashboard
client
=
OpenAI(
api_key
=
os.environ[
'BASETEN_API_KEY'
],
base_url
=
model_url
)
# Chat completion
response_chat
=
client.chat.completions.create(
model
=
""
,
messages
=
[{
"role"
:
"user"
,
"content"
: [
{
"type"
:
"text"
,
"text"
:
"What's in this image?"
},
{
"type"
:
"image_url"
,
"image_url"
: {
"url"
:
"https://picsum.photos/id/237/200/300"
,
},
},
],
}],
temperature
=
0.3
,
max_tokens
=
512
,
)
print
(response_chat)
JSON Output
Copy
Ask AI
{
"id"
:
"143"
,
"choices"
: [
{
"finish_reason"
:
"stop"
,
"index"
:
0
,
"logprobs"
:
null
,
"message"
: {
"content"
:
"[Model output here]"
,
"role"
:
"assistant"
,
"audio"
:
null
,
"function_call"
:
null
,
"tool_calls"
:
null
}
}
],
"created"
:
1741224586
,
"model"
:
""
,
"object"
:
"chat.completion"
,
"service_tier"
:
null
,
"system_fingerprint"
:
null
,
"usage"
: {
"completion_tokens"
:
145
,
"prompt_tokens"
:
38
,
"total_tokens"
:
183
,
"completion_tokens_details"
:
null
,
"prompt_tokens_details"
:
null
}
}
Was this page helpful?
Yes
No
Previous
SDXL Lightning
A variant of Stable Diffusion XL that generates 1024x1024 px images in 4 UNet steps, enabling near real-time image creation.
Next
On this page
Example usage
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/examples/models/kokoro/kokoro:

Baseten
home page
Search...
⌘
K
Examples
Overview
Deploy your first model
Fast LLMs with TensorRT-LLM
Run any LLM with vLLM
Deploy LLMs with SGLang
RAG pipeline with Chains
Transcribe audio with Chains
Image generation
Deploy a ComfyUI project
Embeddings with BEI
Dockerized model
LLM with Streaming
Text to speech
Model library
Overview
Deepseek
Llama
Qwen
Gemma
Stable Diffusion
Flux
Kokoro
Kokoro
Microsoft
Nomic
Whisper
Mars
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Kokoro
Kokoro
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
Deploy Kokoro
​
Example usage
Kokoro uses the following request and response format:
Copy
Ask AI
request:
{"text": "Hello", "voice": "af", "speed": 1.0}
text: str = defaults to "Hi, I'm kokoro"
voice: str = defaults to "af", available options: "af", "af_bella", "af_sarah", "am_adam", "am_michael", "bf_emma", "bf_isabella", "bm_george", "bm_lewis", "af_nicole", "af_sky"
speed: float = defaults to 1.0. The speed of the audio generated
response:
{"base64": "base64 encoded bytestring"}
Copy
Ask AI
import
httpx
import
base64
# Replace the empty string with your model id below
model_id
=
""
baseten_api_key
=
os.environ[
"BASETEN_API_KEY"
]
with
httpx.Client()
as
client:
# Make the API request
resp
=
client.post(
f
"https://model-
{
model_id
}
.api.baseten.co/production/predict"
,
headers
=
{
"Authorization"
:
f
"Api-Key
{
API_KEY
}
"
},
json
=
{
"text"
:
"Hello world"
,
"voice"
:
"af"
,
"speed"
:
1.0
},
timeout
=
None
,
)
# Get the base64 encoded audio
response_data
=
resp.json()
audio_base64
=
response_data[
"base64"
]
# Decode the base64 string
audio_bytes
=
base64.b64decode(audio_base64)
# Write to a WAV file
with
open
(
"output.wav"
,
"wb"
)
as
f:
f.write(audio_bytes)
print
(
"Audio saved to output.wav"
)
JSON Output
Copy
Ask AI
null
Was this page helpful?
Yes
No
Previous
All MPNet Base V2
A text embedding model with a context window of 384 tokens and a dimensionality of 768 values.
Next
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/examples/models/llama/llama-3.3-70B-instruct:

Baseten
home page
Search...
⌘
K
Examples
Overview
Deploy your first model
Fast LLMs with TensorRT-LLM
Run any LLM with vLLM
Deploy LLMs with SGLang
RAG pipeline with Chains
Transcribe audio with Chains
Image generation
Deploy a ComfyUI project
Embeddings with BEI
Dockerized model
LLM with Streaming
Text to speech
Model library
Overview
Deepseek
Llama
Llama 3.3 70B Instruct
Qwen
Gemma
Stable Diffusion
Flux
Kokoro
Microsoft
Nomic
Whisper
Mars
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Llama
Llama 3.3 70B Instruct
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
Deploy Llama 3.3 70B Instruct
​
Example usage
Llama is OpenAI compatible and can be called using the OpenAI client.
Copy
Ask AI
import
os
from
openai
import
OpenAI
# https://model-XXXXXXX.api.baseten.co/environments/production/sync/v1
model_url
=
""
client
=
OpenAI(
base_url
=
model_url,
api_key
=
os.environ.get(
"BASETEN_API_KEY"
),
)
stream
=
client.chat.completions.create(
model
=
"baseten"
,
messages
=
[
{
"role"
:
"system"
,
"content"
:
"You are a helpful assistant."
},
{
"role"
:
"user"
,
"content"
:
"What was the role of Llamas in the Inca empire?"
}
],
stream
=
True
,
)
for
chunk
in
stream:
if
chunk.choices[
0
].delta.content
is
not
None
:
print
(chunk.choices[
0
].delta.content,
end
=
""
)
JSON Output
Copy
Ask AI
[
"streaming"
,
"output"
,
"text"
]
Was this page helpful?
Yes
No
Previous
Qwen-2-5-32B-Coder-Instruct
Qwen 2.5 32B Coder is an OpenAI-compatible model and can be called using the OpenAI SDK in any language.
Next
On this page
Example usage
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/examples/models/mars/MARS6:

Baseten
home page
Search...
⌘
K
Examples
Overview
Deploy your first model
Fast LLMs with TensorRT-LLM
Run any LLM with vLLM
Deploy LLMs with SGLang
RAG pipeline with Chains
Transcribe audio with Chains
Image generation
Deploy a ComfyUI project
Embeddings with BEI
Dockerized model
LLM with Streaming
Text to speech
Model library
Overview
Deepseek
Llama
Qwen
Gemma
Stable Diffusion
Flux
Kokoro
Microsoft
Nomic
Whisper
Mars
MARS6
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Mars
MARS6
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
Deploy MARS6
​
Example usage
This model requires at least four inputs:
text
: The input text that needs to be spoken
audio_ref
: An audio file containing the audio of a single person
ref_text
: What is spoken in audio_ref
language
: The language code for the target language
The model will try to output an audio stream containing the speech in the reference audio’s style. The output is by default an HTTP1.1 chunked encoding response of an encoded audio file using an ADTS AAC stream, but can be configured to stream using flac format, or to not stream at all and return the entire response as a base64 encoded flac file.
Copy
Ask AI
data = {"text": "The quick brown fox jumps over the lazy dog",
"audio_ref": encoded_str,
"ref_text": prompt_txt,
"language": 'en-us', # Target language, in this case english.
# "top_p": 0.7, # Optionally specify a top_p (default 0.7)
# "temperature": 0.7, # Optionally specify a temperature (default 0.7)
# "chunk_length": 200, # Optional text chunk length for splitting long pieces of input text. Default 200
# "max_new_tokens": 0, # Optional limit on max number of new tokens, default is zero (unlimited)
# "repetition_penalty": 1.5 # Optional rep penalty, default 1.5
}
​
Input
Copy
Ask AI
import
base64
import
time
import
torchaudio
import
requests
import
IPython.display
as
ipd
import
librosa, librosa.display
import
torch
import
io
from
torchaudio.io
import
StreamReader
# Step 1: set endpoint url and api key:
url
=
"<YOUR PREDICTION ENDPOINT>"
headers
=
{
"Authorization"
:
"Api-Key <YOUR API KEY>"
}
# Step 2: pick reference audio to clone, encode it as base64
file_path
=
"ref_debug.flac"
# any valid audio filepath, ideally between 6s-90s.
wav, sr
=
librosa.load(file_path,
sr
=
None
,
mono
=
True
,
offset
=
0
,
duration
=
5
)
io_data
=
io.BytesIO()
torchaudio.save(io_data, torch.from_numpy(wav)[
None
],
sample_rate
=
sr,
format
=
"wav"
)
io_data.seek(
0
)
encoded_data
=
base64.b64encode(io_data.read())
encoded_str
=
encoded_data.decode(
"utf-8"
)
# OPTIONAL: specify the transcript of the reference/prompt (slightly speeds up inference, and may make it sound a bit better).
prompt_txt
=
None
# if unspecified, can be left as None
# Step 3: define other inference settings:
data
=
{
"text"
:
"The quick brown fox jumps over the lazy dog"
,
"audio_ref"
: encoded_str,
"ref_text"
: prompt_txt,
"language"
:
"en-us"
,
# Target language, in this case english.
# "top_p": 0.7, # Optionally specify a top_p (default 0.7)
# "temperature": 0.7, # Optionally specify a temperature (default 0.7)
# "chunk_length": 200, # Optional text chunk length for splitting long pieces of input text. Default 200
# "max_new_tokens": 0, # Optional limit on max number of new tokens, default is zero (unlimited)
# "repetition_penalty": 1.5 # Optional rep penalty, default 1.5
# stream: bool = True # whether to stream the response back as an HTTP1.1 chunked encoding response, or run to completion and return the base64 encoded file.
# stream_format: str = "adts" # 'adts' or 'flac' for stream format. Default 'adts'
}
st
=
time.time()
class
UnseekableWrapper
:
def
__init__
(
self
,
obj
):
self
.obj
=
obj
def
read
(
self
,
n
):
return
self
.obj.read(n)
# Step 4: Send the POST request (note the first request might be a bit slow, but following requests should be fast)
response
=
requests.post(url,
headers
=
headers,
json
=
data,
stream
=
True
,
timeout
=
300
)
streamer
=
StreamReader(UnseekableWrapper(response.raw))
streamer.add_basic_audio_stream(
11025
,
buffer_chunk_size
=
3
,
sample_rate
=
44100
,
num_channels
=
1
)
# Step 4.1: check the header format of the returned stream response
for
i
in
range
(streamer.num_src_streams):
print
(streamer.get_src_stream_info(i))
# Step 5: stream the response back and decode it on-the-fly
audio_samples
=
[]
for
chunks
in
streamer.stream():
audio_chunk
=
chunks[
0
]
audio_samples.append(
audio_chunk._elem.squeeze()
)
# this is now just a (T,) float waveform, however you can set your own output format bove.
print
(
f
"Playing audio chunk of size
{
audio_chunk._elem.squeeze().shape
}
at
{
time.time()
-
st
:.2f}
s."
)
# If you wish, you can also play each chunk as you receive it, e.g. using IPython:
# ipd.display(ipd.Audio(audio_chunk._elem.squeeze().numpy(), rate=44100, autoplay=True))
# Step 6: concatenate all the audio chunks and play the full audio (if you didn't play them on the fly above)
final_full_audio
=
torch.concat(audio_samples,
dim
=
0
)
# (T,) float waveform @ 44.1kHz
# ipd.display(ipd.Audio(final_full_audio.numpy(), rate=44100))
​
Output
Copy
Ask AI
{
"reuslt"
:
"base64 encoded audio data"
,
\
}
Was this page helpful?
Yes
No
Previous
Whisper V3
Whisper V3 is a fast and accurate speech recognition model.
On this page
Example usage
Input
Output
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/examples/models/microsoft/all-mpnet-base-v2:

Baseten
home page
Search...
⌘
K
Examples
Overview
Deploy your first model
Fast LLMs with TensorRT-LLM
Run any LLM with vLLM
Deploy LLMs with SGLang
RAG pipeline with Chains
Transcribe audio with Chains
Image generation
Deploy a ComfyUI project
Embeddings with BEI
Dockerized model
LLM with Streaming
Text to speech
Model library
Overview
Deepseek
Llama
Qwen
Gemma
Stable Diffusion
Flux
Kokoro
Microsoft
All MPNet Base V2
Nomic
Whisper
Mars
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Microsoft
All MPNet Base V2
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
Deploy All MPNet Base V2
​
Example usage
This model takes a list of strings and returns a list of embeddings, where each embedding is a list of 768 floating-point number representing the semantic text embedding of the associated string.
Strings can be up to 384 tokens in length (approximately 280 words). If the strings are longer, they’ll be truncated before being run through the embedding model.
Copy
Ask AI
import
requests
import
os
# Replace the empty string with your model id below
model_id
=
""
baseten_api_key
=
os.environ[
"BASETEN_API_KEY"
]
data
=
{
"text"
: [
"I want to eat pasta"
,
"I want to eat pizza"
],
}
# Call model endpoint
res
=
requests.post(
f
"https://model-
{
model_id
}
.api.baseten.co/production/predict"
,
headers
=
{
"Authorization"
:
f
"Api-Key
{
baseten_api_key
}
"
},
json
=
data
)
# Print the output of the model
print
(res.json())
​
JSON output
Copy
Ask AI
[
[
0.2593194842338562
,
"..."
,
-1.4059709310531616
],
[
0.11028853803873062
,
"..."
,
-0.9492666125297546
]
]
Was this page helpful?
Yes
No
Previous
Nomic Embed v1.5
SOTA text embedding model with variable dimensionality — outperforms OpenAI text-embedding-ada-002 and text-embedding-3-small models.
Next
On this page
Example usage
JSON output
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/examples/models/nomic/nomic-embed-v1-5:

Baseten
home page
Search...
⌘
K
Examples
Overview
Deploy your first model
Fast LLMs with TensorRT-LLM
Run any LLM with vLLM
Deploy LLMs with SGLang
RAG pipeline with Chains
Transcribe audio with Chains
Image generation
Deploy a ComfyUI project
Embeddings with BEI
Dockerized model
LLM with Streaming
Text to speech
Model library
Overview
Deepseek
Llama
Qwen
Gemma
Stable Diffusion
Flux
Kokoro
Microsoft
Nomic
Nomic Embed v1.5
Whisper
Mars
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Nomic
Nomic Embed v1.5
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
Deploy Nomic Embed v1.5
​
Example usage
Nomic Embed v1.5 is a state of the art text embedding model with two special features:
You can choose whether to optimize the embeddings for retrieval, search, clustering, or classification.
You can trade off between cost and accuracy by choosing your own dimensionality thanks to Matryoshka Representation Learning.
Nomic Embed v1.5 takes the following parameters:
texts
the strings to embed.
task_type
the task to optimize the embedding for. Can be
search_document
(default),
search_query
,
clustering
, or
classification
.
dimensionality
the size of each output vector, any integer between
64
and
768
(default).
This code sample demonstrates embedding a set of sentences for retrieval with a dimensionality of 512.
Copy
Ask AI
import
requests
import
os
# Replace the empty string with your model id below
model_id
=
""
baseten_api_key
=
os.environ[
"BASETEN_API_KEY"
]
data
=
{
"texts"
: [
"I want to eat pasta"
,
"I want to eat pizza"
],
"task_type"
:
"search_document"
,
"dimensionality"
:
512
}
# Call model endpoint
res
=
requests.post(
f
"https://model-
{
model_id
}
.api.baseten.co/production/predict"
,
headers
=
{
"Authorization"
:
f
"Api-Key
{
baseten_api_key
}
"
},
json
=
data
)
# Print the output of the model
print
(res.json())
​
JSON output
Copy
Ask AI
[
[
-0.03811980411410332
,
"..."
,
-0.023593541234731674
],
[
-0.042617011815309525
,
"..."
,
-0.0191882885992527
]
]
Was this page helpful?
Yes
No
Previous
Whisper V3
Whisper V3 is a fast and accurate speech recognition model.
Next
On this page
Example usage
JSON output
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/examples/models/overview:

Baseten
home page
Search...
⌘
K
Examples
Overview
Deploy your first model
Fast LLMs with TensorRT-LLM
Run any LLM with vLLM
Deploy LLMs with SGLang
RAG pipeline with Chains
Transcribe audio with Chains
Image generation
Deploy a ComfyUI project
Embeddings with BEI
Dockerized model
LLM with Streaming
Text to speech
Model library
Overview
Deepseek
Llama
Qwen
Gemma
Stable Diffusion
Flux
Kokoro
Microsoft
Nomic
Whisper
Mars
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Model library
Overview
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
​
Featured models
DeepSeek R1
Whisper V3
Qwen 2.5 32B Coder Instruct
Llama 3.3 70B Instruct
flux-schnell
Gemma 3 27B IT
MARS6
Was this page helpful?
Yes
No
Previous
Deepseek R1
A state-of-the-art 671B-parameter MoE LLM with o1-style reasoning licensed for commercial use
Next
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/examples/models/qwen/qwen-2-5-32b-coder-instruct:

Baseten
home page
Search...
⌘
K
Examples
Overview
Deploy your first model
Fast LLMs with TensorRT-LLM
Run any LLM with vLLM
Deploy LLMs with SGLang
RAG pipeline with Chains
Transcribe audio with Chains
Image generation
Deploy a ComfyUI project
Embeddings with BEI
Dockerized model
LLM with Streaming
Text to speech
Model library
Overview
Deepseek
Llama
Qwen
Qwen-2-5-32B-Coder-Instruct
Gemma
Stable Diffusion
Flux
Kokoro
Microsoft
Nomic
Whisper
Mars
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Qwen
Qwen-2-5-32B-Coder-Instruct
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
Deploy Qwen 2.5 32B Coder Instruct
​
Example usage
Copy
Ask AI
from
openai
import
OpenAI
import
os
model_url
=
""
# Copy in from API pane in Baseten model dashboard
client
=
OpenAI(
api_key
=
os.environ[
'BASETEN_API_KEY'
],
base_url
=
model_url
)
# Chat completion
response_chat
=
client.chat.completions.create(
model
=
""
,
messages
=
[
{
"role"
:
"user"
,
"content"
:
"Tell me a fun fact about Python."
}
],
temperature
=
0.3
,
max_tokens
=
100
,
)
print
(response_chat)
​
JSON output
Copy
Ask AI
{
"id"
:
"143"
,
"choices"
: [
{
"finish_reason"
:
"stop"
,
"index"
:
0
,
"logprobs"
:
null
,
"message"
: {
"content"
:
"[Model output here]"
,
"role"
:
"assistant"
,
"audio"
:
null
,
"function_call"
:
null
,
"tool_calls"
:
null
}
}
],
"created"
:
1741224586
,
"model"
:
""
,
"object"
:
"chat.completion"
,
"service_tier"
:
null
,
"system_fingerprint"
:
null
,
"usage"
: {
"completion_tokens"
:
145
,
"prompt_tokens"
:
38
,
"total_tokens"
:
183
,
"completion_tokens_details"
:
null
,
"prompt_tokens_details"
:
null
}
}
Was this page helpful?
Yes
No
Previous
Gemma 3 27B IT
Instruct-tuned open model by Google with excellent ELO/size tradeoff and vision capabilities
Next
On this page
Example usage
JSON output
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/examples/models/stable-diffusion/sdxl-lightning:

Baseten
home page
Search...
⌘
K
Examples
Overview
Deploy your first model
Fast LLMs with TensorRT-LLM
Run any LLM with vLLM
Deploy LLMs with SGLang
RAG pipeline with Chains
Transcribe audio with Chains
Image generation
Deploy a ComfyUI project
Embeddings with BEI
Dockerized model
LLM with Streaming
Text to speech
Model library
Overview
Deepseek
Llama
Qwen
Gemma
Stable Diffusion
SDXL Lightning
Flux
Kokoro
Microsoft
Nomic
Whisper
Mars
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Stable Diffusion
SDXL Lightning
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
Deploy SDXL Lightning
​
Example usage
The model accepts a single input, prompt, and returns a base64 string of the image as the key
result
.
This implementation uses the 4-step UNet checkpoint to balance speed and quality. You can
deploy your own version
with either 2 steps for even faster results or 8 steps for even higher quality.
Copy
Ask AI
import
base64
import
requests
import
os
# Replace the empty string with your model id below
model_id
=
""
baseten_api_key
=
os.environ[
"BASETEN_API_KEY"
]
BASE64_PREAMBLE
=
"data:image/png;base64,"
data
=
{
"prompt"
:
"a picture of a rhino wearing a suit"
,
}
# Call model endpoint
res
=
requests.post(
f
"https://model-
{
model_id
}
.api.baseten.co/production/predict"
,
headers
=
{
"Authorization"
:
f
"Api-Key
{
baseten_api_key
}
"
},
json
=
data
)
# Get output image
res
=
res.json()
img_b64
=
res.get(
"result"
)
img
=
base64.b64decode(img_b64)
# Save the base64 string to a PNG
img_file
=
open
(
"sdxl-output-1.png"
,
"wb"
)
img_file.write(img)
img_file.close()
os.system(
"open sdxl-output-1.png"
)
JSON Output
Copy
Ask AI
{
"result"
:
"iVBORw0KGgoAAAANSUhEUgAABAAAAAQACAIAAA..."
}
Was this page helpful?
Yes
No
Previous
Flux-Schnell
Flux-Schnell is a state-of-the-art image generation model
Next
On this page
Example usage
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/examples/models/whisper/whisper-v3-fastest:

Baseten
home page
Search...
⌘
K
Examples
Overview
Deploy your first model
Fast LLMs with TensorRT-LLM
Run any LLM with vLLM
Deploy LLMs with SGLang
RAG pipeline with Chains
Transcribe audio with Chains
Image generation
Deploy a ComfyUI project
Embeddings with BEI
Dockerized model
LLM with Streaming
Text to speech
Model library
Overview
Deepseek
Llama
Qwen
Gemma
Stable Diffusion
Flux
Kokoro
Microsoft
Nomic
Whisper
Whisper V3
Mars
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Whisper
Whisper V3
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
Deploy Whisper V3
​
Example usage
Transcribe audio files at up to a 400x real-time factor — that’s 1 hour of audio in under 9 seconds. This setup requires meaningful production traffic to be cost-effective, but at scale it’s at least 80% cheaper than OpenAI.
Get in touch with us
and we’ll work with you to deploy a transcription pipeline that’s customized to match your needs.
For quick deployments of Whisper suitable for shorter audio files and lower traffic volume, you can deploy Whisper V3 and Whisper V3 Turbo directly from the model library.
Copy
Ask AI
import
requests
import
os
# Model ID for production deployment
model_id
=
""
# Read secrets from environment variables
baseten_api_key
=
os.environ[
"BASETEN_API_KEY"
]
# Call model endpoint
resp
=
requests.post(
f
"https://model-
{
model_id
}
.api.baseten.co/production/predict"
,
headers
=
{
"Authorization"
:
f
"Api-Key
{
baseten_api_key
}
"
},
json
=
{
"url"
:
"https://www2.cs.uic.edu/~i101/SoundFiles/gettysburg10.wav"
,
}
)
print
(resp.content.decode(
"utf-8"
))
JSON Output
Copy
Ask AI
{
"segments"
: [
{
"start"
:
0
,
"end"
:
9.8
,
"text"
:
"Four score and seven years ago, our fathers brought forth on this continent a new nation, conceived in liberty and dedicated to the proposition that all men are created equal."
}
],
"language_code"
:
"en"
}
Was this page helpful?
Yes
No
Previous
MARS6
MARS6 is a frontier text-to-speech model by CAMB.AI with voice/prosody cloning capabilities in 10 languages. MARS6 must be licensed for commercial use, we can help!
Next
On this page
Example usage
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/examples/overview:

Baseten
home page
Search...
⌘
K
Examples
Overview
Deploy your first model
Fast LLMs with TensorRT-LLM
Run any LLM with vLLM
Deploy LLMs with SGLang
RAG pipeline with Chains
Transcribe audio with Chains
Image generation
Deploy a ComfyUI project
Embeddings with BEI
Dockerized model
LLM with Streaming
Text to speech
Model library
Overview
Deepseek
Llama
Qwen
Gemma
Stable Diffusion
Flux
Kokoro
Microsoft
Nomic
Whisper
Mars
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Examples
Building with Baseten
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
These examples cover a variety of use cases on Baseten, from
deploying your first LLM
and
image generation
to
transcription
,
embeddings
, and
RAG pipelines
. Whether you’re optimizing inference with
TensorRT-LLM
or deploying a model with
Truss
, these guides help you build and scale efficiently.
​
Featured examples
Deploy your first model
Fast LLMs with TensorRT-LLM
Run any LLM with vLLM
Deploy LLMs with SGLang
Transcribe audio with a Chain
Embeddings with BEI
​
Model library
For a
quick start
, explore the
model library
with prebuilt, ready to deploy in one click models like DeepSeek, Llama, and Qwen.
DeepSeek R1
Whisper V3
Qwen 2.5 32B Coder Instruct
Llama 3.3 70B Instruct
flux-schnell
MARS6
Was this page helpful?
Yes
No
Deploy your first model
From model weights to API endpoint
Next
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/examples/sglang:

Baseten
home page
Search...
⌘
K
Examples
Overview
Deploy your first model
Fast LLMs with TensorRT-LLM
Run any LLM with vLLM
Deploy LLMs with SGLang
RAG pipeline with Chains
Transcribe audio with Chains
Image generation
Deploy a ComfyUI project
Embeddings with BEI
Dockerized model
LLM with Streaming
Text to speech
Model library
Overview
Deepseek
Llama
Qwen
Gemma
Stable Diffusion
Flux
Kokoro
Microsoft
Nomic
Whisper
Mars
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Examples
Deploy LLMs with SGLang
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
Another great option for inference is
SGlang
, which supports a wide range of models and performance optimizations. Besides TensorRT-LLM it is in many cases the state-of-the-art engine for serving LLMs.
​
Example: Deploy Qwen 2.5 3B on an A10G via SGLang
This configuration serves
Qwen 2.5 3B
with SGLang on an A10G GPU. Running this model is fast and cheap, making it a good example for documentation, but the process of deploying it is very similar to larger models like
Llama 3.3 70B
.
​
Setup
Before you deploy a model, you’ll need three quick setup steps.
1
Create an API key for your Baseten account
Create an
API key
and save it as an environment variable:
Copy
Ask AI
export
BASETEN_API_KEY
=
"abcd.123456"
2
Add an access token for Hugging Face
Some models require that you accept terms and conditions on Hugging Face before deployment. To prevent issues:
Accept the license for any gated models you wish to access, like
Llama 3.3
.
Create a read-only
user access token
from your Hugging Face account.
Add the
hf_access_token
secret
to your Baseten workspace
.
3
Install Truss in your local development environment
Install the latest version of Truss, our open-source model packaging framework, as well as OpenAI’s model inference SDK, with:
Copy
Ask AI
pip
install
--upgrade
truss
openai
​
Configuration
Start with an empty configuration file.
Copy
Ask AI
mkdir
qwen-2-5-3b-engine
touch
qwen-2-5-3b-engine/config.yaml
Below is an example for Qwen 2.5 3B. You can copy-paste it into the empty
config.yaml
we created above.
config.yaml
Copy
Ask AI
model_metadata
:
example_model_input
:
# Loads sample request into Baseten playground
messages
:
-
role
:
system
content
:
"You are a helpful assistant."
-
role
:
user
content
:
"What does Tongyi Qianwen mean?"
stream
:
true
model
:
"baseten-sglang"
max_tokens
:
512
temperature
:
0.6
tags
:
-
openai-compatible
model_name
:
Qwen 2.5 3B SGLang
environment_variables
:
hf_access_token
:
null
base_image
:
image
:
lmsysorg/sglang:v0.4.4.post1-cu125
docker_server
:
start_command
:
sh -c "HF_TOKEN=$(cat /secrets/hf_access_token) python3 -m sglang.launch_server --model-path Qwen/Qwen2.5-3B-Instruct --host 0.0.0.0 --port 8000"
readiness_endpoint
:
/health
liveness_endpoint
:
/health
predict_endpoint
:
/v1/chat/completions
server_port
:
8000
resources
:
accelerator
:
A10G
use_gpu
:
true
runtime
:
predict_concurrency
:
32
​
Deployment
Pushing the model to Baseten kicks off a multi-stage deployment process.
Copy
Ask AI
truss
push
qwen-2-5-3b-engine
--publish
Upon deployment, check your terminal logs or Baseten account to find the URL for the model server.
​
Inference
This model is OpenAI compatible and can be called using the OpenAI client.
call_model.py
Copy
Ask AI
import
os
from
openai
import
OpenAI
# https://model-XXXXXXX.api.baseten.co/environments/production/sync/v1
model_url
=
""
client
=
OpenAI(
base_url
=
model_url,
api_key
=
os.environ.get(
"BASETEN_API_KEY"
),
)
stream
=
client.chat.completions.create(
model
=
"baseten"
,
messages
=
[
{
"role"
:
"system"
,
"content"
:
"You are a helpful assistant."
},
{
"role"
:
"user"
,
"content"
:
"What does Tongyi Qianwen mean?"
}
],
stream
=
True
,
)
for
chunk
in
stream:
if
chunk.choices[
0
].delta.content
is
not
None
:
print
(chunk.choices[
0
].delta.content,
end
=
""
)
That’s it! You have successfully deployed and called a model using SGLang.
Was this page helpful?
Yes
No
Previous
RAG pipeline with Chains
Build a RAG (retrieval-augmented generation) pipeline with  Chains
Next
On this page
Example: Deploy Qwen 2.5 3B on an A10G via SGLang
Setup
Configuration
Deployment
Inference
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/examples/streaming:

Baseten
home page
Search...
⌘
K
Examples
Overview
Deploy your first model
Fast LLMs with TensorRT-LLM
Run any LLM with vLLM
Deploy LLMs with SGLang
RAG pipeline with Chains
Transcribe audio with Chains
Image generation
Deploy a ComfyUI project
Embeddings with BEI
Dockerized model
LLM with Streaming
Text to speech
Model library
Overview
Deepseek
Llama
Qwen
Gemma
Stable Diffusion
Flux
Kokoro
Microsoft
Nomic
Whisper
Mars
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Examples
LLM with Streaming
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
View on Github
In this example, we go through a Truss that serves the Qwen 7B Chat LLM, and streams the output to the client.
​
Why Streaming?
For certain ML models, generations can take a long time. Especially with LLMs, a long output could take
10-20 seconds to generate. However, because LLMs generate tokens in sequence, useful output can be
made available to users sooner. To support this, in Truss, we support streaming output.
​
Set up the imports
In this example, we use the HuggingFace transformers library to build a text generation model.
model/model.py
Copy
Ask AI
from
threading
import
Thread
from
typing
import
Dict
import
torch
from
transformers
import
AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer
from
transformers.generation
import
GenerationConfig
​
Define the load function
In the
load
function of the Truss, we implement logic
involved in downloading the chat version of the Qwen 7B model and loading it into memory.
model/model.py
Copy
Ask AI
class
Model
:
def
__init__
(
self
,
**
kwargs
):
self
.model
=
None
self
.tokenizer
=
None
def
load
(
self
):
self
.tokenizer
=
AutoTokenizer.from_pretrained(
"Qwen/Qwen-7B-Chat"
,
trust_remote_code
=
True
)
self
.model
=
AutoModelForCausalLM.from_pretrained(
"Qwen/Qwen-7B-Chat"
,
device_map
=
"auto"
,
trust_remote_code
=
True
).eval()
​
Define the preprocess function
In the
preprocess
function of the Truss, we set up a
generate_args
dictionary with some generation arguments from the inference request to be used in the
predict
function.
model/model.py
Copy
Ask AI
def
preprocess
(
self
,
request
:
dict
) ->
dict
:
generate_args
=
{
"max_new_tokens"
: request.get(
"max_new_tokens"
,
512
),
"temperature"
: request.get(
"temperature"
,
0.5
),
"top_p"
: request.get(
"top_p"
,
0.95
),
"top_k"
: request.get(
"top_k"
,
40
),
"repetition_penalty"
:
1.0
,
"no_repeat_ngram_size"
:
0
,
"use_cache"
:
True
,
"do_sample"
:
True
,
"eos_token_id"
:
self
.tokenizer.eos_token_id,
"pad_token_id"
:
self
.tokenizer.pad_token_id,
}
request[
"generate_args"
]
=
generate_args
return
request
​
Define the predict function
In the
predict
function of the Truss, we implement the actual
inference logic.
The two main steps are:
Tokenize the input
Call the model’s
generate
function if we’re not streaming the output, otherwise call the
stream
helper function
model/model.py
Copy
Ask AI
def
predict
(
self
,
request
: Dict):
stream
=
request.pop(
"stream"
,
False
)
prompt
=
request.pop(
"prompt"
)
generation_args
=
request.pop(
"generate_args"
)
input_ids
=
self
.tokenizer(prompt,
return_tensors
=
"pt"
).input_ids.cuda()
if
stream:
return
self
.stream(input_ids, generation_args)
with
torch.no_grad():
output
=
self
.model.generate(
inputs
=
input_ids,
**
generation_args)
return
self
.tokenizer.decode(output[
0
])
​
Define the
stream
helper function
In this helper function, we’ll instantiate the
TextIteratorStreamer
object, which we’ll later use for
returning the LLM output to users.
model/model.py
Copy
Ask AI
def
stream
(
self
,
input_ids
:
list
,
generation_args
:
dict
):
streamer
=
TextIteratorStreamer(
self
.tokenizer)
When creating the generation parameters, ensure to pass the
streamer
object
that we created previously.
model/model.py
Copy
Ask AI
generation_config
=
GenerationConfig(
**
generation_args)
generation_kwargs
=
{
"input_ids"
: input_ids,
"generation_config"
: generation_config,
"return_dict_in_generate"
:
True
,
"output_scores"
:
True
,
"max_new_tokens"
: generation_args[
"max_new_tokens"
],
"streamer"
: streamer,
}
Spawn a thread to run the generation, so that it does not block the main
thread.
model/model.py
Copy
Ask AI
with
torch.no_grad():
# Begin generation in a separate thread
thread
=
Thread(
target
=
self
.model.generate,
kwargs
=
generation_kwargs)
thread.start()
In Truss, the way to achieve streaming output is to return a generator
that yields content. In this example, we yield the output of the
streamer
,
which produces output and yields it until the generation is complete.
We define this
inner
function to create our generator.
model/model.py
Copy
Ask AI
# Yield generated text as it becomes available
def
inner
():
for
text
in
streamer:
yield
text
thread.join()
return
inner()
​
Setting up the
config.yaml
Running Qwen 7B requires torch, transformers,
and a few other related libraries.
config.yaml
Copy
Ask AI
model_name
:
qwen-7b-chat
model_metadata
:
example_model_input
:
prompt
:
What is the meaning of life?
requirements
:
-
accelerate==0.23.0
-
tiktoken==0.5.1
-
einops==0.6.1
-
scipy==1.11.3
-
transformers_stream_generator==0.0.4
-
peft==0.5.0
-
deepspeed==0.11.1
-
torch==2.0.1
-
transformers==4.32.0
​
Configure resources for Qwen
Note that we need an A10G to run this model.
config.yaml
Copy
Ask AI
resources
:
accelerator
:
A10G
cpu
:
"3"
memory
:
14Gi
use_gpu
:
true
​
Deploy Qwen 7B Chat
Deploy the model like you would other Trusses, with:
Copy
Ask AI
truss
push
qwen-7b-chat
--publish
Was this page helpful?
Yes
No
Previous
Text to speech
Building a text-to-speech model with Kokoro
Next
On this page
Why Streaming?
Set up the imports
Define the load function
Define the preprocess function
Define the predict function
Define the stream helper function
Setting up the config.yaml
Configure resources for Qwen
Deploy Qwen 7B Chat
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/examples/tensorrt-llm:

Baseten
home page
Search...
⌘
K
Examples
Overview
Deploy your first model
Fast LLMs with TensorRT-LLM
Run any LLM with vLLM
Deploy LLMs with SGLang
RAG pipeline with Chains
Transcribe audio with Chains
Image generation
Deploy a ComfyUI project
Embeddings with BEI
Dockerized model
LLM with Streaming
Text to speech
Model library
Overview
Deepseek
Llama
Qwen
Gemma
Stable Diffusion
Flux
Kokoro
Microsoft
Nomic
Whisper
Mars
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Examples
Fast LLMs with TensorRT-LLM
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
To get the best performance, we recommend using our
TensorRT-LLM Engine Builder
when deploying LLMs. Models deployed with the Engine Builder are
OpenAI compatible
, support
structured output
and
function calling
, and offer deploy-time post-training quantization to FP8 with Hopper GPUs.
The Engine Builder supports LLMs from the following families, both foundation models and fine-tunes:
Llama 3.0 and later (including DeepSeek-R1 distills)
Qwen 2.5 and later (including Math, Coder, and DeepSeek-R1 distills)
Mistral (all LLMs)
You can download preset Engine Builder configs for common models from the
model library
.
The Engine Builder does not support vision-language models like
Llama 3.2 11B
or
Pixtral
. For these models, we recommend
vLLM
.
​
Example: Deploy Qwen 2.5 3B on an H100
This configuration builds an inference engine to serve
Qwen 2.5 3B
on an H100 GPU. Running this model is fast and cheap, making it a good example for documentation, but the process of deploying it is very similar to larger models like
Llama 3.3 70B
.
​
Setup
Before you deploy a model, you’ll need three quick setup steps.
1
Create an API key for your Baseten account
Create an
API key
and save it as an environment variable:
Copy
Ask AI
export
BASETEN_API_KEY
=
"abcd.123456"
2
Add an access token for Hugging Face
Some models require that you accept terms and conditions on Hugging Face before deployment. To prevent issues:
Accept the license for any gated models you wish to access, like
Llama 3.3
.
Create a read-only
user access token
from your Hugging Face account.
Add the
hf_access_token
secret
to your Baseten workspace
.
3
Install Truss in your local development environment
Install the latest version of Truss, our open-source model packaging framework, as well as OpenAI’s model inference SDK, with:
Copy
Ask AI
pip
install
--upgrade
truss
openai
​
Configuration
Start with an empty configuration file.
Copy
Ask AI
mkdir
qwen-2-5-3b-engine
touch
qwen-2-5-3b-engine/config.yaml
This configuration file specifies model information and Engine Builder arguments. You can find dozens of examples in the
model library
as well as details on each config option in the
engine builder reference
.
Below is an example for Qwen 2.5 3B.
config.yaml
Copy
Ask AI
model_metadata
:
example_model_input
:
# Loads sample request into Baseten playground
messages
:
-
role
:
system
content
:
"You are a helpful assistant."
-
role
:
user
content
:
"What does Tongyi Qianwen mean?"
stream
:
true
max_tokens
:
512
temperature
:
0.6
# Check recommended temperature per model
repo_id
:
Qwen/Qwen2.5-3B-Instruct
model_name
:
Qwen 2.5 3B Instruct
python_version
:
py39
resources
:
# Engine Builder GPU cannot be changed post-deployment
accelerator
:
H100
use_gpu
:
true
secrets
: {}
trt_llm
:
build
:
base_model
:
decoder
checkpoint_repository
:
repo
:
Qwen/Qwen2.5-3B-Instruct
source
:
HF
num_builder_gpus
:
1
quantization_type
:
no_quant
# `fp8_kv` often recommended for large models
max_seq_len
:
32768
# option to very the max sequence length, e.g. 131072 for Llama models
tensor_parallel_count
:
1
# Set equal to number of GPUs
plugin_configuration
:
use_paged_context_fmha
:
true
use_fp8_context_fmha
:
false
# Set to true when using `fp8_kv`
paged_kv_cache
:
true
runtime
:
batch_scheduler_policy
:
max_utilization
enable_chunked_context
:
true
request_default_max_tokens
:
32768
# 131072 for Llama models
​
Deployment
Pushing the model to Baseten kicks off a multi-stage build and deployment process.
Copy
Ask AI
truss
push
qwen-2-5-3b-engine
--publish
Upon deployment, check your terminal logs or Baseten account to find the URL for the model server.
​
Inference
This model is OpenAI compatible and can be called using the OpenAI client.
Copy
Ask AI
import
os
from
openai
import
OpenAI
# https://model-XXXXXXX.api.baseten.co/environments/production/sync/v1
model_url
=
""
client
=
OpenAI(
base_url
=
model_url,
api_key
=
os.environ.get(
"BASETEN_API_KEY"
),
)
stream
=
client.chat.completions.create(
model
=
"baseten"
,
messages
=
[
{
"role"
:
"system"
,
"content"
:
"You are a helpful assistant."
},
{
"role"
:
"user"
,
"content"
:
"What does Tongyi Qianwen mean?"
}
],
stream
=
True
,
)
for
chunk
in
stream:
if
chunk.choices[
0
].delta.content
is
not
None
:
print
(chunk.choices[
0
].delta.content,
end
=
""
)
That’s it! You have successfully deployed and called an LLM optimized with the TensorRT-LLM Engine Builder. Check the
model library
for more examples and the
engine builder reference
for details on each config option.
Was this page helpful?
Yes
No
Previous
Run any LLM with vLLM
Serve a wide range of models
Next
On this page
Example: Deploy Qwen 2.5 3B on an H100
Setup
Configuration
Deployment
Inference
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/examples/text-to-speech:

Baseten
home page
Search...
⌘
K
Examples
Overview
Deploy your first model
Fast LLMs with TensorRT-LLM
Run any LLM with vLLM
Deploy LLMs with SGLang
RAG pipeline with Chains
Transcribe audio with Chains
Image generation
Deploy a ComfyUI project
Embeddings with BEI
Dockerized model
LLM with Streaming
Text to speech
Model library
Overview
Deepseek
Llama
Qwen
Gemma
Stable Diffusion
Flux
Kokoro
Microsoft
Nomic
Whisper
Mars
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Examples
Text to speech
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
View example on GitHub
In this example, we go through a Truss that serves Kokoro, a frontier text-to-speech model.
​
Set up imports
We import necessary libraries and enable Hugging Face file transfers. We also download the NLTK tokenizer data.
model/model.py
Copy
Ask AI
import
logging
import
os
os.environ[
"HF_HUB_ENABLE_HF_TRANSFER"
]
=
"1"
import
base64
import
io
import
sys
import
time
import
nltk
import
numpy
as
np
import
scipy.io.wavfile
as
wav
import
torch
from
huggingface_hub
import
snapshot_download
from
nltk.tokenize
import
sent_tokenize
from
models
import
build_model
from
kokoro
import
generate
logger
=
logging.getLogger(
__name__
)
nltk.download(
"punkt"
)
​
Downloading model weights
We need to prepare model weights by doing the following:
Create a directory for the model data
Download the Kokoro model from Hugging Face into the created model data directory
Add the model data directory to the system path
model/model.py
Copy
Ask AI
# Ensure data directory exists
os.makedirs(
"/app/data/Kokoro-82M"
,
exist_ok
=
True
)
# Download model
snapshot_download(
repo_id
=
"hexgrad/Kokoro-82M"
,
repo_type
=
"model"
,
revision
=
"c97b7bbc3e60f447383c79b2f94fee861ff156ac"
,
local_dir
=
"/app/data/Kokoro-82M"
,
ignore_patterns
=
[
"*.onnx"
,
"kokoro-v0_19.pth"
,
"demo/"
],
max_workers
=
8
,
)
# Add data_dir to the system path
sys.path.append(
"/app/data/Kokoro-82M"
)
​
Define the
Model
class and
load
function
In the
load
function of the Truss, we download and set up the model. This
load
function handles setting up the device, loading the model weights, and loading the default voice. We also define the available voices.
model/model.py
Copy
Ask AI
class
Model
:
def
__init__
(
self
,
**
kwargs
):
self
._data_dir
=
kwargs[
"data_dir"
]
self
.model
=
None
self
.device
=
None
self
.default_voice
=
None
self
.voices
=
None
return
def
load
(
self
):
logger.info(
"Starting setup..."
)
self
.device
=
"cuda"
if
torch.cuda.is_available()
else
"cpu"
logger.info(
f
"Using device:
{
self
.device
}
"
)
# Load model
logger.info(
"Loading model..."
)
model_path
=
"/app/data/Kokoro-82M/fp16/kokoro-v0_19-half.pth"
logger.info(
f
"Model path:
{
model_path
}
"
)
if
not
os.path.exists(model_path):
logger.info(
f
"Error: Model file not found at
{
model_path
}
"
)
raise
FileNotFoundError
(
f
"Model file not found at
{
model_path
}
"
)
try
:
self
.model
=
build_model(model_path,
self
.device)
logger.info(
"Model loaded successfully"
)
except
Exception
as
e:
logger.info(
f
"Error loading model:
{
str
(e)
}
"
)
raise
# Load default voice
logger.info(
"Loading default voice..."
)
voice_path
=
"/app/data/Kokoro-82M/voices/af.pt"
if
not
os.path.exists(voice_path):
logger.info(
f
"Error: Voice file not found at
{
voice_path
}
"
)
raise
FileNotFoundError
(
f
"Voice file not found at
{
voice_path
}
"
)
try
:
self
.default_voice
=
torch.load(voice_path).to(
self
.device)
logger.info(
"Default voice loaded successfully"
)
except
Exception
as
e:
logger.info(
f
"Error loading default voice:
{
str
(e)
}
"
)
raise
# Dictionary of available voices
self
.voices
=
{
"default"
:
"af"
,
"bella"
:
"af_bella"
,
"sarah"
:
"af_sarah"
,
"adam"
:
"am_adam"
,
"michael"
:
"am_michael"
,
"emma"
:
"bf_emma"
,
"isabella"
:
"bf_isabella"
,
"george"
:
"bm_george"
,
"lewis"
:
"bm_lewis"
,
"nicole"
:
"af_nicole"
,
"sky"
:
"af_sky"
,
}
return
​
Define the
predict
function
The
predict
function contains the actual inference logic. The steps here are:
Process input text and handle voice selection
Chunk text for long inputs
Generate audio
Convert resulting audio to base64 and return it
model/model.py
Copy
Ask AI
def
predict
(
self
,
model_input
):
# Run model inference here
start
=
time.time()
text
=
str
(model_input.get(
"text"
,
"Hi, I'm kokoro"
))
voice
=
str
(model_input.get(
"voice"
,
"af"
))
speed
=
float
(model_input.get(
"speed"
,
1.0
))
logger.info(
f
"Text has
{
len
(text)
}
characters. Using voice
{
voice
}
and speed
{
speed
}
."
)
if
voice
!=
"af"
:
voicepack
=
torch.load(
f
"/app/data/Kokoro-82M/voices/
{
voice
}
.pt"
).to(
self
.device
)
else
:
voicepack
=
self
.default_voice
if
len
(text)
>=
400
:
logger.info(
"Text is longer than 400 characters, splitting into sentences."
)
wavs
=
[]
def
group_sentences
(
text
,
max_length
=
400
):
sentences
=
sent_tokenize(text)
# Split long sentences
while
max
([
len
(sent)
for
sent
in
sentences])
>
max_length:
max_sent
=
max
(sentences,
key
=
len
)
sentences_before
=
sentences[: sentences.index(max_sent)]
sentences_after
=
sentences[sentences.index(max_sent)
+
1
:]
new_sentences
=
[
s.strip()
+
"."
for
s
in
max_sent.split(
"."
)
if
s.strip()
]
sentences
=
sentences_before
+
new_sentences
+
sentences_after
return
sentences
sentences
=
group_sentences(text)
logger.info(
f
"Processing
{
len
(sentences)
}
chunks. Starting generation..."
)
for
sent
in
sentences:
if
sent.strip():
audio, _
=
generate(
self
.model, sent.strip(), voicepack,
lang
=
voice[
0
],
speed
=
speed
)
# Remove potential artifacts at the end
audio
=
audio[:
-
2000
]
if
len
(audio)
>
2000
else
audio
wavs.append(audio)
# Concatenate all audio chunks
audio
=
np.concatenate(wavs)
else
:
logger.info(
"No splitting needed. Generating audio..."
)
audio, _
=
generate(
self
.model, text, voicepack,
lang
=
voice[
0
],
speed
=
speed)
# Write audio to in-memory buffer
buffer
=
io.BytesIO()
wav.write(buffer,
24000
, audio)
wav_bytes
=
buffer.getvalue()
duration_seconds
=
len
(audio)
/
24000
logger.info(
f
"Generation took
{
time.time()
-
start
}
seconds to generate
{
duration_seconds
:.2f}
seconds of audio"
)
return
{
"base64"
: base64.b64encode(wav_bytes).decode(
"utf-8"
)}
​
Setting up the
config.yaml
Running Kokoro requires a handful of Python libraries, including
torch
,
transformers
, and others.
config.yaml
Copy
Ask AI
build_commands
:
-
python3 -c "import nltk; nltk.download('punkt'); nltk.download('punkt_tab')"
environment_variables
: {}
model_metadata
:
example_model_input
: {
"text"
:
"Kokoro is a frontier TTS model for its size of 82 million parameters (text in/audio out). On 25 Dec 2024, Kokoro v0.19 weights were permissively released in full fp32 precision under an Apache 2.0 license. As of 2 Jan 2025, 10 unique Voicepacks have been released, and a .onnx version of v0.19 is available.In the weeks leading up to its release, Kokoro v0.19 was the #1🥇 ranked model in TTS Spaces Arena. Kokoro had achieved higher Elo in this single-voice Arena setting over other models, using fewer parameters and less data. Kokoro's ability to top this Elo ladder suggests that the scaling law (Elo vs compute/data/params) for traditional TTS models might have a steeper slope than previously expected."
,
"voice"
:
"af"
,
"speed"
:
1.0
}
model_name
:
kokoro
python_version
:
py311
requirements
:
-
torch==2.5.1
-
transformers==4.48.0
-
scipy==1.15.1
-
phonemizer==3.3.0
-
nltk==3.9.1
-
numpy
-
huggingface_hub[hf_transfer]
-
hf_transfer==0.1.9
-
munch==4.0.0
resources
:
accelerator
:
T4
use_gpu
:
true
runtime
:
predict_concurrency
:
1
secrets
: {}
system_packages
:
-
espeak-ng
​
Configuring resources for Kokoro
Note that we need an T4 GPU to run this model.
config.yaml
Copy
Ask AI
resources
:
accelerator
:
T4
use_gpu
:
true
​
System Packages
Running Kokoro requires
espeak-ng
to synthesize speech output.
config.yaml
Copy
Ask AI
system_packages
:
-
espeak-ng
​
Deploy the model
Deploy the model like you would other Trusses by running the following command:
Copy
Ask AI
truss
push
kokoro
--publish
​
Run an inference
Use a Python script to call the deployed model and parse its response. In this example, the script sends text input to the model and saves the returned audio (decoded from base64) as a WAV file:
output.wav
.
infer.py
Copy
Ask AI
import
httpx
import
base64
# Replace the empty string with your model id below
model_id
=
""
baseten_api_key
=
os.environ[
"BASETEN_API_KEY"
]
with
httpx.Client()
as
client:
# Make the API request
resp
=
client.post(
f
"https://model-
{
model_id
}
.api.baseten.co/production/predict"
,
headers
=
{
"Authorization"
:
f
"Api-Key
{
baseten_api_key
}
"
},
json
=
{
"text"
:
"Hello world"
,
"voice"
:
"af"
,
"speed"
:
1.0
},
timeout
=
None
,
)
# Get the base64 encoded audio
response_data
=
resp.json()
audio_base64
=
response_data[
"base64"
]
# Decode the base64 string
audio_bytes
=
base64.b64decode(audio_base64)
# Write to a WAV file
with
open
(
"output.wav"
,
"wb"
)
as
f:
f.write(audio_bytes)
print
(
"Audio saved to output.wav"
)
Was this page helpful?
Yes
No
Previous
Overview
Browse our library of open source models that are ready to deploy behind an API endpoint in seconds.
Next
On this page
Set up imports
Downloading model weights
Define the Model class and load function
Define the predict function
Setting up the config.yaml
Configuring resources for Kokoro
System Packages
Deploy the model
Run an inference
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/examples/vllm:

Baseten
home page
Search...
⌘
K
Examples
Overview
Deploy your first model
Fast LLMs with TensorRT-LLM
Run any LLM with vLLM
Deploy LLMs with SGLang
RAG pipeline with Chains
Transcribe audio with Chains
Image generation
Deploy a ComfyUI project
Embeddings with BEI
Dockerized model
LLM with Streaming
Text to speech
Model library
Overview
Deepseek
Llama
Qwen
Gemma
Stable Diffusion
Flux
Kokoro
Microsoft
Nomic
Whisper
Mars
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Examples
Run any LLM with vLLM
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
Another great option for inference is
vLLM
, which supports a wide range of models and performance optimizations.
​
Example: Deploy Qwen 2.5 3B on an A10G
This configuration serves
Qwen 2.5 3B
with vLLM on an A10G GPU. Running this model is fast and cheap, making it a good example for documentation, but the process of deploying it is very similar to larger models like
Llama 3.3 70B
.
​
Setup
Before you deploy a model, you’ll need three quick setup steps.
1
Create an API key for your Baseten account
Create an
API key
and save it as an environment variable:
Copy
Ask AI
export
BASETEN_API_KEY
=
"abcd.123456"
2
Add an access token for Hugging Face
Some models require that you accept terms and conditions on HuggingFace before deployment. To prevent issues:
Accept the license for any gated models you wish to access, like
Llama 3.3
.
Create a read-only
user access token
from your Hugging Face account.
Add the
hf_access_token
secret
to your Baseten workspace
.
3
Install Truss in your local development environment
Install the latest version of Truss, our open-source model packaging framework, as well as OpenAI’s model inference SDK, with:
Copy
Ask AI
pip
install
--upgrade
truss
openai
​
Configuration
Start with an empty configuration file.
Copy
Ask AI
mkdir
qwen-2-5-3b-engine
touch
qwen-2-5-3b-engine/config.yaml
Below is an example for Qwen 2.5 3B. You can copy-paste it into the empty
config.yaml
we created above.
config.yaml
Copy
Ask AI
model_metadata
:
engine_args
:
model
:
Qwen/Qwen2.5-3B-Instruct
example_model_input
:
# Loads sample request into Baseten playground
messages
:
-
role
:
system
content
:
"You are a helpful assistant."
-
role
:
user
content
:
"What does Tongyi Qianwen mean?"
stream
:
true
max_tokens
:
512
temperature
:
0.6
base_image
:
image
:
vllm/vllm-openai:v0.7.3
docker_server
:
start_command
:
sh -c "HF_TOKEN=$(cat /secrets/hf_access_token) vllm serve Qwen/Qwen2.5-3B-Instruct --enable-prefix-caching --enable-chunked-prefill"
readiness_endpoint
:
/health
liveness_endpoint
:
/health
predict_endpoint
:
/v1/completions
server_port
:
8000
runtime
:
predict_concurrency
:
256
resources
:
accelerator
:
A10G
use_gpu
:
true
environment_variables
:
hf_access_token
:
null
​
Deployment
Pushing the model to Baseten kicks off a multi-stage deployment process.
Copy
Ask AI
truss
push
qwen-2-5-3b-engine
--publish
Upon deployment, check your terminal logs or Baseten account to find the URL for the model server.
​
Inference
This model is OpenAI compatible and can be called using the OpenAI client.
call_model.py
Copy
Ask AI
import
os
from
openai
import
OpenAI
# https://model-XXXXXXX.api.baseten.co/environments/production/sync/v1
model_url
=
""
client
=
OpenAI(
base_url
=
model_url,
api_key
=
os.environ.get(
"BASETEN_API_KEY"
),
)
stream
=
client.chat.completions.create(
model
=
"Qwen/Qwen2.5-3B-Instruct"
,
messages
=
[
{
"role"
:
"system"
,
"content"
:
"You are a helpful assistant."
},
{
"role"
:
"user"
,
"content"
:
"What does Tongyi Qianwen mean?"
}
],
stream
=
True
,
)
for
chunk
in
stream:
if
chunk.choices[
0
].delta.content
is
not
None
:
print
(chunk.choices[
0
].delta.content,
end
=
""
)
That’s it! You have successfully deployed and called a model using vLLM.
Was this page helpful?
Yes
No
Previous
Deploy LLMs with SGLang
Optimized inference for LLMs with SGLang
Next
On this page
Example: Deploy Qwen 2.5 3B on an A10G
Setup
Configuration
Deployment
Inference
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/inference/async:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Developing a Chain
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Exporting metrics
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Inference
Async inference
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
Async requests are a “fire and forget” way of executing model inference requests. Instead of waiting for a response from a model, making an async request queues the request, and immediately returns with a request identifier. Optionally, async request results are sent via a
POST
request to a user-defined webhook upon completion.
Use async requests for:
Long-running inference tasks that may otherwise hit request timeouts.
Batched inference jobs.
Prioritizing certain inference requests.
Async fast facts:
Async requests can be made to any model—
no model code changes necessary
.
Async requests can remain queued for up to 72 hours and run for up to 1 hour.
Async requests are
not
compatible with streaming model output.
Async request inputs and model outputs are
not
stored after an async request has been completed. Instead, model outputs will be sent to your webhook via a
POST
request.
​
Quick start
There are two ways to use async inference:
Provide a webhook endpoint where model outputs will be sent via a
POST
request. If providing a webhook, you can
use async inference on any model, without making any changes to your model code
.
Inside your Truss’
model.py
, save prediction results to cloud storage. If a webhook endpoint is provided, your model outputs will also be sent to your webhook.
Note that Baseten
does not
store model outputs. If you do not wish to use a webhook, your
model.py
must write model outputs to a cloud storage bucket or database as part of its implementation.
Quick start with webhook
Quick start with saving model outputs
1
Setup webhook endpoint
Set up a webhook endpoint for handling completed async requests. Since Baseten doesn’t store model outputs, model outputs from async requests will be sent to your webhook endpoint.
Before creating your first async request, try running a sample request against your webhook endpoint to ensure that it can consume async predict results properly. Check out
this example webhook test
.
We recommend using
this Repl
as a starting point.
2
Schedule an async predict request
Call
/async_predict
on your model. The body of an
/async_predict
request includes the model input in
model_input
field, with the addition of a webhook endpoint (from the previous step) in the
webhook_endpoint
field.
Python
Copy
Ask AI
import
requests
import
os
model_id
=
""
# Replace this with your model ID
webhook_endpoint
=
""
# Replace this with your webhook endpoint URL
# Read secrets from environment variables
baseten_api_key
=
os.environ[
"BASETEN_API_KEY"
]
# Call the async_predict endpoint of the production deployment
resp
=
requests.post(
f
"https://model-
{
model_id
}
.api.baseten.co/production/async_predict"
,
headers
=
{
"Authorization"
:
f
"Api-Key
{
baseten_api_key
}
"
},
json
=
{
"model_input"
: {
"prompt"
:
"hello world!"
},
"webhook_endpoint"
: webhook_endpoint
# Optional fields for priority, max_time_in_queue_seconds, etc
},
)
print
(resp.json())
Save the
request_id
from the
/async_predict
response to check its status or cancel it.
201
Copy
Ask AI
{
"request_id"
:
"9876543210abcdef1234567890fedcba"
}
See the
async inference API reference
for more endpoint details.
3
Check async predict results
Using the
request_id
saved from the previous step, check the status of your async predict request:
Python
Copy
Ask AI
import
requests
import
os
model_id
=
""
request_id
=
""
# Read secrets from environment variables
baseten_api_key
=
os.environ[
"BASETEN_API_KEY"
]
resp
=
requests.get(
f
"https://model-
{
model_id
}
.api.baseten.co/async_request/
{
request_id
}
"
,
headers
=
{
"Authorization"
:
f
"Api-Key
{
baseten_api_key
}
"
}
)
print
(resp.json())
Once your model has finished executing the request, the async predict result will be sent to your webhook in a
POST
request.
Copy
Ask AI
{
"request_id"
:
"9876543210abcdef1234567890fedcba"
,
"model_id"
:
"my_model_id"
,
"deployment_id"
:
"my_deployment_id"
,
"type"
:
"async_request_completed"
,
"time"
:
"2024-04-30T01:01:08.883423Z"
,
"data"
: {
"my_model_output"
:
"hello world!"
},
"errors"
: []
}
4
Secure your webhook
We strongly recommend securing the requests sent to your webhooks to validate that they are from Baseten.
For instructions, see our
guide to securing async requests
.
1
Setup webhook endpoint
Set up a webhook endpoint for handling completed async requests. Since Baseten doesn’t store model outputs, model outputs from async requests will be sent to your webhook endpoint.
Before creating your first async request, try running a sample request against your webhook endpoint to ensure that it can consume async predict results properly. Check out
this example webhook test
.
We recommend using
this Repl
as a starting point.
2
Schedule an async predict request
Call
/async_predict
on your model. The body of an
/async_predict
request includes the model input in
model_input
field, with the addition of a webhook endpoint (from the previous step) in the
webhook_endpoint
field.
Python
Copy
Ask AI
import
requests
import
os
model_id
=
""
# Replace this with your model ID
webhook_endpoint
=
""
# Replace this with your webhook endpoint URL
# Read secrets from environment variables
baseten_api_key
=
os.environ[
"BASETEN_API_KEY"
]
# Call the async_predict endpoint of the production deployment
resp
=
requests.post(
f
"https://model-
{
model_id
}
.api.baseten.co/production/async_predict"
,
headers
=
{
"Authorization"
:
f
"Api-Key
{
baseten_api_key
}
"
},
json
=
{
"model_input"
: {
"prompt"
:
"hello world!"
},
"webhook_endpoint"
: webhook_endpoint
# Optional fields for priority, max_time_in_queue_seconds, etc
},
)
print
(resp.json())
Save the
request_id
from the
/async_predict
response to check its status or cancel it.
201
Copy
Ask AI
{
"request_id"
:
"9876543210abcdef1234567890fedcba"
}
See the
async inference API reference
for more endpoint details.
3
Check async predict results
Using the
request_id
saved from the previous step, check the status of your async predict request:
Python
Copy
Ask AI
import
requests
import
os
model_id
=
""
request_id
=
""
# Read secrets from environment variables
baseten_api_key
=
os.environ[
"BASETEN_API_KEY"
]
resp
=
requests.get(
f
"https://model-
{
model_id
}
.api.baseten.co/async_request/
{
request_id
}
"
,
headers
=
{
"Authorization"
:
f
"Api-Key
{
baseten_api_key
}
"
}
)
print
(resp.json())
Once your model has finished executing the request, the async predict result will be sent to your webhook in a
POST
request.
Copy
Ask AI
{
"request_id"
:
"9876543210abcdef1234567890fedcba"
,
"model_id"
:
"my_model_id"
,
"deployment_id"
:
"my_deployment_id"
,
"type"
:
"async_request_completed"
,
"time"
:
"2024-04-30T01:01:08.883423Z"
,
"data"
: {
"my_model_output"
:
"hello world!"
},
"errors"
: []
}
4
Secure your webhook
We strongly recommend securing the requests sent to your webhooks to validate that they are from Baseten.
For instructions, see our
guide to securing async requests
.
1
Update your model to save prediction results
Update your Truss’s
model.py
to save prediction results to cloud storage, such as S3 or GCS. We recommend implementing this in your model’s
postprocess()
method, which will run on CPU after the prediction has completed.
2
Setup webhook endpoint
Optionally, set up a webhook endpoint so Baseten can notify you when your async request completes.
Before creating your first async request, try running a sample request against your webhook endpoint to ensure that it can consume async predict results properly. Check out
this example webhook test
.
We recommend using
this Repl
as a starting point.
3
Schedule an async predict request
Call
/async_predict
on your model. The body of an
/async_predict
request includes the model input in
model_input
field, with the addition of a webhook endpoint (from the previous step) in the
webhook_endpoint
field.
Python
Copy
Ask AI
import
requests
import
os
model_id
=
""
# Replace this with your model ID
webhook_endpoint
=
""
# Replace this with your webhook endpoint URL
# Read secrets from environment variables
baseten_api_key
=
os.environ[
"BASETEN_API_KEY"
]
# Call the async_predict endpoint of the production deployment
resp
=
requests.post(
f
"https://model-
{
model_id
}
.api.baseten.co/production/async_predict"
,
headers
=
{
"Authorization"
:
f
"Api-Key
{
baseten_api_key
}
"
},
json
=
{
"model_input"
: {
"prompt"
:
"hello world!"
},
"webhook_endpoint"
: webhook_endpoint
# Optional fields for priority, max_time_in_queue_seconds, etc
},
)
print
(resp.json())
Save the
request_id
from the
/async_predict
response to check its status or cancel it.
201
Copy
Ask AI
{
"request_id"
:
"9876543210abcdef1234567890fedcba"
}
See the
async inference API reference
for more endpoint details.
4
Check async predict results
Using the
request_id
saved from the previous step, check the status of your async predict request:
Python
Copy
Ask AI
import
requests
import
os
model_id
=
""
request_id
=
""
# Read secrets from environment variables
baseten_api_key
=
os.environ[
"BASETEN_API_KEY"
]
resp
=
requests.get(
f
"https://model-
{
model_id
}
.api.baseten.co/async_request/
{
request_id
}
"
,
headers
=
{
"Authorization"
:
f
"Api-Key
{
baseten_api_key
}
"
}
)
print
(resp.json())
Once your model has finished executing the request, the async predict result will be sent to your webhook in a
POST
request.
Copy
Ask AI
{
"request_id"
:
"9876543210abcdef1234567890fedcba"
,
"model_id"
:
"my_model_id"
,
"deployment_id"
:
"my_deployment_id"
,
"type"
:
"async_request_completed"
,
"time"
:
"2024-04-30T01:01:08.883423Z"
,
"data"
: {
"my_model_output"
:
"hello world!"
},
"errors"
: []
}
5
Secure your webhook
We strongly recommend securing the requests sent to your webhooks to validate that they are from Baseten.
For instructions, see our
guide to securing async requests
.
Chains
: this guide is written for Truss models, but
Chains
support async inference likewise. An
Chain entrypoint can be invoked via its
async_run_remote
endpoint, e.g.
https://chain-{chain_id}.api.baseten.co/production/async_run_remote
. The
internal Chainlet-Chainlet call will still run synchronously.
​
User guide
​
Configuring the webhook endpoint
Configure your webhook endpoint to handle
POST
requests with
async predict results
. We require that webhook endpoints use HTTPS.
We recommend running a sample request against your webhook endpoint to ensure that it can consume async predict results properly. Try running
this webhook test
.
For local development, we recommend using
this Repl
as a starting point. This code validates the webhook request and logs the payload.
​
Making async requests
Production deployment
Development deployment
Other published deployments
Python
Copy
Ask AI
import
requests
import
os
model_id
=
""
# Replace this with your model ID
webhook_endpoint
=
""
# Replace this with your webhook endpoint URL
# Read secrets from environment variables
baseten_api_key
=
os.environ[
"BASETEN_API_KEY"
]
# Call the async_predict endpoint of the production deployment
resp
=
requests.post(
f
"https://model-
{
model_id
}
.api.baseten.co/production/async_predict"
,
headers
=
{
"Authorization"
:
f
"Api-Key
{
baseten_api_key
}
"
},
json
=
{
"model_input"
: {
"prompt"
:
"hello world!"
},
"webhook_endpoint"
: webhook_endpoint
# Optional fields for priority, max_time_in_queue_seconds, etc
},
)
print
(resp.json())
Python
Copy
Ask AI
import
requests
import
os
model_id
=
""
# Replace this with your model ID
webhook_endpoint
=
""
# Replace this with your webhook endpoint URL
# Read secrets from environment variables
baseten_api_key
=
os.environ[
"BASETEN_API_KEY"
]
# Call the async_predict endpoint of the production deployment
resp
=
requests.post(
f
"https://model-
{
model_id
}
.api.baseten.co/production/async_predict"
,
headers
=
{
"Authorization"
:
f
"Api-Key
{
baseten_api_key
}
"
},
json
=
{
"model_input"
: {
"prompt"
:
"hello world!"
},
"webhook_endpoint"
: webhook_endpoint
# Optional fields for priority, max_time_in_queue_seconds, etc
},
)
print
(resp.json())
Python
Copy
Ask AI
import
requests
import
os
model_id
=
""
# Replace this with your model ID
webhook_endpoint
=
""
# Replace this with your webhook endpoint URL
# Read secrets from environment variables
baseten_api_key
=
os.environ[
"BASETEN_API_KEY"
]
# Call the async_predict endpoint of the development deployment
resp
=
requests.post(
f
"https://model-
{
model_id
}
.api.baseten.co/development/async_predict"
,
headers
=
{
"Authorization"
:
f
"Api-Key
{
baseten_api_key
}
"
},
json
=
{
"model_input"
: {
"prompt"
:
"hello world!"
},
"webhook_endpoint"
: webhook_endpoint
# Optional fields for priority, max_time_in_queue_seconds, etc
},
)
print
(resp.json())
Python
Copy
Ask AI
import
requests
import
os
model_id
=
""
# Replace this with your model ID
deployment_id
=
""
# Replace this with your deployment ID
webhook_endpoint
=
""
# Replace this with your webhook endpoint URL
# Read secrets from environment variables
baseten_api_key
=
os.environ[
"BASETEN_API_KEY"
]
# Call the async_predict endpoint of the given deployment
resp
=
requests.post(
f
"https://model-
{
model_id
}
.api.baseten.co/deployment/
{
deployment_id
}
/async_predict"
,
headers
=
{
"Authorization"
:
f
"Api-Key
{
baseten_api_key
}
"
},
json
=
{
"model_input"
: {
"prompt"
:
"hello world!"
},
"webhook_endpoint"
: webhook_endpoint
# Optional fields for priority, max_time_in_queue_seconds, etc
},
)
print
(resp.json())
Create an async request by calling a model’s
/async_predict
endpoint. See the
async inference API reference
for more endpoint details.
​
Getting and canceling async requests
Get async request details
Cancel async request
You may get the status of an async request for up to 1 hour after the request has been completed.
Python
Copy
Ask AI
import
requests
import
os
model_id
=
""
request_id
=
""
# Read secrets from environment variables
baseten_api_key
=
os.environ[
"BASETEN_API_KEY"
]
resp
=
requests.get(
f
"https://model-
{
model_id
}
.api.baseten.co/async_request/
{
request_id
}
"
,
headers
=
{
"Authorization"
:
f
"Api-Key
{
baseten_api_key
}
"
}
)
print
(resp.json())
You may get the status of an async request for up to 1 hour after the request has been completed.
Python
Copy
Ask AI
import
requests
import
os
model_id
=
""
request_id
=
""
# Read secrets from environment variables
baseten_api_key
=
os.environ[
"BASETEN_API_KEY"
]
resp
=
requests.get(
f
"https://model-
{
model_id
}
.api.baseten.co/async_request/
{
request_id
}
"
,
headers
=
{
"Authorization"
:
f
"Api-Key
{
baseten_api_key
}
"
}
)
print
(resp.json())
Python
Copy
Ask AI
import
requests
import
os
model_id
=
""
request_id
=
""
# Read secrets from environment variables
baseten_api_key
=
os.environ[
"BASETEN_API_KEY"
]
resp
=
requests.delete(
f
"https://model-
{
model_id
}
.api.baseten.co/async_request/
{
request_id
}
"
,
headers
=
{
"Authorization"
:
f
"Api-Key
{
baseten_api_key
}
"
}
)
print
(resp.json())
Manage async requests using the
get async request API endpoint
and the
cancel async request API endpoint
.
​
Processing async predict results
Baseten does not store async predict results. Ensure that prediction outputs are either processed by your webhook, or saved to cloud storage in your model code (for example, in your model’s
postprocess
method).
If a webhook endpoint was provided in the
/async_predict
request, the async predict results will be sent in a
POST
request to the webhook endpoint. Errors in executing the async prediction will be included in the
errors
field of the async predict result.
Async predict result schema:
request_id
(string): the ID of the completed async request. This matches the
request_id
field of the
/async_predict
response.
model_id
(string): the ID of the model that executed the request
deployment_id
(string): the ID of the deployment that executed the request
type
(string): the type of the async predict result. This will always be
"async_request_completed"
, even in error cases.
time
(datetime): the time in UTC at which the request was sent to the webhook
data
(dict or string): the prediction output
errors
(list): any errors that occurred in processing the async request
Example async predict result:
Copy
Ask AI
{
"request_id"
:
"9876543210abcdef1234567890fedcba"
,
"model_id"
:
"my_model_id"
,
"deployment_id"
:
"my_deployment_id"
,
"type"
:
"async_request_completed"
,
"time"
:
"2024-04-30T01:01:08.883423Z"
,
"data"
: {
"my_model_output"
:
"hello world!"
},
"errors"
: []
}
​
Observability
Metrics for async request execution are available on the
Metrics tab
of your model dashboard.
Async requests are included in inference latency and volume metrics.
A time in async queue chart displays the time an async predict request spent in the
QUEUED
state before getting processed by the model.
A async queue size chart displays the current number of queued async predict requests.
The time in async queue chart.
​
Securing async inference
Since async predict results are sent to a webhook available to anyone over the internet with the endpoint, you’ll want to have some verification that these results sent to the webhook are actually coming from Baseten.
We recommend leveraging webhook signatures to secure webhook payloads and ensure they are from Baseten.
This is a two-step process:
Create a webhook secret.
Validate a webhook signature sent as a header along with the webhook request payload.
​
Creating webhook secrets
Webhook secrets can be generated via the
Secrets tab
.
Generate a webhook secret with the "Add webhook secret" button.
A webhook secret looks like:
Copy
Ask AI
whsec_AbCdEf123456GhIjKlMnOpQrStUvWxYz12345678
Ensure this webhook secret is saved securely. It can be viewed at any time and
rotated if necessary
in the Secrets tab.
​
Validating webhook signatures
If a webhook secret exists, Baseten will include a webhook signature in the
"X-BASETEN-SIGNATURE"
header of the webhook request so you can verify that it is coming from Baseten.
A Baseten signature header looks like:
"X-BASETEN-SIGNATURE": "v1=signature"
Where
signature
is an
HMAC
generated using a
SHA-256
hash function calculated over the whole async predict result and signed using a webhook secret.
If multiple webhook secrets are active, a signature will be generated using each webhook secret. In the example below, the newer webhook secret was used to create
newsignature
and the older (soon to expire) webhook secret was used to create
oldsignature
.
"X-BASETEN-SIGNATURE": "v1=newsignature,v1=oldsignature"
To validate a Baseten signature, we recommend the following. A full Baseten signature validation example can be found in
this Repl
.
1
Compare timestamps
Compare the async predict result timestamp with the current time and decide if it was received within an acceptable tolerance window.
Copy
Ask AI
TIMESTAMP_TOLERANCE_SECONDS
=
300
# Check timestamp in async predict result against current time to ensure its within our tolerance
if
(datetime.now(timezone.utc)
-
async_predict_result.time).total_seconds()
>
TIMESTAMP_TOLERANCE_SECONDS
:
logging.error(
f
"Async predict result was received after
{
TIMESTAMP_TOLERANCE_SECONDS
}
seconds and is considered stale, Baseten signature was not validated."
)
2
Recompute Baseten signature
Recreate the Baseten signature using webhook secret(s) and the async predict result.
Copy
Ask AI
WEBHOOK_SECRETS
=
[]
# Add your webhook secrets here
async_predict_result_json
=
async_predict_result.model_dump_json()
# We recompute expected Baseten signatures with each webhook secret
for
webhook_secret
in
WEBHOOK_SECRETS
:
for
actual_signature
in
baseten_signature.replace(
"v1="
,
""
).split(
","
):
expected_signature
=
hmac.digest(
webhook_secret.encode(
"utf-8"
),
async_predict_result_json.encode(
"utf-8"
),
hashlib.sha256,
).hex()
3
Compare signatures
Compare the expected Baseten signature with the actual computed signature using
compare_digest
, which will return a boolean representing whether the signatures are indeed the same.
Copy
Ask AI
hmac.compare_digest(expected_signature, actual_signature)
​
Keeping webhook secrets secure
We recommend periodically rotating webhook secrets.
In the event that a webhook secret is exposed, you’re able to rotate or remove it.
Rotating a secret in the UI will set the existing webhook secret to expire in 24 hours, and generate a new webhook secret. During this period, Baseten will include multiple signatures in the signature headers.
Removing webhook secrets could cause your signature validation to fail. Recreate a webhook secret after deleting and ensure your signature validation code is up to date with the new webhook secret.
​
FAQs
​
Can I run sync and async requests on the same model?
Yes, you can run both sync and async requests on the same model. Sync requests always take priority over async requests. Keep the following in mind:
Rate Limits
: Ensure you adhere to rate limits, as they apply to async requests.
Learn more
Concurrency
: Both sync and async requests count toward the total number of concurrent requests.
Learn more
Was this page helpful?
Yes
No
Previous
Structured output (JSON mode)
Enforce an output schema on LLM inference
Next
On this page
Quick start
User guide
Configuring the webhook endpoint
Making async requests
Getting and canceling async requests
Processing async predict results
Observability
Securing async inference
Creating webhook secrets
Validating webhook signatures
Keeping webhook secrets secure
FAQs
Can I run sync and async requests on the same model?
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/inference/calling-your-model:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Developing a Chain
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Exporting metrics
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Inference
Call your model
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
Once deployed, your model is accessible via an
API endpoint
. To make an inference request, you’ll need:
Model ID
An
API key
for your Baseten account.
JSON-serializable model input
​
Predict API endpoints
Baseten provides multiple endpoints for different inference modes:
/predict
– Standard synchronous inference.
/async_predict
– Asynchronous inference for long-running tasks.
Endpoints are available for environments and all deployments. See the
API reference
for details.
​
Sync API endpoints
Custom servers support both
predict
endpoints as well as a special
sync
endpoint. By using the
sync
endpoint you are able to call different routes in your custom server.
Copy
Ask AI
https://model-{model-id}.api.baseten.co/environments/{production}/sync/{route}
Here are a few example for the given example that show how the sync endpoint maps to the custom server’s routes.
https://model-{model_id}.../sync/health
->
/health
https://model-{model_id}.../sync/items
->
/items
https://model-{model_id}.../sync/items/123
->
/items/123
​
OpenAI SDK
When deploying a model with Engine Builder, you will get an OpenAI compatible server. If you are already using one of the OpenAI SDKs, you will simply need to update the base url to your Baseten model URL and include your Baseten API Key.
Copy
Ask AI
import
os
from
openai
import
OpenAI
model_id
=
"abcdef"
#
TODO
: replace with your model id
api_key
=
os.environ.get(
"BASETEN_API_KEY"
)
model_url
=
f
"https://model-
{
model_id
}
.api.baseten.co/environments/production/sync/v1"
client
=
OpenAI(
base_url
=
model_url,
api_key
=
api_key,
)
stream
=
client.chat.completions.create(
model
=
"baseten"
,
messages
=
[
{
"role"
:
"system"
,
"content"
:
"You are a helpful assistant."
},
{
"role"
:
"user"
,
"content"
:
"What is the capital of France?"
}
],
stream
=
True
,
)
for
chunk
in
stream:
if
chunk.choices[
0
].delta.content
is
not
None
:
print
(chunk.choices[
0
].delta.content,
end
=
""
)
​
Alternative invocation methods
Truss CLI
:
truss predict
Model Dashboard
: “Playground” button in the Baseten UI
Was this page helpful?
Yes
No
Previous
Streaming
How to call a model that has a streaming-capable endpoint.
Next
On this page
Predict API endpoints
Sync API endpoints
OpenAI SDK
Alternative invocation methods
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/inference/concepts:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Developing a Chain
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Exporting metrics
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Inference
Concepts
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
Inference on Baseten is designed for flexibility, efficiency, and scalability. Models can be served
synchronously
,
asynchronously
, or via
streaming
to meet different performance and latency needs.
Synchronously
inference is ideal for low-latency, real-time responses.
Asynchronously
inference handles long-running tasks efficiently without blocking resources.
Streaming
inference delivers partial results as they become available for faster response times.
Baseten supports various input and output formats, including structured data, binary files, and function calls, making it adaptable to different workloads.
Was this page helpful?
Yes
No
Previous
Call your model
Run inference on deployed models
Next
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/inference/function-calling:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Developing a Chain
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Structured output (JSON mode)
Function calling (tool use)
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Exporting metrics
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Structured LLM output
Function calling (tool use)
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
Function calling requires an LLM deployed using the
TensorRT-LLM Engine
Builder
.
To use function calling:
Define a set of functions/tools in Python.
Pass the function set to the LLM with the
tools
argument.
Receive selected function(s) as output.
With function calling, it’s essential to understand that
the LLM itself is not capable of executing the code in the function
. Instead, the LLM is used to suggest appropriate function(s), if they exist, based on the prompt. Any code execution must be handled outside of the LLM call – a great use for
chains
.
​
Define functions in Python
Functions can be anything: API calls, ORM access, SQL queries, or just a script. It’s essential that functions are well-documented; the LLM relies on the docstrings to select the correct function.
As a simple example, consider the four basic functions of a calculator:
Copy
Ask AI
def
multiply
(
a
:
float
,
b
:
float
):
"""
A function that multiplies two numbers
Args:
a: The first number to multiply
b: The second number to multiply
"""
return
a
*
b
def
divide
(
a
:
float
,
b
:
float
):
"""
A function that divides two numbers
Args:
a: The dividend
b: The divisor
"""
return
a
/
b
def
add
(
a
:
float
,
b
:
float
):
"""
A function that adds two numbers
Args:
a: The first number
b: The second number
"""
return
a
+
b
def
subtract
(
a
:
float
,
b
:
float
):
"""
A function that subtracts two numbers
Args:
a: The number to subtract from
b: The number to subtract
"""
return
a
-
b
These functions must be serialized into LLM-accessible tools:
Copy
Ask AI
from
transformers.utils
import
get_json_schema
calculator_functions
=
{
'multiply'
: multiply,
'divide'
: divide,
'add'
: add,
'subtract'
: subtract
}
tools
=
[get_json_schema(f)
for
f
in
calculator_functions.values()]
​
Pass functions to the LLM
The input spec for models like Llama 3.1 includes a
tools
key that we use to pass the functions:
Copy
Ask AI
import
json
import
requests
payload
=
{
"messages"
: [
{
"role"
:
"system"
,
"content"
:
"You are a helpful assistant"
},
{
"role"
:
"user"
,
"content"
:
"What is 3.14+3.14?"
},
],
"tools"
: tools,
# tools are provided in the same format as OpenAI's API
"tool_choice"
:
"auto"
,
# auto is default - the model will choose whether or not to make a function call
}
MODEL_ID
=
""
BASETEN_API_KEY
=
""
resp
=
requests.post(
f
"https://model-
{
MODEL_ID
}
.api.baseten.co/production/predict"
,
headers
=
{
"Authorization"
:
f
"Api-Key
{
BASETEN_API_KEY
}
"
},
json
=
payload,
)
​
tool_choice: auto (default) – may return a function
The default
tool_choice
option,
auto
, leaves it up to the LLM whether to return one function, multiple functions, or no functions at all, depending on what the model feels is most appropriate based on the prompt.
​
tool_choice: required – will always return a function
The
required
option for
tool_choice
means that the LLM is guaranteed to chose at least one function, no matter what.
​
tool_choice: none – will always return a function
The
none
option for
tool_choice
means that the LLM will
not
return a function, and will instead produce ordinary text output. This is useful when you want to provide the full context of a conversation without adding and dropping the
tools
parameter call-by-call.
​
tool_choice: direct – will return a specified function
You can also pass a specific function directly into the call, which is guaranteed to be returned. This is useful if you want to hardcode specific behavior into your model call for testing or conditional execution.
Copy
Ask AI
"tool_choice"
: {
"type"
:
"function"
,
"function"
: {
"name"
:
"subtract"
}}
​
Receive function(s) as output
When the model returns functions, they’ll be a list that can be parsed as follows:
Copy
Ask AI
func_calls
=
json.loads(resp.text)
# In this example, we execute the first function (one of +-/*) on the provided parameters
func_call
=
func_calls[
0
]
calculator_functions[func_call[
"name"
]](
**
func_call[
"parameters"
])
After reading the LLM’s selection, your execution environment can run the necessary functions. For more on combining LLMs with other logic, see the
chains documentation
.
Was this page helpful?
Yes
No
Previous
Model I/O in binary
Decode and save binary model output
Next
On this page
Define functions in Python
Pass functions to the LLM
tool_choice: auto (default) – may return a function
tool_choice: required – will always return a function
tool_choice: none – will always return a function
tool_choice: direct – will return a specified function
Receive function(s) as output
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/inference/integrations:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Developing a Chain
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Exporting metrics
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Inference
Integrations
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
Chainlit
Build your own open-source ChatGPT with Baseten and Chainlit.
LangChain
Use your Baseten models in the LangChain ecosystem.
LiteLLM
Use your Baseten models in LiteLLM projects.
LiveKit
Build real-time voice agents with TTS models hosted on Baseten.
Build your own
Want to integrate Baseten with your platform or project? Reach out to
support@baseten.co
and we’ll help with building
and marketing the integration.
Was this page helpful?
Yes
No
Previous
Overview
An introduction to Baseten Training for streamlining and managing the model training lifecycle.
Next
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/inference/output-format/binary:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Developing a Chain
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Model I/O in binary
Model I/O with files
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Exporting metrics
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Output formats
Model I/O in binary
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
Baseten and Truss natively support model I/O in binary and use msgpack encoding for efficiency.
​
Deploy a basic Truss for binary I/O
If you need a deployed model to try the invocation examples below, follow these steps to create and deploy a super basic Truss that accepts and returns binary data. The Truss performs no operations and is purely illustrative.
Steps for deploying an example Truss
1
Create a Truss
To create a Truss, run:
Copy
Ask AI
truss
init
binary_test
This creates a Truss in a new directory
binary_test
. By default, newly created Trusses implement an identity function that returns the exact input they are given.
2
Add logging
Optionally, modify
binary_test/model/model.py
to log that the data received is of type
bytes
:
binary_test/model/model.py
Copy
Ask AI
def
predict
(
self
,
model_input
):
# Run model inference here
print
(
f
"Input type:
{
type
(model_input[
'byte_data'
])
}
"
)
return
model_input
3
Deploy the Truss
Deploy the Truss to Baseten with:
Copy
Ask AI
truss
push
​
Send raw bytes as model input
To send binary data as model input:
Set the
content-type
HTTP header to
application/octet-stream
Use
msgpack
to encode the data or file
Make a POST request to the model
This code sample assumes you have a file
Gettysburg.mp3
in the current working directory. You can download the
11-second file from our CDN
or replace it with your own file.
call_model.py
Copy
Ask AI
import
os
import
requests
import
msgpack
model_id
=
"MODEL_ID"
# Replace with your model ID
deployment
=
"development"
# `development`, `production`, or a deployment ID
baseten_api_key
=
os.environ[
"BASETEN_API_KEY"
]
# Specify the URL to which you want to send the POST request
url
=
f
"https://model-
{
model_id
}
.api.baseten.co/
{
deployment
}
/predict"
headers
=
{
"Authorization"
:
f
"Api-Key
{
baseten_api_key
}
"
,
"content-type"
:
"application/octet-stream"
,
}
with
open
(
'Gettysburg.mp3'
,
'rb'
)
as
file
:
response
=
requests.post(
url,
headers
=
headers,
data
=
msgpack.packb({
'byte_data'
:
file
.read()})
)
print
(response.status_code)
print
(response.headers)
To support certain types like numpy and datetime values, you may need to
extend client-side
msgpack
encoding with the same
encoder and decoder used
by
Truss
.
​
Parse raw bytes from model output
To use the output of a non-streaming model response, decode the response content.
call_model.py
Copy
Ask AI
# Continues `call_model.py` from above
binary_output
=
msgpack.unpackb(response.content)
# Change extension if not working with mp3 data
with
open
(
'output.mp3'
,
'wb'
)
as
file
:
file
.write(binary_output[
"byte_data"
])
​
Streaming binary outputs
You can also stream output as binary. This is useful for sending large files or reading binary output as it is generated.
In the
model.py
, you must create a streaming output.
model/model.py
Copy
Ask AI
# Replace the predict function in your Truss
def
predict
(
self
,
model_input
):
import
os
current_dir
=
os.path.dirname(
__file__
)
file_path
=
os.path.join(current_dir,
"tmpfile.txt"
)
with
open
(file_path,
mode
=
"wb"
)
as
file
:
file
.write(
bytes
(model_input[
"text"
],
encoding
=
"utf-8"
))
def
iterfile
():
# Get the directory of the current file
current_dir
=
os.path.dirname(
__file__
)
# Construct the full path to the .wav file
file_path
=
os.path.join(current_dir,
"tmpfile.txt"
)
with
open
(file_path,
mode
=
"rb"
)
as
file_like:
yield from
file_like
return
iterfile()
Then, in your client, you can use streaming output directly without decoding.
stream_model.py
Copy
Ask AI
import
os
import
requests
import
json
model_id
=
"MODEL_ID"
# Replace with your model ID
deployment
=
"development"
# `development`, `production`, or a deployment ID
baseten_api_key
=
os.environ[
"BASETEN_API_KEY"
]
# Specify the URL to which you want to send the POST request
url
=
f
"https://model-
{
model_id
}
.api.baseten.co/
{
deployment
}
/predict"
headers
=
{
"Authorization"
:
f
"Api-Key
{
baseten_api_key
}
"
,
}
s
=
requests.Session()
with
s.post(
# Endpoint for production deployment, see API reference for more
f
"https://model-
{
model_id
}
.api.baseten.co/
{
deployment
}
/predict"
,
headers
=
{
"Authorization"
:
f
"Api-Key
{
baseten_api_key
}
"
},
data
=
json.dumps({
"text"
:
"Lorem Ipsum"
}),
# Include stream=True as an argument so the requests libray knows to stream
stream
=
True
,
)
as
response:
for
token
in
response.iter_content(
1
):
print
(token)
# Prints bytes
Was this page helpful?
Yes
No
Previous
Model I/O with files
Call models by passing a file or URL
Next
On this page
Deploy a basic Truss for binary I/O
Send raw bytes as model input
Parse raw bytes from model output
Streaming binary outputs
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/inference/output-format/files:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Developing a Chain
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Model I/O in binary
Model I/O with files
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Exporting metrics
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Output formats
Model I/O with files
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
Baseten supports a wide variety of file-based I/O approaches. These examples show our recommendations for working with files during model inference, whether local or remote, public or private, in the Truss or in your invocation code.
​
Files as input
​
Example: Send a file with JSON-serializable content
The Truss CLI has a
-f
flag to pass file input. If you’re using the API endpoint via Python, get file contents with the standard
f.read()
function.
Truss CLI
Python script
Copy
Ask AI
truss
predict
-f
input.json
​
Example: Send a file with non-serializable content
The
-f
flag for
truss predict
only applies to JSON-serializable content. For other files, like the audio files required by
MusicGen Melody
, the file content needs to be base64 encoded before it is sent.
Copy
Ask AI
import
urllib3
model_id
=
""
# Read secrets from environment variables
baseten_api_key
=
os.environ[
"BASETEN_API_KEY"
]
# Open a local file
with
open
(
"mymelody.wav"
,
"rb"
)
as
f:
# mono wav file, 48khz sample rate
# Convert file contents into JSON-serializable format
encoded_data
=
base64.b64encode(f.read())
encoded_str
=
encoded_data.decode(
"utf-8"
)
# Define the data payload
data
=
{
"prompts"
: [
"happy rock"
,
"energetic EDM"
,
"sad jazz"
],
"melody"
: encoded_str,
"duration"
:
8
}
# Make the POST request
response
=
requests.post(url,
headers
=
headers,
data
=
data)
resp
=
urllib3.request(
"POST"
,
# Endpoint for production deployment, see API reference for more
f
"https://model-
{
model_id
}
.api.baseten.co/production/predict"
,
headers
=
{
"Authorization"
:
f
"Api-Key
{
baseten_api_key
}
"
},
json
=
data
)
data
=
resp.json()[
"data"
]
# Save output to files
for
idx, clip
in
enumerate
(data):
with
open
(
f
"clip_
{
idx
}
.wav"
,
"wb"
)
as
f:
f.write(base64.b64decode(clip))
​
Example: Send a URL to a public file
Rather than encoding and serializing a file to send in the HTTP request, you can instead write a Truss that takes a URL as input and loads the content in the
preprocess()
function.
Here’s an example from
Whisper in the model library
.
Copy
Ask AI
from
tempfile
import
NamedTemporaryFile
import
requests
# Get file content without blocking GPU
def
preprocess
(
self
,
request
):
resp
=
requests.get(request[
"url"
])
return
{
"content"
: resp.content}
# Use file content in model inference
def
predict
(
self
,
model_input
):
with
NamedTemporaryFile()
as
fp:
fp.write(model_input[
"content"
])
result
=
whisper.transcribe(
self
._model,
fp.name,
temperature
=
0
,
best_of
=
5
,
beam_size
=
5
,
)
segments
=
[
{
"start"
: r[
"start"
],
"end"
: r[
"end"
],
"text"
: r[
"text"
]}
for
r
in
result[
"segments"
]
]
return
{
"language"
: whisper.tokenizer.
LANGUAGES
[result[
"language"
]],
"segments"
: segments,
"text"
: result[
"text"
],
}
​
Files as output
​
Example: Save model output to local file
When saving model output to a local file, there’s nothing Baseten-specific about the code. Just use the standard
>
operator in bash or
file.write()
function in Python to save the model output.
Truss CLI
Python script
Copy
Ask AI
truss
predict
-d
'"Model input!"'
>
output.json
Output for some models, like image and audio generation models, may need to be decoded before you save it. See our
image generation example
for how to parse base64 output.
Was this page helpful?
Yes
No
Previous
Integrations
Integrate your models with tools like LangChain, LiteLLM, and more.
Next
On this page
Files as input
Example: Send a file with JSON-serializable content
Example: Send a file with non-serializable content
Example: Send a URL to a public file
Files as output
Example: Save model output to local file
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/inference/streaming:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Developing a Chain
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Exporting metrics
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Inference
Streaming
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
Any model could be packaged with support for streaming output, but it only makes sense to do so for models where:
Generating a complete output takes a relatively long time.
The first tokens of output are useful without the context of the rest of the output.
Reducing the time to first token improves the user experience.
LLMs in chat applications are the perfect use case for streaming model output.
​
Example: Streaming with Mistral
Mistral 7B Instruct
from Baseten’s model library is a recent LLM with streaming support. Invocation should be the same for any other model library LLM as well as any Truss that follows the same standard.
Deploy Mistral 7B Instruct
or a similar LLM to run the following examples.
​
Truss CLI
The Truss CLI has built-in support for streaming model output.
Copy
Ask AI
truss
predict
-d
'{"prompt": "What is the Mistral wind?", "stream": true}'
​
API endpoint
When using a streaming endpoint with cURL, use the
--no-buffer
flag to stream output as it is received.
As with all cURL invocations, you’ll need a model ID and API key.
Copy
Ask AI
curl
-X
POST
https://app.baseten.co/models/MODEL_ID/predict
\
-H
'Authorization: Api-Key YOUR_API_KEY'
\
-d
'{"prompt": "What is the Mistral wind?", "stream": true}'
\
--no-buffer
​
Python application
Let’s take things a step further and look at how to integrate streaming output with a Python application.
Copy
Ask AI
import
requests
import
json
import
os
# Model ID for production deployment
model_id
=
""
# Read secrets from environment variables
baseten_api_key
=
os.environ[
"BASETEN_API_KEY"
]
# Open session to enable streaming
s
=
requests.Session()
with
s.post(
# Endpoint for production deployment, see API reference for more
f
"https://model-
{
model_id
}
.api.baseten.co/production/predict"
,
headers
=
{
"Authorization"
:
f
"Api-Key
{
baseten_api_key
}
"
},
# Include "stream": True in the data dict so the model knows to stream
data
=
json.dumps({
"prompt"
:
"What even is AGI?"
,
"stream"
:
True
,
"max_new_tokens"
:
4096
}),
# Include stream=True as an argument so the requests libray knows to stream
stream
=
True
,
)
as
resp:
# Print the generated tokens as they get streamed
for
content
in
resp.iter_content():
print
(content.decode(
"utf-8"
),
end
=
""
,
flush
=
True
)
Was this page helpful?
Yes
No
Previous
Async inference
Run asynchronous inference on deployed models
Next
On this page
Example: Streaming with Mistral
Truss CLI
API endpoint
Python application
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/inference/structured-output:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Developing a Chain
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Structured output (JSON mode)
Function calling (tool use)
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Exporting metrics
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Structured LLM output
Structured output (JSON mode)
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
Structured outputs requires an LLM deployed using the
TensorRT-LLM Engine Builder
.
If you want to try this structured output example code for yourself, deploy
this implementation of Llama 3.1 8B
.
To generate structured outputs:
Define an object schema with
Pydantic
.
Pass the schema to the LLM with the
response_format
argument.
Receive output that is guaranteed to match the provided schema, including types and validations like
max_length
.
Using structured output, you should observe approximately equivalent tokens per second output speed to an ordinary call to the model after an initial delay for schema processing. If you’re interested in the mechanisms behind structured output, check out this
engineering deep dive on our blog
.
​
Schema generation with Pydantic
Pydantic
is an industry standard Python library for data validation. With Pydantic, we’ll build precise schemas for LLM output to match.
For example, here’s a schema for a basic
Person
object.
Copy
Ask AI
from
pydantic
import
BaseModel, Field
from
typing
import
Optional
from
datetime
import
date
class
Person
(
BaseModel
):
first_name:
str
=
Field(
...
,
description
=
"The person's first name"
,
max_length
=
50
)
last_name:
str
=
Field(
...
,
description
=
"The person's last name"
,
max_length
=
50
)
age:
int
=
Field(
...
,
description
=
"The person's age, must be a non-negative integer"
)
email:
str
=
Field(
...
,
description
=
"The person's email address"
)
Structured output supports multiple data types, required and optional fields, and additional validations like
max_length
.
​
Add response format to LLM call
The first time that you pass a given schema for the model, it can take a
minute for the schema to be processed and cached. Subsequent calls with the
same schema will run at normal speeds.
Once your object is defined, you can add it as a parameter to your LLM call with the
response_format
field:
Copy
Ask AI
import
json
import
requests
payload
=
{
"messages"
: [
{
"role"
:
"system"
,
"content"
:
"You are a helpful assistant"
},
{
"role"
:
"user"
,
"content"
:
"Make up a new person!"
},
],
"max_tokens"
:
512
,
"response_format"
: {
# Add this parameter to use structured outputs
"type"
:
"json_schema"
,
"json_schema"
: {
"schema"
: Person.model_json_schema()},
},
}
MODEL_ID
=
""
BASETEN_API_KEY
=
""
resp
=
requests.post(
f
"https://model-
{
MODEL_ID
}
.api.baseten.co/production/predict"
,
headers
=
{
"Authorization"
:
f
"Api-Key
{
BASETEN_API_KEY
}
"
},
json
=
payload,
)
json.loads(resp.text)
The response may have an end of sequence token, which will need to be removed before the JSON can be parsed.
​
Parsing LLM output
From the LLM, we expect output in the following format:
Copy
Ask AI
{
"first_name"
:
"Astrid"
,
"last_name"
:
"Nyxoria"
,
"age"
:
28
,
"email"
:
"astrid.nyxoria@starlightmail.com"
}
This example output is valid, which can be double-checked with:
Copy
Ask AI
Person.parse_raw(resp.text)
Was this page helpful?
Yes
No
Previous
Function calling (tool use)
Use an LLM to select amongst provided tools
Next
On this page
Schema generation with Pydantic
Add response format to LLM call
Parsing LLM output
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/observability/access:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Developing a Chain
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Secure model inference
Workspace access control
Best practices for API keys
Best practices for secrets
Exporting metrics
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Security
Workspace access control
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
Permission
Admin
Creator
Manage members
✅
❌
Manage billing
✅
❌
Deploy models
✅
✅
Call models
✅
✅
Note:
Startup plan workspaces are limited to five users.
Contact us
if you need to increase this limit.
Was this page helpful?
Yes
No
Previous
Best practices for API keys
Securely access your Baseten models
Next
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/observability/api-keys:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Developing a Chain
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Secure model inference
Workspace access control
Best practices for API keys
Best practices for secrets
Exporting metrics
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Security
Best practices for API keys
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
API keys enable secure access to Baseten models for:
Model deployment
via Truss CLI
Inference API calls
(
truss predict
,
/wake
requests)
Model management
via the
management API
Metrics export
via the
/metrics
endpoint
You can create and revoke API keys from your
Baseten account
.
​
API key scope: Personal vs Workspace
There are two types of API keys on Baseten:
Personal API Keys:
Tied to a user account.
Inherit full workspace permissions.
Actions are linked to the specific user.
Workspace API Keys:
Shared across a workspace.
Can have full access or be restricted to specific models.
Use personal keys for testing and workspace keys for automation and production.
​
Using API keys with Truss
Add your API key to
~/.trussrc
for authentication:
~/.trussrc
Copy
Ask AI
[baseten]
remote_provider
=
baseten
api_key
=
abcdefgh.1234567890ABCDEFGHIJKL1234567890
remote_url
=
https://app.baseten.co
If rotating keys, update the file with the new key.
​
Using API keys with endpoints
Include the API key in request headers:
Copy
Ask AI
curl
-X
POST
https://app.baseten.co/models/MODEL_ID/predict
\
-H
'Authorization: Api-Key abcdefgh.1234567890ABCDEFGHIJKL1234567890'
\
-d
'MODEL_INPUT'
Or in Python:
Copy
Ask AI
headers
=
{
"Authorization"
:
"Api-Key abcdefgh.1234567890ABCDEFGHIJKL1234567890"
}
​
Tips for managing API keys
Best practices for API key use apply to your Baseten API keys:
Always store API keys securely.
Never commit API keys to your codebase.
Never share or leak API keys in notebooks or screenshots.
Name your API keys to keep them organized.
The
API key list on your Baseten account
shows when each key was first created and last used. Rotate API keys regularly and remove any unused API keys to reduce the risk of accidental leaks.
Was this page helpful?
Yes
No
Previous
Best practices for secrets
Securely store and access passwords, tokens, keys, and more
Next
On this page
API key scope: Personal vs Workspace
Using API keys with Truss
Using API keys with endpoints
Tips for managing API keys
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/observability/export-metrics/datadog:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Developing a Chain
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Exporting metrics
Overview
Export to Prometheus
Export to Datadog
Export to Grafana Cloud
Export to New Relic
Metrics support matrix
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Exporting metrics
Export to Datadog
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
Exporting metrics is in beta mode.
The Baseten metrics endpoint can be integrated with
OpenTelemetry Collector
by configuring a Prometheus receiver that scrapes the endpoint. This allows Baseten metrics to be pushed to a variety of popular exporters—see the
OpenTelemetry registry
for a full list.
Using OpenTelemetry Collector to push to Datadog
config.yaml
Copy
Ask AI
receivers
:
# Configure a Prometheus receiver to scrape the Baseten metrics endpoint.
prometheus
:
config
:
scrape_configs
:
-
job_name
:
'baseten'
scrape_interval
:
60s
metrics_path
:
'/metrics'
scheme
:
https
authorization
:
type
:
"Api-Key"
credentials
:
"{BASETEN_API_KEY}"
static_configs
:
-
targets
: [
'app.baseten.co'
]
processors
:
batch
:
exporters
:
# Configure a Datadog exporter.
datadog
:
api
:
key
:
"{DATADOG_API_KEY}"
service
:
pipelines
:
metrics
:
receivers
: [
prometheus
]
processors
: [
batch
]
exporters
: [
datadog
]
Was this page helpful?
Yes
No
Previous
Export to Grafana Cloud
Export metrics from Baseten to Grafana Cloud
Next
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/observability/export-metrics/grafana:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Developing a Chain
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Exporting metrics
Overview
Export to Prometheus
Export to Datadog
Export to Grafana Cloud
Export to New Relic
Metrics support matrix
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Exporting metrics
Export to Grafana Cloud
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
The Baseten + Grafana Cloud integration enables you to get real-time inference metrics within your existing Grafana setup.
​
Video tutorial
See below for step-by-step details from the video.
​
Set up the integration
For a visual guide, please follow along with the video above.
Open your Grafana Cloud account:
Navigate to “Home > Connections > Add new connection”.
In the search bar, type
Metrics Endpoint
and select it.
Give your scrape job a name like
baseten_metrics_scrape
.
Set the scrape job URL to
https://app.baseten.co/metrics
.
Leave the scrape interval set to
Every minute
.
Select
Bearer
for authentication credentials.
In your Baseten account, generate a metrics-only workspace API key.
In Grafana, enter the Bearer Token as
Api-Key abcd.1234567890
where the latter value is replaced by your API key.
Use the “Test Connection” button to ensure everything is entered correctly.
Click “Save Scrape Job.”
Click “Install.”
In your integrations list, select your new export and go through the “Enable” flow shown on video.
Now, you can navigate to your Dashboards tab, where you will see your data! Please note that it can take a couple of minutes for data to arrive and only new data will be scraped, not historical metrics.
​
Build a Grafana dashboard
Importing the data is a great first step, but you’ll need a dashboard to properly visualize the incoming information.
We’ve prepared a basic dashboard to get you started, which you can import by:
Downloading
baseten_grafana_dashboard.json
from
this GitHub Gist
.
Selecting “New > Import” from the dropdown in the top-right corner of the Dashboard page.
Dropping in the provided JSON file.
For visual reference in navigating the dashboard, please see the video above.
Was this page helpful?
Yes
No
Previous
Export to New Relic
Export metrics from Baseten to New Relic
Next
On this page
Video tutorial
Set up the integration
Build a Grafana dashboard
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/observability/export-metrics/new-relic:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Developing a Chain
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Exporting metrics
Overview
Export to Prometheus
Export to Datadog
Export to Grafana Cloud
Export to New Relic
Metrics support matrix
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Exporting metrics
Export to New Relic
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
Exporting metrics is in beta mode.
Export Baseten metrics to New Relic by integrating with
OpenTelemetry Collector
. This involves configuring a Prometheus receiver that scrapes Baseten’s metrics endpoint and configuring a New Relic exporter to send the metrics to your observability backend.
Using OpenTelemetry Collector to push to New Relic
config.yaml
Copy
Ask AI
receivers
:
# Configure a Prometheus receiver to scrape the Baseten metrics endpoint.
prometheus
:
config
:
scrape_configs
:
-
job_name
:
'baseten'
scrape_interval
:
60s
metrics_path
:
'/metrics'
scheme
:
https
authorization
:
type
:
"Api-Key"
credentials
:
"{BASETEN_API_KEY}"
static_configs
:
-
targets
: [
'app.baseten.co'
]
processors
:
batch
:
exporters
:
# Configure a New Relic exporter. Visit New Relic documentation to get your regional otlp endpoint.
otlphttp/newrelic
:
endpoint
:
https://otlp.nr-data.net
headers
:
api-key
:
"{NEW_RELIC_KEY}"
service
:
pipelines
:
metrics
:
receivers
: [
prometheus
]
processors
: [
batch
]
exporters
: [
otlphttp/newrelic
]
Was this page helpful?
Yes
No
Previous
Metrics support matrix
Which metrics can be exported
Next
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/observability/export-metrics/overview:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Developing a Chain
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Exporting metrics
Overview
Export to Prometheus
Export to Datadog
Export to Grafana Cloud
Export to New Relic
Metrics support matrix
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Exporting metrics
Overview
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
Baseten provides a metrics endpoint in Prometheus format, allowing integration with observability tools like Prometheus, OpenTelemetry Collector, Datadog Agent, and Vector.
​
Setting Up Metrics Scraping
1
Scrape endpoint: https://app.baseten.co/metrics
2
Authentication
Use the Authorization header with a
Baseten API key
:
Copy
Ask AI
{
"Authorization"
:
"Api-Key YOUR_API_KEY"
}
3
Scrape interval
Recommended 1-minute interval (metrics update every 30 seconds).
​
Supported Integrations
Baseten metrics can be collected via
OpenTelemetry Collector
and exported to:
Prometheus
Datadog
Grafana
New Relic
For available metrics, see the
supported metrics reference
.
​
Rate Limits
6 requests per minute per organization
Exceeding this limit results in
HTTP 429 (Too Many Requests)
responses.
To stay within limits, use a
1-minute scrape interval
.
Was this page helpful?
Yes
No
Previous
Export to Prometheus
Export metrics from Baseten to Prometheus
Next
On this page
Setting Up Metrics Scraping
Supported Integrations
Rate Limits
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/observability/export-metrics/prometheus:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Developing a Chain
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Exporting metrics
Overview
Export to Prometheus
Export to Datadog
Export to Grafana Cloud
Export to New Relic
Metrics support matrix
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Exporting metrics
Export to Prometheus
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
Exporting metrics is in beta mode.
To integrate with Prometheus, specify the Baseten metrics endpoint in a scrape config. For example:
prometheus.yml
Copy
Ask AI
global
:
scrape_interval
:
60s
scrape_configs
:
-
job_name
:
'baseten'
metrics_path
:
'/metrics'
authorization
:
type
:
"Api-Key"
credentials
:
"{BASETEN_API_KEY}"
static_configs
:
-
targets
: [
'app.baseten.co'
]
scheme
:
https
See the Prometheus docs for more details on
getting started
and
configuration options
.
Was this page helpful?
Yes
No
Previous
Export to Datadog
Export metrics from Baseten to Datadog
Next
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/observability/export-metrics/supported-metrics:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Developing a Chain
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Exporting metrics
Overview
Export to Prometheus
Export to Datadog
Export to Grafana Cloud
Export to New Relic
Metrics support matrix
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Exporting metrics
Metrics support matrix
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
Exporting metrics is in beta mode.
​
baseten_inference_requests_total
Cumulative number of requests to the model.
Type:
counter
Labels:
​
model_id
label
required
The ID of the model.
​
model_name
label
required
The name of the model.
​
deployment_id
label
required
The ID of the deployment.
​
status_code
label
required
The status code of the response.
​
is_async
label
required
Whether the request was an
async inference request
.
​
environment
label
The environment that the deployment corresponds to. Empty if the deployment is not associated with an environment.
​
rollout_phase
label
The phase of the deployment in the
promote to production process
. Empty if the deployment is not associated with an environment.
Possible values:
"promoting"
"stable"
​
baseten_end_to_end_response_time_seconds
End-to-end response time in seconds.
Type:
histogram
Labels:
​
model_id
label
required
The ID of the model.
​
model_name
label
required
The name of the model.
​
deployment_id
label
required
The ID of the deployment.
​
status_code
label
required
The status code of the response.
​
is_async
label
required
Whether the request was an
async inference request
.
​
environment
label
The environment that the deployment corresponds to. Empty if the deployment is not associated with an environment.
​
rollout_phase
label
The phase of the deployment in the
promote to production process
. Empty if the deployment is not associated with an environment.
Possible values:
"promoting"
"stable"
​
baseten_container_cpu_usage_seconds_total
Cumulative CPU time consumed by the container in core-seconds.
Type:
counter
Labels:
​
model_id
label
required
The ID of the model.
​
model_name
label
required
The name of the model.
​
deployment_id
label
required
The ID of the deployment.
​
replica
label
required
The ID of the replica.
​
environment
label
The environment that the deployment corresponds to. Empty if the deployment is
not associated with an environment.
​
rollout_phase
label
The phase of the deployment in the
promote to production process
. Empty if the deployment is not associated with an environment.
Possible values:
"promoting"
"stable"
​
baseten_replicas_active
Number of replicas ready to serve model requests.
Type:
gauge
Labels:
​
model_id
label
required
The ID of the model.
​
model_name
label
required
The name of the model.
​
deployment_id
label
required
The ID of the deployment.
​
environment
label
The environment that the deployment corresponds to. Empty if the deployment is
not associated with an environment.
​
rollout_phase
label
The phase of the deployment in the
promote to production process
. Empty if the deployment is not associated with an environment.
Possible values:
"promoting"
"stable"
​
baseten_replicas_starting
Number of replicas starting up—i.e. either waiting for resources to be available or loading the model.
Type:
gauge
Labels:
​
model_id
label
required
The ID of the model.
​
model_name
label
required
The name of the model.
​
deployment_id
label
required
The ID of the deployment.
​
environment
label
The environment that the deployment corresponds to. Empty if the deployment is
not associated with an environment.
​
rollout_phase
label
The phase of the deployment in the
promote to production process
. Empty if the deployment is not associated with an environment.
Possible values:
"promoting"
"stable"
​
baseten_container_cpu_memory_working_set_bytes
Cumulative CPU time consumed by the container in seconds.
Type:
gauge
Labels:
​
model_id
label
required
The ID of the model.
​
model_name
label
required
The name of the model.
​
deployment_id
label
required
The ID of the deployment.
​
replica
label
required
The ID of the replica.
​
environment
label
The environment that the deployment corresponds to. Empty if the deployment is not associated with an environment.
​
rollout_phase
label
The phase of the deployment in the
promote to production process
. Empty if the deployment is not associated with an environment.
Possible values:
"promoting"
"stable"
​
baseten_gpu_memory_used
GPU memory used in MiB.
Type:
gauge
Labels:
​
model_id
label
required
The ID of the model.
​
model_name
label
required
The name of the model.
​
deployment_id
label
required
The ID of the deployment.
​
replica
label
required
The ID of the replica.
​
gpu
label
required
The ID of the GPU.
​
environment
label
The environment that the deployment corresponds to. Empty if the deployment is not associated with an environment.
​
rollout_phase
label
The phase of the deployment in the
promote to production process
. Empty if the deployment is not associated with an environment.
Possible values:
"promoting"
"stable"
​
baseten_gpu_utilization
GPU utilization as a percentage (between 0 and 100).
Type:
gauge
Labels:
​
model_id
label
required
The ID of the model.
​
model_name
label
required
The name of the model.
​
deployment_id
label
required
The ID of the deployment.
​
replica
label
required
The ID of the replica.
​
gpu
label
required
The ID of the GPU.
​
environment
label
The environment that the deployment corresponds to. Empty if the deployment is not associated with an environment.
​
rollout_phase
label
The phase of the deployment in the promote to production process. Empty if the deployment is not associated with an environment.
Possible values:
"promoting"
"stable"
Was this page helpful?
Yes
No
Previous
Tracing
Investigate the prediction flow in detail
Next
On this page
baseten_inference_requests_total
baseten_end_to_end_response_time_seconds
baseten_container_cpu_usage_seconds_total
baseten_replicas_active
baseten_replicas_starting
baseten_container_cpu_memory_working_set_bytes
baseten_gpu_memory_used
baseten_gpu_utilization
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/observability/health:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Developing a Chain
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Exporting metrics
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Observability
Status and health
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
​
Model statuses
Healthy states:
Active
: The deployment is active and available. It can be called with
truss predict
or from its API endpoints.
Scaled to zero
: The deployment is active but is not consuming resources. It will automatically start up when called, then scale back to zero after traffic ceases.
Starting up
: The deployment is starting up from a scaled to zero state after receiving a request.
Inactive
: The deployment is unavailable and is not consuming resources. It may be manually reactivated.
Error states:
Unhealthy
: The deployment is active but is in an unhealthy state due to errors while running, such as an external service it relies on going down or a problem in your Truss that prevents it from responding to requests.
Build failed
: The deployment is not active due to a Docker build failure.
Deployment failed
: The deployment is not active due to a model deployment failure.
​
Fixing unhealthy deployments
If you have an unhealthy or failed deployment, check the model logs to see if there’s any indication of what the problem is. You can try deactivating and reactivating your deployment to see if the issue goes away. In the case of an external service outage, you may need to wait for the service to come back up before your deployment works again. For issues inside your Truss, you’ll need to diagnose your code to see what is making it unresponsive.
Was this page helpful?
Yes
No
Previous
Secure model inference
Keeping your models safe and private
Next
On this page
Model statuses
Fixing unhealthy deployments
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/observability/metrics:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Developing a Chain
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Exporting metrics
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Observability
Metrics
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
The Metrics tab in the model dashboard provides deployment-specific insights into model load and performance. Use the dropdowns to filter by environment or deployment and time range.
​
Inference volume
Tracks the request rate over time, segmented by HTTP status codes:
2xx
: 🟢 Successful requests
4xx
: 🟡 Client errors
5xx
: 🔴 Server errors (includes model prediction exceptions)
​
Response time
Measured at different percentiles (p50, p90, p95, p99):
End-to-end response time:
Includes cold starts, queuing, and inference (excludes client-side latency). Reflects real-world performance.
Inference time:
Covers only model execution, including pre/post-processing. Useful for optimizing single-replica performance.
Time to first byte:
Measures the time-to-first-byte time distribution, including any queueing and routing time. A proxy for TTFT.
​
Request and response size
Measured at different percentiles (p50, p90, p95, p99):
Request size:
Tracks the request size distribution. A proxy for input tokens.
Response size:
Tracks the response size distribution. A proxy for generated tokens.
​
Replicas
Tracks the number of
active
and
starting
replicas:
Starting:
Waiting for resources or loading the model.
Active:
Ready to serve requests.
For development deployments, a replica is considered active while running the live reload server.
​
CPU usage and memory
Displays resource utilization across replicas. Metrics are averaged and may not capture short spikes.
​
Considerations:
High CPU/memory usage
: May degrade performance—consider upgrading to a larger instance type.
Low CPU/memory usage
: Possible overprovisioning—switch to a smaller instance to reduce costs.
​
GPU usage and memory
Shows GPU utilization across replicas.
GPU usage
: Percentage of time a kernel function occupies the GPU.
GPU memory
: Total memory used.
​
Considerations:
High GPU load
: Can slow inference—check response time metrics.
High memory usage
: May cause out-of-memory failures.
Low utilization
: May indicate overprovisioning—consider a smaller GPU.
​
Async Queue Metrics
Time in Async Queue
: Time spent in the async queue before execution (p50, p90, p95, p99).
Async Queue Size
: Number of queued async requests.
​
Considerations:
Large queue size indicates requests are queued faster than they are processed.
To improve async throughput, increase the max replicas or adjust autoscaling concurrency.
Was this page helpful?
Yes
No
Previous
Status and health
Every model deployment in your Baseten workspace has a status to represent its activity and health.
Next
On this page
Inference volume
Response time
Request and response size
Replicas
CPU usage and memory
Considerations:
GPU usage and memory
Considerations:
Async Queue Metrics
Considerations:
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/observability/secrets:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Developing a Chain
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Secure model inference
Workspace access control
Best practices for API keys
Best practices for secrets
Exporting metrics
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Security
Best practices for secrets
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
Store sensitive credentials like API keys and passwords using the
secrets dashboard
.
Secrets are stored as
key-value pairs
(name → token).
Naming rules
: Non-alphanumeric characters are treated the same (e.g.,
hf_access_token
and
hf-access-token
are identical). Creating a new secret with a similar name overwrites the existing one.
Changes to secrets
immediately
affect all models using them.
Any model deployed to a workspace will have access to any secrets specified on the workspace.
​
Using secrets in Truss
1
Add the secret name to config.yaml, setting the value to null:
config.yaml
Copy
Ask AI
...
secrets
:
hf_access_token
:
null
...
Never set the actual value of the secret in
config.yaml
or any other file that gets committed to your codebase.
2
Access secrets in model.py:
model/model.py
Copy
Ask AI
def
__init__
(
self
,
**
kwargs
):
self
._secrets
=
kwargs[
"secrets"
]
3
Use secrets in load or predict:
model/model.py
Copy
Ask AI
def
load
(
self
):
self
._model
=
pipeline(
"fill-mask"
,
model
=
"baseten/docs-example-gated-model"
,
use_auth_token
=
self
._secrets[
"hf_access_token"
]
)
Was this page helpful?
Yes
No
Previous
Overview
Export metrics from Baseten to your observability stack
Next
On this page
Using secrets in Truss
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/observability/security:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Developing a Chain
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Secure model inference
Workspace access control
Best practices for API keys
Best practices for secrets
Exporting metrics
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Security
Secure model inference
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
Baseten maintains
SOC 2 Type II certification
and
HIPAA compliance
, with robust security measures beyond compliance.
​
Data privacy
Baseten does not store model inputs, outputs, or weights by default.
Model inputs/outputs
: Inputs for
async inference
are temporarily stored until processed. Outputs are never stored.
Model weights
: Loaded dynamically from sources like Hugging Face, GCS, or S3, moving directly to GPU memory.
Users can enable caching via Truss. Cached weights can be permanently deleted on request.
Postgres data tables
: Existing users may store data in Baseten’s hosted Postgres tables, which can be deleted anytime.
Baseten’s network accelerator optimizes model downloads.
Contact support
to disable it.
​
Workload security
Inference workloads are isolated to protect users and Baseten’s infrastructure.
Container security
:
No GPUs are shared across users.
Security tooling: Falco (Sysdig), Gatekeeper (Pod Security Policies).
Minimal privileges for workloads and nodes to limit incident impact.
Network security
:
Each customer has a dedicated Kubernetes namespace.
Isolation enforced via
Calico
.
Nodes run in a private subnet with firewall protections.
Pentesting
:
Extended pentesting by
RunSybil
(ex-OpenAI and CrowdStrike experts).
Malicious model deployments tested in a dedicated prod-like environment.
​
Self-hosted model inference
Baseten offers single-tenant environments and self-hosted deployments. The cloud version is recommended for ease of setup, cost efficiency, and elastic GPU access.
For self-hosting,
contact support
.
Was this page helpful?
Yes
No
Previous
Workspace access control
Workspaces use role-based access control (RBAC) with two roles:
Next
On this page
Data privacy
Workload security
Self-hosted model inference
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/observability/tracing:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Developing a Chain
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Exporting metrics
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Observability
Tracing
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
Baseten’s Truss server includes built-in
OpenTelemetry
(OTEL) instrumentation, with support for custom tracing.
Tracing helps diagnose performance bottlenecks but introduces minor overhead, so it is
disabled by default
.
​
Exporting builtin trace data to Honeycomb
Create a Honeycomb API
key and add it to
Baseten secrets
.
Update
config.yaml
for the target model:
config.yaml
Copy
Ask AI
environment_variables
:
HONEYCOMB_DATASET
:
your_dataset_name
runtime
:
enable_tracing_data
:
true
secrets
:
HONEYCOMB_API_KEY
:
'***'
Send requests with tracing
Provide traceparent headers for distributed tracing.
If omitted, Baseten generates random trace IDs.
​
Adding custom OTEL instrumentation
To define custom spans and events, integrate OTEL directly:
model.py
Copy
Ask AI
import
time
from
typing
import
Any, Generator
import
opentelemetry.exporter.otlp.proto.http.trace_exporter
as
oltp_exporter
import
opentelemetry.sdk.resources
as
resources
import
opentelemetry.sdk.trace
as
sdk_trace
import
opentelemetry.sdk.trace.export
as
trace_export
from
opentelemetry
import
trace
from
opentelemetry.sdk.resources
import
Resource
from
opentelemetry.sdk.trace
import
TracerProvider
trace.set_tracer_provider(
TracerProvider(
resource
=
Resource.create({resources.
SERVICE_NAME
:
"UserModel"
}))
)
tracer
=
trace.get_tracer(
__name__
)
trace_provider
=
trace.get_tracer_provider()
class
Model
:
def
__init__
(
self
,
**
kwargs
) ->
None
:
honeycomb_api_key
=
kwargs[
"secrets"
][
"HONEYCOMB_API_KEY"
]
honeycomb_exporter
=
oltp_exporter.OTLPSpanExporter(
endpoint
=
"https://api.honeycomb.io/v1/traces"
,
headers
=
{
"x-honeycomb-team"
: honeycomb_api_key,
"x-honeycomb-dataset"
:
"marius_testing_user"
,
},
)
honeycomb_processor
=
sdk_trace.export.BatchSpanProcessor(honeycomb_exporter)
trace_provider.add_span_processor(honeycomb_processor)
@tracer.start_as_current_span
(
"load_model"
)
def
load
(
self
):
...
def
preprocess
(
self
,
model_input
):
with
tracer.start_as_current_span(
"preprocess"
):
...
return
model_input
@tracer.start_as_current_span
(
"predict"
)
def
predict
(
self
,
model_input
: Any) -> Generator[
str
,
None
,
None
]:
with
tracer.start_as_current_span(
"start-predict"
)
as
span:
def
inner
():
time.sleep(
0.01
)
for
i
in
range
(
5
):
span.add_event(
"yield"
)
yield
str
(i)
return
inner()
Baseten’s built-in tracing
does not interfere
with user-defined OTEL implementations.
Was this page helpful?
Yes
No
Previous
Billing and usage
Manage payments and track overall Baseten usage
Next
On this page
Exporting builtin trace data to Honeycomb
Adding custom OTEL instrumentation
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/observability/usage:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Developing a Chain
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Exporting metrics
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Observability
Billing and usage
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
The
billing and usage dashboard
provides a breakdown of model usage and costs, updated hourly. Usage is tracked per deployment, and any available credits are automatically applied to your bill.
​
Billing
​
Credits
New workspaces receive free credits for testing and deployment.
If credits run out and no payment method is set, models will be deactivated until a payment method is added.
​
Payment method
Payment details can be added or updated on the
billing page
.
Payment information is securely stored with our payment processor, not Baseten.
On the
billing page
, you can set and update your payment method. Your payment information, including credit card numbers and bank information, is always stored securely with our payments processor and not by Baseten directly.
​
Invoice history
View past invoices and payments in the billing dashboard.
For questions,
contact support
.
​
Usage and billing FAQs
For full details, see our
pricing page
, but here are answers to some common questions:
​
How exactly is usage calculated?
Usage is billed per minute while a model is deploying, scaling, or serving requests.
Costs are based on the
instance type
used.
​
How often are payments due?
Initially, charges occur when usage
exceeds $50
or at the
end of the month
, whichever comes first.
After a history of successful payments, billing occurs
monthly
.
​
Do you offer volume discounts?
Volume discounts are available on the
Pro plan
.
Contact support
for details.
​
Do you offer education and non-profit discounts?
Yes, discounts are available for educational and nonprofit ML projects.
Contact support
to apply.
Was this page helpful?
Yes
No
Previous
Deployments
Troubleshoot common problems during model deployment
Next
On this page
Billing
Credits
Payment method
Invoice history
Usage and billing FAQs
How exactly is usage calculated?
How often are payments due?
Do you offer volume discounts?
Do you offer education and non-profit discounts?
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/overview:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Developing a Chain
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Exporting metrics
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Get started
Documentation
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
Build with Baseten
Baseten is a platform for deploying and serving AI models performantly,
scalably, and cost-efficiently.
Quick start
Choose from common AI/ML usecases and modalities to get started on Baseten quickly.
How Baseten works
Baseten makes it easy to deploy, serve, and scale AI models so you can focus on building, not infrastructure.
Baseten is an inference and training platform that lets you:
​
Deploy dedicated models with full control
Package any model for production
: Define dependencies, hardware, and custom code without needing to learn Docker. Build with your preferred frameworks (e.g.
PyTorch
,
transformers
,
diffusers
),
inference engines
(e.g.
TensorRT-LLM
,
SGLang
,
vLLM
), and serving tools (e.g.
Triton
) as well as
any package
installable via
pip
or
apt
.
Build complex AI systems
: Orchestrate multi-step workflows with
Chains
, combining models, business logic, and external APIs.
Deploy with confidence
:
Autoscale
models, manage
environments
, and roll out updates with zero-downtime deployments.
Run high-performance inference
: Serve
synchronous
,
asynchronous
, and
streaming
predictions with low-latency execution controls.
Monitor and optimize in production
: Monitor performance, debug issues, and
export metrics
with built-in observability tooling.
​
Start fast with model APIs
Try model APIs
: Model APIs provide a fast path to production with reliable, high-performance inference. Use OpenAI-compatible endpoints to integrate models like Llama, DeepSeek, and Qwen, with built-in support for structured outputs and tool calling.
​
Pre-train and fine-tune models
Run training jobs on scalable infrastructure
: Launch containerized training jobs with configurable environments, compute (CPU/GPU), and resource scaling. Supports any training framework via a framework-agnostic API.
Manage artifacts and streamline workflows
: Track experiments, organize training runs, and handle large artifacts like checkpoints and logs. Seamlessly transition from training to deployment within the Baseten ecosystem.
Resources
Examples
From deploying AI models to optimizing inference and scaling ML models.
Model library
Prebuilt, ready to deploy in one click models like DeepSeek, Llama, and
Qwen.
Explore API reference
API reference for calling deployed models, Chains or managing models and
your workspace.
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/quickstart:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Developing a Chain
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Exporting metrics
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Get started
Quick start
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
1
What modality are you working with?
Choose from common modalities like LLMs, transcription, and image generation to get started quickly.
LLMs
Build and deploy large language models
Transcription
Transcribe audio and video
Image generation
Rapidly generate images
Text to speech
Build humanlike experiences
Compound AI
Build real-time AI-native applications
Embeddings
Process millions of data points
Custom models
Deploy any model
2
Select a model or guide to get started...
Choose a use case or modality above first…
Was this page helpful?
Yes
No
Previous
Why Baseten
Baseten delivers fast, scalable AI/ML inference with enterprise-grade security and reliability—whether in our cloud or yours.
Next
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/status/status:

Baseten
home page
Search...
⌘
K
Baseten platform status
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Baseten platform status
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
This page automatically refreshes with real-time data from our status monitoring system.
All systems are operational.
Model Inference
Normal
Management API
Normal
Web Application
Normal
Last updated: Loading…
Was this page helpful?
Yes
No
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/training/concepts:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Developing a Chain
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Exporting metrics
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Training
Concepts
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
Baseten Training is designed to provide a structured yet flexible way to manage your machine learning training workflows. To use it effectively, it helps to understand the main ideas behind its components and how they fit together. This isn’t an API reference, but rather a guide to thinking about how to organize and execute your training tasks.
​
Organizing Your Work with
TrainingProject
s
A
TrainingProject
is a lightweight organization tool to help you group different
TrainingJob
s together.
While there a few technical details to consider, your team can use
TrainingProject
s to facilitate collaboration and organization.
​
Running a
TrainingJob
Once you have a
TrainingProject
, the actual work of training a model happens within a
TrainingJob
. Each
TrainingJob
represents a single, complete execution of your training script with a specific configuration.
What it is:
A
TrainingJob
is the fundamental unit of execution. It bundles together:
Your training code.
A base
image
.
The
compute
resources needed to run the job.
The
runtime
configurations like startup commands and environment variables.
Why use it:
Each job is a self-contained, reproducible experiment. If you want to try training your model with a different learning rate, more GPUs, or a slightly modified script, you can create new
TrainingJob
s while knowing that previous ones have been persisted on Baseten.
Lifecycle:
A job goes through various stages, from being created (
TRAINING_JOB_CREATED
), to resources being set up (
TRAINING_JOB_DEPLOYING
), to actively running your script (
TRAINING_JOB_RUNNING
), and finally to a terminal state like
TRAINING_JOB_COMPLETED
. More details on the job lifecycle can be found on the
Lifecycle
page.
​
Iterate Faster with the Training Cache
The training cache enables you to persist data between training jobs. This can significantly improve iteration speed by skipping expensive downloads and data transformations.
How to use it:
set
enable_cache=True
in your
Runtime
.
Cache Directories
: The cache will be mounted at
/root/.cache/huggingface
and at
$BT_RW_CACHE_DIR
.
Seeding Your Data
: For multi-gpu training, you should ensure that your data is seeded before running multi-process training jobs. You can do this by separating your training script into training script and data loading script.
Speedup:
For a 400 GB HF Dataset, you can expect to save
nearly an hour
of compute time for each job - data download and preparation have been done already!
​
Taking Advantage of Automated Checkpointing
Training machine learning models can be lengthy and resource-intensive. Baseten’s automated
Checkpointing
provides seemless storage for checkpoints and a jumping off point for inference and eval.
What it is:
Automated Checkpointing provides a seamless way to save model checkpoints to cloud storage.
Why use it:
Fault Tolerance:
Resume from the last saved checkpoint if a job fails, saving time and compute.
Experimentation:
Use saved checkpoints as starting points for new training runs with different hyperparameters or for transfer learning.
Model Evaluation:
Deploy intermediate model versions to track progress.
To enable checkpointing, add a
CheckpointingConfig
to the
Runtime
and set
enabled
to
True
. Baseten will automatically export the
$BT_CHECKPOINT_DIR
environment variable in your job’s environment. Ensure your code is writing checkpoints to the
$BT_CHECKPOINT_DIR
.
​
Multinode Training
Baseten Training supports multinode training via infiniband. To deploy a multinode training job:
Configure the
Compute
resource in your
TrainingJob
by setting the
node_count
to the number of nodes you’d like to use (e.g. 2).
Make sure you’ve properly integrated with the
Baseten provided environment variables
.
​
Securely Integrate with External Services with
SecretReference
Successfully training a model often requires many tools and services. Baseten provides
SecretReference
for secure handling of secrets.
How to use it:
Store your secret (e.g., an API key for Weights & Biases) in your Baseten workspace with a specific name. In your job’s configuration (e.g., environment variables), you refer to this secret by its name using
SecretReference
. The actual secret value is never exposed in your code.
How it works:
Baseten injects the secret value at runtime under the environment variable name that you specify.
​
Running Inference on Trained Models
The journey from training to a usable model in Baseten typically follows this path:
A
TrainingJob
with checkpointing enabled, produces one or more model artifacts.
You run
truss train deploy_checkpoint
to deploy a model from your most recent training job. You can read more about this at
Deploying Trained Models
.
Once deployed, your model will be avialble for inference via API. See more at
Calling Your Model
.
Was this page helpful?
Yes
No
Previous
Management
How to monitor, manage, and interact with your Baseten Training projects and jobs.
Next
On this page
Organizing Your Work with TrainingProjects
Running a TrainingJob
Iterate Faster with the Training Cache
Taking Advantage of Automated Checkpointing
Multinode Training
Securely Integrate with External Services with SecretReference
Running Inference on Trained Models
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/training/deployment:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Developing a Chain
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Exporting metrics
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Training
Deploying checkpoints
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
Baseten Training seamlessly integrates with Baseten’s model deployment capabilities. Once your
TrainingJob
has produced model checkpoints, you can deploy them as fully operational model endpoints.
To leverage deploying checkpoints, first ensure you have a
TrainingJob
that’s running with a
checkpointing_config
enabled.
Copy
Ask AI
runtime
=
definitions.Runtime(
start_commands
=
[
"/bin/sh -c './run.sh'"
,
],
checkpointing_config
=
definitions.CheckpointingConfig(
enabled
=
True
,
),
)
In your training code or configuration, ensure that your checkpoints are being written to the checkpointing directory, which can be referenced in via
$BT_CHECKPOINT_DIR
.
The contents of this directory are uploaded to Baseten’s storage and made immediately available for deployment.
(You can optionally specify a
checkpoint_path
in your
checkpointing_config
if you prefer to write to a specific directory).
To deploy your checkpoint(s) as a
Deployment
, you can:
run
truss train deploy_checkpoints [--job-id <job_id>]
and follow the setup wizard.
define an instance of a
DeployCheckpointsConfig
class (this is helpful for small changes that aren’t provided by the wizard) and run
truss train deploy_checkpoints --config <path_to_config_file>
.
When
deploy_checkpoints
is run,
truss
will construct a deployment
config.yml
and store it on disk in a temporary directory. If you’d like to preserve or modify the resulting deployment config, you can copy paste it
into a permanent directory and customize it as needed.
This file defines the source of truth for the deployment and can be deployed independently via
truss push
. See
deployments
for more details.
After successful deployment, your model will be deployed on Baseten, where you can run inference requests and evaluate performance. See
Calling Your Model
for more details.
Was this page helpful?
Yes
No
Previous
Metrics
Understand the load and performance of your model
Next
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/training/getting-started:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Developing a Chain
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Exporting metrics
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Training
Getting started
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
This guide will walk you through the initial setup and the process of submitting
your first
TrainingJob
using Baseten Training.
​
Prerequisites
Before you begin, ensure you have the following:
Baseten Account:
You’ll need an active Baseten account. If you don’t
have one, please sign up on the
Baseten web app
.
API Key:
Obtain an API key for your
Baseten account
.
This key is
required to authenticate with the Baseten API and SDK.
Truss SDK and CLI:
The
truss
package provides a python-native way for defining and running your training jobs.
jobs. The CLI provides a convenient way to deploy and manage your training jobs. Install or update it:
Copy
Ask AI
pip
install
-U
truss
​
Step 1: Define your training configuration
The primary way to define your training jobs is through a Python configuration
file, typically named
config.py
. This file uses the
truss
package to specify all
aspects of your
TrainingProject
and
TrainingJob
.
A simple example of a
config.py
file is shown below:
config.py
Copy
Ask AI
# Import necessary classes from the Baseten Training SDK
from
truss_train
import
definitions
from
truss.base
import
truss_config
# 1. Define a base image for your training job
BASE_IMAGE
=
"axolotlai/axolotl:main-20250324-py3.11-cu124-2.6.0"
# 2. Define the Runtime Environment for the Training Job
# This includes start commands and environment variables.
# Secrets from the baseten workspace like API keys are referenced using
# `SecretReference`.
training_runtime
=
definitions.Runtime(
start_commands
=
[
# Example: list of commands to run your training script
# "pip install -r requirements.txt", # pip install requirements on top of base image
"/bin/sh -c './run.sh'"
,
],
environment_variables
=
{
# Secrets (ensure these are configured in your Baseten workspace)
"HF_TOKEN"
: definitions.SecretReference(
name
=
"hf_access_token"
),
"WANDB_API_KEY"
: definitions.SecretReference(
name
=
"wandb_api_key"
),
"HELLO"
:
"WORLD"
},
)
# 3. Define the Compute Resources for the Training Job
training_compute
=
definitions.Compute(
accelerator
=
truss_config.AcceleratorSpec(
accelerator
=
truss_config.Accelerator.H100,
count
=
4
,
),
)
# 4. Define the Training Job
# This brings together the image, compute, and runtime configurations.
my_training_job
=
definitions.TrainingJob(
image
=
definitions.Image(
base_image
=
BASE_IMAGE
),
compute
=
training_compute,
runtime
=
training_runtime
)
# This config will be pushed using the Truss CLI.
# The association of the job to the project happens at the time of push.
first_project_with_job
=
definitions.TrainingProject(
name
=
project_name,
job
=
my_training_job
)
​
Key considerations for your Baseten training configuration file
Local Artifacts:
If your training requires local scripts (like
a
train.py
or a
run.sh
), helper files, or
configuration files (e.g., accelerate config), place them in the same
directory as your
config.py
or in subdirectories. When you push the training
job,
truss
will package these artifacts and upload them. They will be copied
into the container at the root of the base image’s working directory.
Ignore Folders and Files
: You can exclude specific files from being pushed by creating a
.truss_ignore
file in root directory of your project.
In this file, you can add entries in a style similar to
.gitignore
. Refer to the
CLI reference
for more details.
Secrets:
Ensure any secrets referenced via
SecretReference
(e.g.,
hf_access_token
,
wandb_api_key
) are defined in your Baseten
workspace settings
.
For a complete guide on the
TrainingJob
type, check out our
SDK-reference
.
​
What can I run in the
start_commands
?
In short, anything! Baseten Training is a framework-agnostic training platform. Any training framework and training methodology
is supported. Typically, a
run.sh
script is used. An example might look like this:
run.sh
Copy
Ask AI
#!/bin/bash
# Exit immediately if a command exits with a non-zero status
set
-e
# Install dependencies
pip
install
-r
requirements.txt
# authenticate with wandb
wandb
login
$WANDB_API_KEY
# defined via Runtime.EnvironmentVariables
# download models and datasets
huggingface-cli
download
google/gemma-3-27b-it
huggingface-cli
download
Abirate/english_quotes
--repo-type
dataset
# Run training
accelerate
launch
--config_file
config.yml
--num_processes
$BT_NUM_GPUS
train.py
​
Additional features
We’ve kept the above config simple to help you get off the ground - but there’s a lot more you can do Baseten Training:
Checkpointing
- automatically save and deploy your model checkpoints.
Training Cache
- speed up training by caching data and models between jobs.
Multinode
- train on multiple GPU nodes to make the most out of your compute.
​
Step 2: Submit Your Training Job
Once your
config.py
and any local artifacts are ready, you submit the training
job using the
truss
CLI
:
Copy
Ask AI
truss
train
push
config.py
This command does the following:
Parses your
config.py
.
Packages any local files in the directory (and subdirectories) alongside
config.py
.
Creates or updates the
TrainingProject
specified in your config.
Submits the defined
TrainingJob
under that project.
Upon successful submission, the CLI will output helpful information about your job:
Copy
Ask AI
✨ Training job successfully created!
🪵 View logs for your job via `truss train logs --job-id e3m512w [--tail]`
🔍 View metrics for your job via `truss train metrics --job-id e3m512w`
Keep the Job ID handy, as you’ll use it for
managing and monitoring your job
.
​
Next steps
Core Concepts
: Deepen your understanding of Baseten
Training and explore key features like
CheckpointingConfig
, Training Cache, and Multinode.
Management
: Learn how to check status,
view logs and metrics, and stop jobs.
Was this page helpful?
Yes
No
Previous
Concepts
Understanding the conceptual framework of Baseten Training for effective model development.
Next
On this page
Prerequisites
Step 1: Define your training configuration
Key considerations for your Baseten training configuration file
What can I run in the start_commands?
Additional features
Step 2: Submit Your Training Job
Next steps
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/training/management:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Developing a Chain
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Exporting metrics
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Training
Management
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
Once you have submitted training jobs, Baseten provides tools to manage your
TrainingProject
s and individual
TrainingJob
s. You can use the
CLI
or the
API
to manage your jobs.
​
TrainingProject
management
Listing Projects:
To view all your training projects:
Copy
Ask AI
truss
train
view
This command will list all
TrainingProject
s you have access to, typically showing their names and IDs. Additionally, this command will show all active jobs.
Viewing Jobs within a Project:
To see all jobs associated with a specific project, use its
project-id
(obtained when creating the project or from
truss train view
):
Copy
Ask AI
truss
train
view
--project-id
<
your_project_i
d
>
​
TrainingJob
management
After submitting a job with
truss train push config.py
, you receive a
project_id
and
job_id
.
Listing Jobs:
As shown above, you can list all jobs within a project using:
Copy
Ask AI
truss
train
view
--project-id
<
your_project_i
d
>
This will typically show job IDs, statuses, creation times, etc.
Checking Status and Retrieving Logs:
To view the logs for a specific job, you can tail them in real-time or fetch existing logs.
To view logs for the most recently submitted job in the current context (e.g., if you just pushed a job from your current terminal directory):
Copy
Ask AI
truss
train
logs
--tail
To view logs for a specific job using its
job-id
:
Copy
Ask AI
truss
train
logs
--job-id
<
your_job_i
d
>
[--tail]
Add
--tail
to follow the logs live.
Understanding Job Statuses:
The
truss train view
and
truss train logs
commands will help you track which status a job is in. For more on the job lifecycle, see the
Lifecycle
page.
Stopping a
TrainingJob
:
If you need to stop a running job, use the
stop
command with the job’s project ID and job ID:
Copy
Ask AI
truss
train
stop
--job-id
<
your_job_i
d
>
truss
train
stop
--all
# Stops all active jobs; Will prompt the user for confirmation.
This will transition the job to the
TRAINING_JOB_STOPPED
state.
Understanding Job Outputs & Checkpoints:
The primary outputs of a successful
TrainingJob
are model
checkpoints
(if checkpointing is enabled and configured).
These checkpoints are stored by Baseten. Refer to the
Checkpointing section in Core Concepts
for how
CheckpointingConfig
works.
When you are ready to
deploy a model
, you will specify which checkpoints to use. The
model_name
you assign during deployment (via
DeployCheckpointsConfig
) becomes the identifier for this trained model version derived from your specific job’s checkpoints.
You can see the available checkpoints for a job via the
Training API
.
Was this page helpful?
Yes
No
Previous
Deploying checkpoints
How to deploy checkpoints from Baseten Training jobs as usable models.
Next
On this page
TrainingProject management
TrainingJob management
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/training/overview:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Developing a Chain
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Exporting metrics
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Training
Overview
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
Welcome to Baseten
Training
, a powerful product designed to streamline and manage the entire lifecycle of your machine learning model training.
​
Use cases
Baseten Training provides a robust, scalable, and configurable platform for:
Training machine learning models efficiently.
Managing training artifacts, such as model checkpoints and logs.
Seamlessly productionize trained models to scalable deployments.
​
Benefits of using Baseten training
Leveraging Baseten Training for your model training workflows offers several key advantages:
Reproducibility:
Ensure consistent training runs by precisely defining your environment, code, and configurations.
Scalability:
Easily scale your training jobs from single-node to multi-node distributed training to handle large datasets and complex models.
Simplified Management:
Organize, monitor, and manage your training projects and jobs in a centralized platform.
Resource Flexibility:
Configure compute resources (CPU, GPU, memory) tailored to the specific needs of each training job.
Integrated Workflow:
Transition from training to inference and evals seemlessly within the Baseten ecosystem.
Artifact Management:
Handle large artifacts like models, checkpoints, and datasets efficiently with Baseten storage.
Framework Agnostic:
Bring your favorite training framework or roll your own with Baseten’s framework-agnostic training API.
​
Get started
Check out our
Getting Started
guide to get started with training on Baseten.
​
Go deeper
Use the following resources to learn more about training on Baseten:
CLI Reference
SDK Reference
API Reference
Was this page helpful?
Yes
No
Previous
Getting started
Your first steps to creating and running training jobs on Baseten.
Next
On this page
Use cases
Benefits of using Baseten training
Get started
Go deeper
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/troubleshooting/deployments:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Developing a Chain
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Exporting metrics
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Troubleshooting
Deployments
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
​
Issue:
truss push
can’t find
config.yaml
Copy
Ask AI
[Errno 2] No such file or directory:
'/Users/philipkiely/Code/demo_docs/config.yaml'
​
Fix: set correct target directory
The directory
truss push
is looking at is not a Truss. Make sure you’re giving
truss push
access to the correct directory by:
Running
truss push
from the directory containing the Truss. You should see the file
config.yaml
when you run
ls
in your working directory.
Or passing the target directory as an argument, such as
truss push /path/to/my-truss
.
​
Issue: unexpected failure during model build
During the model build step, there can be unexpected failures from temporary circumstances. An example is a network error while downloading model weights from Hugging Face or installing a Python package from PyPi.
​
Fix: restart deploy from Baseten UI
First, check your model logs to determine the exact cause of the error. If it’s an error during model download, package installation, or similar, you can try restarting the deploy from the model dashboard in your workspace.
Was this page helpful?
Yes
No
Previous
Inference
Troubleshoot common problems during model inference
Next
On this page
Issue: truss push can’t find config.yaml
Fix: set correct target directory
Issue: unexpected failure during model build
Fix: restart deploy from Baseten UI
Assistant
Responses are generated using AI and may contain mistakes.


Content from https://docs.baseten.co/troubleshooting/inference:

Baseten
home page
Search...
⌘
K
Get started
Overview
Quick start
Concepts
Why Baseten
How Baseten works
Development
Concepts
Model APIs
Developing a model
Developing a Chain
Deployment
Concepts
Deployments
Environments
Resources
Autoscaling
Inference
Concepts
Call your model
Streaming
Async inference
Structured LLM output
Output formats
Integrations
Training
Overview
Getting started
Concepts
Management
Deploying checkpoints
Observability
Metrics
Status and health
Security
Exporting metrics
Tracing
Billing and usage
Troubleshooting
Deployments
Inference
Support
Return to Baseten
Baseten
home page
Search...
⌘
K
Ask AI
Search...
Navigation
Troubleshooting
Inference
Documentation
Examples
Reference
Status
Documentation
Examples
Reference
Status
​
Model I/O issues
​
Error: JSONDecodeError
Copy
Ask AI
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
This error means you’re attempting to pass a model input that is not JSON-serializable. For example, you might have left out the double quotes required for a valid string:
Copy
Ask AI
truss
predict
-d
'This is not a string'
# Wrong
truss
predict
-d
'"This is a string"'
# Correct
​
Model version issues
​
Error: No OracleVersion matches the given query
Copy
Ask AI
<Server response: {
'errors': [{
'message': 'No OracleVersion matches the given query.',
'locations': [{'line': 3, 'column': 13}],
'path': ['model_version']
}],
'data': {'model_version': None}
}>
Make sure that the model ID or deployment ID you’re passing is correct and that the associated model has not been deleted.
Additionally, make sure you’re using the correct endpoint:
Production environment endpoint
.
Development deployment endpoint
.
Deployment endpoint
.
​
Authentication issues
​
Error: Service provider not found
Copy
Ask AI
ValueError: Service provider example-service-provider not found in ~/.trussrc
This error means your
~/.trussrc
is incomplete or incorrect. It should be formatted as follows:
Copy
Ask AI
[baseten]
remote_provider = baseten
api_key = YOUR.API_KEY
remote_url = https://app.baseten.co
​
Error: You have to log in to perform the request
Copy
Ask AI
<Server response: {
'errors': [{
'message': 'You have to log in to perform the request',
'locations': [{'line': 3, 'column': 13}],
'path': ['model_version'],
'extensions': {'code': 'UNAUTHENTICATED_ACCESS'}
}],
'data': {'model_version': None}
}>
This error occurs on
truss predict
when the API key in
~/.trussrc
for a given host is missing or incorrect. To fix it, update your API key in the
~/.trussrc
file.
​
Error: Please check the API key you provided
Copy
Ask AI
{
"error": "please check the api-key you provided"
}
This error occurs when using
curl
or similar to call the model via its API endpoint when the API key passed in the request header is not valid. Make sure you’re using a valid API key then try again.
Was this page helpful?
Yes
No
Previous
Deployments
Troubleshoot common problems during model deployment
On this page
Model I/O issues
Error: JSONDecodeError
Model version issues
Error: No OracleVersion matches the given query
Authentication issues
Error: Service provider not found
Error: You have to log in to perform the request
Error: Please check the API key you provided
Assistant
Responses are generated using AI and may contain mistakes.

